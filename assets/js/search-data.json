{
  
    
        "post0": {
            "title": "(6주차) 10월14일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/4) 시험일정 공지 . - (2/4) 미니배치 . - (3/4) 딥러닝용 컴퓨터를 고르는 요령 . - (4/4) 과제설명 . import . import torch from fastai.vision.all import * . Dataset . X=torch.tensor([3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]) y=torch.tensor([1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]) . X,y . (tensor([3., 4., 5., 6., 7., 8., 9.]), tensor([1., 0., 1., 0., 1., 1., 0.])) . ds=torch.utils.data.TensorDataset(X,y) . ds ## 그냥 텐서들의 pair . &lt;torch.utils.data.dataset.TensorDataset at 0x7f7e8ae947f0&gt; . ds.tensors . (tensor([3., 4., 5., 6., 7., 8., 9.]), tensor([1., 0., 1., 0., 1., 1., 0.])) . DataLoader . - 배치사이즈=2, 셔플= True, . dl=torch.utils.data.DataLoader(ds,batch_size=2,shuffle=True) . dl . &lt;torch.utils.data.dataloader.DataLoader at 0x7f7e8a1f4f40&gt; . dir(dl) . [&#39;_DataLoader__initialized&#39;, &#39;_DataLoader__multiprocessing_context&#39;, &#39;_IterableDataset_len_called&#39;, &#39;__annotations__&#39;, &#39;__class__&#39;, &#39;__class_getitem__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__orig_bases__&#39;, &#39;__parameters__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__slots__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_auto_collation&#39;, &#39;_dataset_kind&#39;, &#39;_get_iterator&#39;, &#39;_index_sampler&#39;, &#39;_is_protocol&#39;, &#39;_iterator&#39;, &#39;batch_sampler&#39;, &#39;batch_size&#39;, &#39;check_worker_number_rationality&#39;, &#39;collate_fn&#39;, &#39;dataset&#39;, &#39;drop_last&#39;, &#39;generator&#39;, &#39;multiprocessing_context&#39;, &#39;num_workers&#39;, &#39;persistent_workers&#39;, &#39;pin_memory&#39;, &#39;prefetch_factor&#39;, &#39;sampler&#39;, &#39;timeout&#39;, &#39;worker_init_fn&#39;] . dl은 배치를 만드는 기능이 있어보임 | . for xx,yy in dl: print(xx,yy) . tensor([7., 3.]) tensor([1., 1.]) tensor([6., 9.]) tensor([0., 0.]) tensor([4., 5.]) tensor([0., 1.]) tensor([8.]) tensor([1.]) . - 배치사이즈=2, 셔플= False . dl=torch.utils.data.DataLoader(ds,batch_size=2,shuffle=False) . for xx,yy in dl: print(xx,yy) . tensor([3., 4.]) tensor([1., 0.]) tensor([5., 6.]) tensor([1., 0.]) tensor([7., 8.]) tensor([1., 1.]) tensor([9.]) tensor([0.]) . - 배치사이즈=3, 셔플= True . dl=torch.utils.data.DataLoader(ds,batch_size=3,shuffle=True) . for xx,yy in dl: print(xx,yy) . tensor([3., 6., 5.]) tensor([1., 0., 1.]) tensor([8., 7., 9.]) tensor([1., 1., 0.]) tensor([4.]) tensor([0.]) . MNIST 3/7 &#50696;&#51228; . - 우선 텐서로 이루어진 X,y를 만들자. . path = untar_data(URLs.MNIST_SAMPLE) . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) y=torch.tensor([0.0]*6265 + [1.0]*6131).reshape(12396,1) . - dataset=(X,y) 를 만들자. . ds=torch.utils.data.TensorDataset(X,y) . - dataloader를 만들자. . dl=torch.utils.data.DataLoader(ds,batch_size=2048,shuffle=True) . - 네트워크(아키텍처), 손실함수, 옵티마이저 . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . - 저번시간 복습 . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(yhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7e8bd37b50&gt;] . f=torch.nn.Sigmoid() plt.plot(f(yhat.data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7e73ba0910&gt;] . - 미니배치활용 . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . 네트워크 파라메터 다시 초기화 | . 12396 / 2048 . 6.052734375 . 총 7개의 미니배치가 만들어질것임 $ to$ 따라서 파라메터를 업데이트하는 횟수는 7 $ times$ epoc 임 (실제적으로는 6 $ times$ epoc) | . 200/6 . 33.333333333333336 . for epoc in range(33): for xx,yy in dl: ### 총 7번돌면 끝나는 for ## 1 yyhat=net(xx) ## 2 loss= loss_fn(yyhat,yy) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(yyhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7e746f2d30&gt;] . 이게 왜이러지?? | . - 배치사이즈를 다시 확인해보자. . for xx,yy in dl: print(xx.shape,yy.shape) . torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([108, 784]) torch.Size([108, 1]) . - 마지막이 108개이므로 108개의 y만 그려짐 . plt.plot(net(X).data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7e7474f1c0&gt;] . - 2048개 정도만 대충학습해도 동일 반복횟수에 대하여 거의 대등한 효율이 나옴 . - GPU에 있는 메모리로 12396개의 데이터를 모두 보내지 않아도 괜찮겠다 $ to$ 그래픽카드의 메모리를 얼마나 큰 것으로 살지는 자료의 크기와는 상관없다. . - net.parameters()에 저장된 값들은 그대로 GPU로 가야만한다. $ to$ 그래픽카드의 메모리를 얼마나 큰것으로 살지는 모형의 복잡도와 관련이 있다. . 컴퓨터사는방법 . 메모리: $n$이 큰 자료를 다룰수록 메모리가 커야한다. | GPU의 메모리: 모형의 복잡도가 커질수록 GPU의 메모리가 커야한다. | . &#49689;&#51228; . - batchsize=1024로 바꾼후 학습해보고 결과를 관찰할것 .",
            "url": "https://guebin.github.io/2021BDA/2021/10/14/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9414%EC%9D%BC.html",
            "relUrl": "/2021/10/14/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9414%EC%9D%BC.html",
            "date": " • Oct 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "(5주차) 10월12일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/8) 손실함수 (1) . - (2/8) 손실함수 (2) . - (3/8) 손실함수차이를 애니메이션으로 . - (4/8) Adam . - (5/8) Adam animation . - (6/8) 신경망은 왜 깊어졌는가?,universal approximation theorem . - (7/8) MNIST with MLP 풀이1 . - (8/8) MNIST with MLP 풀이2,3 . MSEloss &#50752; BCEloss &#48708;&#44368; . &#49552;&#49892;&#54632;&#49688;&#51032; &#47784;&#50577;&#48708;&#44368; . import torch import numpy as np import matplotlib.pyplot as plt . torch.manual_seed(1) X=torch.linspace(-1,1,2000).reshape(2000,1) w0=-1.0 w1=5.0 u=w0+X*w1 v=torch.exp(u)/(1+torch.exp(u)) y=torch.bernoulli(v) . plt.scatter(X,y,alpha=0.01) plt.plot(X,v) . [&lt;matplotlib.lines.Line2D at 0x7fb387cbaa30&gt;] . _w0= np.arange(-10,3,0.05) _w1= np.arange(-1,10,0.05) . _w0, _w1 =np.meshgrid(_w0,_w1,indexing=&#39;ij&#39;) . _w0=_w0.reshape(-1) _w1=_w1.reshape(-1) . def lossfn_crossenp(w0,w1): yhat=torch.exp( w0+w1*X) / (1+torch.exp( w0+w1*X)) loss= - torch.mean (y*torch.log(yhat)+(1-y)*torch.log(1-yhat)) return loss.tolist() . def lossfn_mse(w0,w1): yhat=torch.exp( w0+w1*X) / (1+torch.exp( w0+w1*X)) loss= torch.mean((y-yhat)**2) return loss.tolist() . _l1=list(map(lossfn_crossenp,_w0,_w1)) _l2=list(map(lossfn_mse,_w0,_w1)) . fig = plt.figure() ax1=fig.add_subplot(1,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(1,2,2,projection=&#39;3d&#39;) ax1.elev=15 ax2.elev=15 ax1.azim=75 ax2.azim=75 fig.set_figheight(15) fig.set_figwidth(15) . ax1.scatter(_w0,_w1,_l1,s=0.01) ax2.scatter(_w0,_w1,_l2,s=0.01) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fb3872b78b0&gt; . _w0[np.argmin(_l1)],_w1[np.argmin(_l1)] . (-0.9999999999998721, 5.150000000000006) . _w0[np.argmin(_l2)],_w1[np.argmin(_l2)] . (-0.9999999999998721, 5.100000000000005) . ax1.scatter(_w0[np.argmin(_l1)],_w1[np.argmin(_l1)],np.min(_l1),s=200,marker=&#39;*&#39;) ax2.scatter(_w0[np.argmin(_l2)],_w1[np.argmin(_l2)],np.min(_l2),s=200,marker=&#39;*&#39;) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fb38770d910&gt; . fig . &#50500;&#53412;&#53581;&#52376;, &#50741;&#54000;&#47560;&#51060;&#51200; . l1=torch.nn.Linear(in_features=1,out_features=1,bias=True) a1=torch.nn.Sigmoid() net=torch.nn.Sequential(l1,a1) optimizer=torch.optim.SGD(net.parameters(),lr=0.05) . &#52488;&#44592;&#44050; $(w_0,w_1)=(-3,-1)$&#51012; &#45824;&#51077;&#54616;&#44256; &#49688;&#47156;&#44284;&#51221;&#51012; animation&#51004;&#47196; &#44288;&#52272;&#54616;&#51088;. . - 파라메터 초기값 $(w_0,w_1)=(-3,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([0.0331]), tensor([[-0.1853]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - BCEloss를 이용하여 학습+기록 . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-0.6726]), tensor([[3.3696]])) . - 파라메터 초기값 $(w_0,w_1)=(-3,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-0.6726]), tensor([[3.3696]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - MSEloss를 이용하여 학습+기록 . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-0.9688]), tensor([[0.7116]])) . - plot . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-3,-1,lossfn_crossenp(-3,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-3,-1,lossfn_mse(-3,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#52488;&#44592;&#44050; $(w_0,w_1)=(-10,-1)$&#51012; &#45824;&#51077;&#54616;&#44256; &#49688;&#47156;&#44284;&#51221;&#51012; animation&#51004;&#47196; &#44288;&#52272;&#54616;&#51088;. . - 파라메터 초기값 $(w_0,w_1)=(-10,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-0.9688]), tensor([[0.7116]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - BCEloss를 이용하여 학습+기록 . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-0.8302]), tensor([[4.0264]])) . - 파라메터 초기값 $(w_0,w_1)=(-10,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-0.8302]), tensor([[4.0264]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - MSEloss를 이용하여 학습+기록 . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-9.9990]), tensor([[-0.9995]])) . - plot . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-10,-1,lossfn_crossenp(-10,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-10,-1,lossfn_mse(-10,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect Adam &#50741;&#54000;&#47560;&#51060;&#51200;, $(w_0,w_1)=(-3,-1)$ . - 옵티마이저 재설정 . optimizer=torch.optim.Adam(net.parameters(),lr=0.05) . - 파라메터 초기값 $(w_0,w_1)=(-3,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-9.9990]), tensor([[-0.9995]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - BCEloss를 사용하여 학습 + 기록 . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . - 파라메터 초기값 $(w_0,w_1)=(-3,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-1.0201]), tensor([[5.1584]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - MSEloss를 사용하여 학습+기록 . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-3,-1,lossfn_crossenp(-3,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-3,-1,lossfn_mse(-3,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect Adam &#50741;&#54000;&#47560;&#51060;&#51200;, $(w_0,w_1)=(-10,-1)$ . - 옵티마이저 재설정 . optimizer=torch.optim.Adam(net.parameters(),lr=0.05) . - 파라메터 초기값 $(w_0,w_1)=(-10,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-0.9995]), tensor([[5.0790]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - BCEloss를 사용하여 학습 + 기록 . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . - 파라메터 초기값 $(w_0,w_1)=(-10,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-1.0243]), tensor([[5.1769]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - MSEloss를 사용하여 학습+기록 . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-10,-1,lossfn_crossenp(-10,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-10,-1,lossfn_mse(-10,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#47784;&#54805;&#51032; &#54364;&#54788;&#47141;: &#50780; &#49888;&#44221;&#47581;&#51008; &#44618;&#50612;&#51276;&#45716;&#44032;? . &#45331;&#51008; &#49888;&#44221;&#47581; (&#54616;&#45208;&#51032; &#51008;&#45769;&#52789; + &#52649;&#48516;&#55176; &#53360; &#45432;&#46300;&#47484; &#44032;&#51652; &#49888;&#44221;&#47581;) . - (universal approximation theorem) 하나의 은닉층과 충분히 큰 노드를 가진 신경망은 거의 모든 함수를 근사할 수 있다. . - 핵심아이디어: (node1=선형+비선형) + (node2=선형+비선형) $ to$ locally compact basis $ to$ 구불구불하게 다 맞출수가 있다. . 선형변환을 무한번 선형변환해도 결과는 그냥 선형변환이다 $ to$ 모든 range에 값이 있는 basis $ to$ 표현력이 약하다. (한쪽을 맞추면 다른쪽을 맞추기 힘듬) | 하지만 아주 단순한 비선형변환을 섞기만 해도 표현력이 비약적으로 상승한다. | . - 트릭은 비선형변환 . &#44536;&#47111;&#45796;&#47732; &#50780; &#45331;&#51008; &#49888;&#44221;&#47581;&#51012; &#50416;&#51648; &#50506;&#45716;&#44032;? . - 안전한 대답 (그리고 쓸모없는 대답): 실험적으로 깊은 신경망이 더 효과적임이 입증되었다. . - 좀 더 고민을 해본 대답 . 넓은신경망보다 깊은신경망이 파라메터수 대비 복잡도를 더 쉽게 올릴수 있다. | 넓은신경망보다 깊은신경망이 오퍼피팅 이슈를 피하기 쉽다. | . - 내 생각 . 깊은 신경망은 계층적인 모형이다. | 즉 깊은 신경망은 여러스케일로 자료를 관찰한다. | . Pytoch MLP (MNIST 3,7) . import torch from fastai.vision.all import * . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . data . - download data . path = untar_data(URLs.MNIST_SAMPLE) . path.ls() . (#3) [Path(&#39;/home/cgb2/.fastai/data/mnist_sample/labels.csv&#39;),Path(&#39;/home/cgb2/.fastai/data/mnist_sample/valid&#39;),Path(&#39;/home/cgb2/.fastai/data/mnist_sample/train&#39;)] . - list . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . - list $ to$ image . Image.open(threes[4]) . - image $ to$ tensor . tensor(Image.open(threes[4])) . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 114, 255, 228, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 38, 203, 252, 224, 169, 169, 169, 131, 57, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 7, 81, 168, 168, 216, 252, 252, 252, 247, 197, 159, 47, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 28, 28, 178, 253, 252, 252, 240, 101, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 76, 200, 250, 226, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 231, 225, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 191, 252, 225, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 51, 126, 231, 252, 202, 75, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 204, 253, 253, 253, 254, 253, 244, 75, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 253, 252, 252, 252, 197, 121, 56, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 216, 252, 186, 56, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 215, 205, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 101, 247, 253, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 172, 252, 101, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 57, 243, 235, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 175, 253, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 179, 254, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 7, 32, 57, 57, 169, 169, 234, 252, 247, 65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 176, 204, 228, 252, 252, 253, 252, 252, 177, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 75, 140, 139, 177, 151, 140, 65, 28, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8) . 여기에서 tensor는 파이토치가 아니라 fastai에서 구현한 함수임 | . - 여러개의 리스트를 모두 텐서로 바꿔보자. . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . - $X$와 $y$를 만들자. . seven_tensor.shape, three_tensor.shape . (torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28])) . y=torch.tensor([0.0]*6265+ [1.0]*6131).reshape(12396,1) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) . X.shape, y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . &#47784;&#54805; . ${ bf X} to { bf WX+b} to f({ bf WX+b}) to dots to { bf y}$ . ${ bf X}=12396 times 784$ matrix | ${ bf y}=12396 times 1$ (col) vector | . - 모델을 어떻게 구성할것인가? . 아키텍처: 적당히 깊게... + 적당히 넓게... + 표현력이 충분하면서도 + 과적합은 일어나지 않도록.. (저도 잘 몰라요) | 손실함수: BCEloss | 옵티마이저: Adam | . - 교재의 모형 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node30&quot; -&gt; &quot;y&quot; label = &quot;Layer 2: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2: Sigmoid x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y y node1&#45;&gt;y node2&#45;&gt;y ...&#45;&gt;y node30&#45;&gt;y &#54400;&#51060;1 . - 그럼 이제 풀어보자. (아키텍처만 만들어주면 금방구현한다.) . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28, out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30, out_features=1), torch.nn.Sigmoid() ) optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= -torch.mean(y*torch.log(yhat)+(1-y)*torch.log(1-yhat)) ## BCEloss ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], requires_grad=True), Parameter containing: tensor([-0.0098, 0.0315, 0.0363, -0.0093, 0.1433, 0.0175, 0.0139, -0.0237, 0.0323, 0.0351, -0.0125, 0.0443, 0.0176, 0.0745, 0.0098, 0.0042, 0.0361, 0.0394, 0.0534, 0.0175, 0.0567, 0.0148, 0.0459, 0.0648, 0.0009, -0.0279, 0.0972, 0.0478, 0.0612, 0.0504], requires_grad=True), Parameter containing: tensor([[ 0.2154, 0.1926, 0.2019, 0.1671, -0.1840, -0.0726, -0.1608, 0.1046, -0.2522, -0.2444, 0.1257, -0.1815, 0.1002, -0.0963, -0.3047, 0.1256, 0.1862, 0.2499, -0.1381, 0.2051, -0.2633, 0.1915, -0.1853, -0.1719, 0.1156, 0.1573, -0.1129, 0.1308, -0.1625, -0.1472]], requires_grad=True), Parameter containing: tensor([-0.1153], requires_grad=True)] . plt.plot(y) plt.plot(yhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f1809b322b0&gt;] . ypred=yhat&gt;0.5 . sum(ypred==y)/12396 . tensor([0.9893]) . &#54400;&#51060;2: torch&#50640; &#45236;&#51109;&#46108; &#49552;&#49892;&#54632;&#49688; &#51060;&#50857; . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28, out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30, out_features=1), #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= loss_fn(yhat,y) ## BCEloss ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], requires_grad=True), Parameter containing: tensor([-0.0098, 0.0315, 0.0363, -0.0093, 0.1433, 0.0175, 0.0139, -0.0237, 0.0323, 0.0351, -0.0125, 0.0443, 0.0176, 0.0745, 0.0098, 0.0042, 0.0361, 0.0394, 0.0534, 0.0175, 0.0567, 0.0148, 0.0459, 0.0648, 0.0009, -0.0279, 0.0972, 0.0478, 0.0612, 0.0504], requires_grad=True), Parameter containing: tensor([[ 0.2154, 0.1926, 0.2019, 0.1671, -0.1840, -0.0726, -0.1608, 0.1046, -0.2522, -0.2444, 0.1257, -0.1815, 0.1002, -0.0963, -0.3047, 0.1256, 0.1862, 0.2499, -0.1381, 0.2051, -0.2633, 0.1915, -0.1853, -0.1719, 0.1156, 0.1573, -0.1129, 0.1308, -0.1625, -0.1472]], requires_grad=True), Parameter containing: tensor([-0.1153], requires_grad=True)] . plt.plot(y) plt.plot(yhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f180998d100&gt;] . f=torch.nn.Sigmoid() plt.plot(y) plt.plot(f(yhat.data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f1809969460&gt;] . &#54400;&#51060;3: torch&#50640; &#45236;&#51109;&#46108; &#49552;&#49892;&#54632;&#49688; &#51060;&#50857; + GPU . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28, out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30, out_features=1), #torch.nn.Sigmoid() ) . net.to(&quot;cuda:0&quot;) . Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) . X_gpu=X.to(&quot;cuda:0&quot;) y_gpu=y.to(&quot;cuda:0&quot;) . loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat_gpu=net(X_gpu) ## 2 loss= loss_fn(yhat_gpu,y_gpu) ## BCEloss ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([-0.0098, 0.0315, 0.0363, -0.0093, 0.1433, 0.0175, 0.0139, -0.0237, 0.0323, 0.0351, -0.0125, 0.0443, 0.0176, 0.0745, 0.0098, 0.0042, 0.0361, 0.0394, 0.0534, 0.0175, 0.0567, 0.0148, 0.0459, 0.0648, 0.0009, -0.0279, 0.0972, 0.0478, 0.0612, 0.0504], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([[ 0.2154, 0.1926, 0.2019, 0.1671, -0.1840, -0.0726, -0.1608, 0.1046, -0.2522, -0.2444, 0.1257, -0.1815, 0.1002, -0.0963, -0.3047, 0.1256, 0.1862, 0.2499, -0.1381, 0.2051, -0.2633, 0.1915, -0.1853, -0.1719, 0.1156, 0.1573, -0.1129, 0.1308, -0.1625, -0.1472]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([-0.1153], device=&#39;cuda:0&#39;, requires_grad=True)] .",
            "url": "https://guebin.github.io/2021BDA/2021/10/12/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%9412%EC%9D%BC.html",
            "relUrl": "/2021/10/12/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%9412%EC%9D%BC.html",
            "date": " • Oct 12, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "(5주차) 10월7일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/2) 기말고사 안내 + 알렉스넷 . - (2/2) 로지스틱 회귀분석 . Logistic regression . import torch import matplotlib.pyplot as plt . Example . - 현실에서 이런 경우가 많음 . $x$가 커질수록 (혹은 작아질수록) 성공확률이 증가함. | . - 이러한 모형은 아래와 같이 설계할 수 있음 &lt; 외우세요!!! . $y_i sim Ber( pi_i), quad $ where $ pi_i = frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$ . | $ hat{y}_i= frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= frac{1}{1+ exp(- hat{w}_0- hat{w}_1x_i)}$ . | $loss= - sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ &lt; 외우세요!! . | . - 예제시작 . X=torch.linspace(-1,1,2000).reshape(2000,1) w0= - 1 w1= 5 u = w0+X*w1 v = torch.exp(u)/(1+torch.exp(u)) # v=πi y = torch.bernoulli(v) . plt.scatter(X,y,alpha=0.05) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4205e1c460&gt;] . - 다이어그램으로 표현하면 . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39; + s + &#39;; }&#39;) . 나름대로 딥러닝의 골치덩이를 해결하는 방법을 제시함.. --&gt; 관심 X . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;X@W&quot;[label=&quot;@W&quot;] &quot;X@W&quot; -&gt; &quot;Sigmoid(X@W)=yhat&quot;[label=&quot;Sigmoid&quot;] label = &quot;Layer 1&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 X X X@W X@W X&#45;&gt;X@W @W Sigmoid(X@W)=yhat Sigmoid(X@W)=yhat X@W&#45;&gt;Sigmoid(X@W)=yhat Sigmoid gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; X label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; X -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Sigmoid X X node1=yhat node1=yhat X&#45;&gt;node1=yhat - 아키텍처, 손실함수, 옵티마이저 . torch.manual_seed(43052) l1=torch.nn.Linear(in_features=1,out_features=1,bias=True) a1=torch.nn.Sigmoid() net=torch.nn.Sequential(l1,a1) #loss = torch.mean((y-yhat)**2) &lt; 이러면 안됩니다!!! optimizer=torch.optim.SGD(net.parameters(),lr=0.05) . plt.scatter(X,y,alpha=0.01) plt.plot(X,net(X).data,&#39;--&#39;) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f4205baa9a0&gt;] . - step1~4 . for epoc in range(10000): ## 1 yhat=net(X) ## 2 loss=-torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[4.6663]], requires_grad=True), Parameter containing: tensor([-0.9541], requires_grad=True)] . plt.scatter(X,y,alpha=0.01) plt.plot(X,net(X).data,&#39;--&#39;) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f41fc7c5040&gt;] . &#49689;&#51228; . loss를 mse로 바꿔서 돌려볼것 . torch.manual_seed(43052) l1=torch.nn.Linear(in_features=1,out_features=1,bias=True) a1=torch.nn.Sigmoid() net=torch.nn.Sequential(l1,a1) #loss = torch.mean((y-yhat)**2) &lt; 이러면 안됩니다!!! optimizer=torch.optim.SGD(net.parameters(),lr=0.05) . for epoc in range(10000): ## 1 yhat=net(X) ## 2 ##### loss=-torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) &lt;-- 여기만수정해서!! ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[4.6663]], requires_grad=True), Parameter containing: tensor([-0.9541], requires_grad=True)] . plt.scatter(X,y,alpha=0.01) plt.plot(X,net(X).data,&#39;--&#39;) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f41fc7c5040&gt;] .",
            "url": "https://guebin.github.io/2021BDA/2021/10/07/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%947%EC%9D%BC.html",
            "relUrl": "/2021/10/07/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%947%EC%9D%BC.html",
            "date": " • Oct 7, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "(4주차) 10월5일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/5) : 신경망 다어어그램 소개, polynomial regression (1) . - (2/5) : polynomial regression (2), peicewise linear model . - (3/5) : 표현력의 증가, 신경망 다이어그램을 다시 소개 . - (4/5) : 노드수가 증가하면 국소최소점에 빠질 확률이 높음 (파라메터 학습어려움) . - (5/5) : 노드수가 증가하면 오버피팅 이슈가 있음 . Import . import torch import numpy as np import matplotlib.pyplot as plt . graphviz setting . import graphviz . 설치가 되어있지 않다면 아래를 실행할것 (2개모두) | . !conda install -c conda-forge graphviz -y !pip install graphviz . - 다이어그램을 그리기 위한 준비 . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . &#50696;&#51228;1: &#49440;&#54805;&#47784;&#54805; . - $y_i= w_0+w_1 x_i + epsilon_i Longrightarrow hat{y}_i = hat{w}_0+ hat{w}_1 x_i$ . $ epsilon_i sim N(0,1)$ | . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;w0 + x*w1&quot;[label=&quot;* w0&quot;] &quot;x&quot; -&gt; &quot;w0 + x*w1&quot; [label=&quot;* w1&quot;] &quot;w0 + x*w1&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 w0 + x*w1 w0 + x*w1 1&#45;&gt;w0 + x*w1 * w0 yhat yhat w0 + x*w1&#45;&gt;yhat indentity x x x&#45;&gt;w0 + x*w1 * w1 gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@W, bias=False&quot;[label=&quot;@W&quot;] ; &quot;X@W, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@W, bias=False X@W, bias=False X&#45;&gt;X@W, bias=False @W yhat yhat X@W, bias=False&#45;&gt;yhat indentity gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] ; &quot;x*w, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*w, bias=True x*w, bias=True x&#45;&gt;x*w, bias=True *w yhat yhat x*w, bias=True&#45;&gt;yhat indentity &#50696;&#51228;2: polynomial regression . $y_i=w_0+w_1x_i + w_2 x_i^2 + w_3 x_i^3 + epsilon_i$ . gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@W, bias=True&quot;[label=&quot;@W&quot;] &quot;X@W, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@W, bias=True X@W, bias=True X&#45;&gt;X@W, bias=True @W yhat yhat X@W, bias=True&#45;&gt;yhat indentity ${ bf X} = begin{bmatrix} x_1 &amp; x_1^2 &amp; x_1^3 x_2 &amp; x_2^2 &amp; x_2^3 dots &amp; dots &amp; dots x_n &amp; x_n^2 &amp; x_n^3 end{bmatrix}, quad { bf W} = begin{bmatrix} w_1 w_2 w_3 end{bmatrix}$. | . &#49884;&#48044;&#47112;&#51060;&#49496; &#50672;&#49845; . - 모형 . torch.manual_seed(43052) x,_ = torch.randn(100).sort() X=torch.vstack([x,x**2,x**3]).T W=torch.tensor([[4.0],[3.0],[-2.0]]) bias=1.0 ϵ=torch.randn(100,1) y=X@W+bias + ϵ . plt.plot(X[:,0],y,&#39;.&#39;) #plt.plot(X[:,0],X@W+bias,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9514460a90&gt;] . - 아키텍처 . net = torch.nn.Linear(in_features=3,out_features=1,bias=True) . - 손실함수 . loss_fn=torch.nn.MSELoss() . - 옵티마이저 . optimizer= torch.optim.SGD(net.parameters(),lr=0.01) . - step1~4 . for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 3.7411, 2.8648, -1.9074]], requires_grad=True), Parameter containing: tensor([1.0239], requires_grad=True)] . plt.plot(X[:,0],y,&#39;.&#39;) plt.plot(X[:,0],yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f95143bf730&gt;] . &#50696;&#51228;3: piece-wise linear regression . - 모델 . _x = np.linspace(-1,1,100).tolist() _f = lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5 +np.random.normal()*0.3 _y = list(map(_f,_x)) . plt.plot(_x,_y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f951431ba00&gt;] . X=torch.tensor(_x).reshape(100,1) y=torch.tensor(_y).reshape(100,1) . &#54400;&#51060;1 . - 아키텍처 + 손실함수(MSE) + 옵티마이저(SGD) . net=torch.nn.Linear(in_features=1,out_features=1,bias=True) loss_fn = torch.nn.MSELoss() optimizer = torch.optim.SGD(net.parameters(),lr=0.1) . - step1~4 . for epoc in range(10000): ## 1 yhat=net(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f95142907c0&gt;] . - 실패: 그리고 epoc을 10억번 반복해도 이건 실패할 모형임 . 왜? 모델자체가 틀렸음. | 모델의 표현력이 너무 부족하다. $ to$ underfitting | . &#54400;&#51060;2 (&#48708;&#49440;&#54805; &#54876;&#49457;&#54868;&#54632;&#49688;&#47484; &#46020;&#51077;) . - 비선형활성화함수를 도입하자. (네트워크수정) . torch.manual_seed(1) layer1 = torch.nn.Linear(in_features=1,out_features=1,bias=False) activation1 = torch.nn.ReLU() layer2 = torch.nn.Linear(in_features=1,out_features=1,bias=False) net2 = torch.nn.Sequential(layer1,activation1,layer2) . _x=np.linspace(-1,1,100) plt.plot(_x,_x) plt.plot(_x,activation1(torch.tensor(_x))) . [&lt;matplotlib.lines.Line2D at 0x7f95142873a0&gt;] . - 표현력 확인 . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net2(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9514200310&gt;] . - 옵티마이저2 . optimizer2 = torch.optim.SGD(net2.parameters(),lr=0.1) . - step1~4 . for epoc in range(1000): ## 1 yhat=net2(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer2.step() net2.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f951416d3a0&gt;] . - discussion . 이것 역시 수백억번 epoc을 반복해도 이 이상 적합하기 힘들다. $ to$ 모형의 표현력이 낮다. | 해결책: 주황색점선이 2개 있다면 어떨까? | . &#54400;&#51060;3 (&#45432;&#46300;&#49688; &#52628;&#44032;) . - 아키텍처 + 옵티마이저 . torch.manual_seed(1) ## 초기가중치를 동일하게 layer1 = torch.nn.Linear(in_features=1,out_features=2,bias=False) activation1 = torch.nn.ReLU() layer2 = torch.nn.Linear(in_features=2,out_features=1,bias=False) net3 = torch.nn.Sequential(layer1,activation1,layer2) optimizer3= torch.optim.SGD(net3.parameters(),lr=0.1) . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net3(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f95140dd280&gt;] . - Step 1~4 . for epoc in range(1000): ## 1 yhat=net3(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer3.step() net3.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f95140c9760&gt;] . - discussion . list(net3.parameters()) . [Parameter containing: tensor([[ 1.9043], [-1.0061]], requires_grad=True), Parameter containing: tensor([[ 1.8523, -0.9666]], requires_grad=True)] . 파라메터확인 | . W1=(layer1.weight.data).T W2=(layer2.weight.data).T W1,W2 . (tensor([[ 1.9043, -1.0061]]), tensor([[ 1.8523], [-0.9666]])) . 파라메터 저장 | . - 어떻게 적합이 이렇게 우수하게 되었는지 따져보자. . u1=X@W1 plt.plot(u1) #plt.plot(X@W1) . [&lt;matplotlib.lines.Line2D at 0x7f951402fbb0&gt;, &lt;matplotlib.lines.Line2D at 0x7f951402fbe0&gt;] . v1=activation1(u1) plt.plot(v1) #plt.plot(activation1(X@W1)) . [&lt;matplotlib.lines.Line2D at 0x7f9513fa55b0&gt;, &lt;matplotlib.lines.Line2D at 0x7f9513fa55e0&gt;] . _yhat=v1@W2 plt.plot(X,y,&#39;.&#39;) plt.plot(X,_yhat,&#39;--&#39;) #plt.plot(X,activation1(X@W1)@W2,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9513f0eee0&gt;] . &#51104;&#44624;&#50836;&#50557; (&#49888;&#44221;&#47581;) . - 계산과정 . (1) $X to X@W^{(1)} to ReLU(X@W^{(1)}) to ReLU(X@W^{(1)})@W^{(2)}=yhat$ . $X: n times 1$ | $W^{(0)}: 1 times 2$ | $W^{(1)}: 2 times 1$ | . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;X@W1&quot;[label=&quot;@W1&quot;] &quot;X@W1&quot; -&gt; &quot;ReLU(X@W1)&quot;[label=&quot;ReLU&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;ReLU(X@W1)&quot; -&gt; &quot;ReLU(X@W1)@W2:=yhat&quot;[label=&quot;@W2&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 cluster_3 Layer 2 X X X@W1 X@W1 X&#45;&gt;X@W1 @W1 ReLU(X@W1) ReLU(X@W1) X@W1&#45;&gt;ReLU(X@W1) ReLU ReLU(X@W1)@W2:=yhat ReLU(X@W1)@W2:=yhat ReLU(X@W1)&#45;&gt;ReLU(X@W1)@W2:=yhat @W2 (2) 아래와 같이 표현할 수도 있다. . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;u1[:,0]&quot;[label=&quot;*W1[0,0]&quot;] &quot;X&quot; -&gt; &quot;u1[:,1]&quot;[label=&quot;*W1[0,1]&quot;] &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;Relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;Relu&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;v1[:,0]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[0,0]&quot;] &quot;v1[:,1]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[1,0]&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 cluster_3 Layer 2 X X u1[:,0] u1[:,0] X&#45;&gt;u1[:,0] *W1[0,0] u1[:,1] u1[:,1] X&#45;&gt;u1[:,1] *W1[0,1] v1[:,0] v1[:,0] u1[:,0]&#45;&gt;v1[:,0] Relu v1[:,1] v1[:,1] u1[:,1]&#45;&gt;v1[:,1] Relu yhat yhat v1[:,0]&#45;&gt;yhat *W2[0,0] v1[:,1]&#45;&gt;yhat *W2[1,0] gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2 X X node1 node1 X&#45;&gt;node1 node2 node2 X&#45;&gt;node2 yhat yhat node1&#45;&gt;yhat node2&#45;&gt;yhat - 위와 같은 다이어그램을 적용하면 예제1은 아래와 같이 표현가능 . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;1&quot; &quot;x&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;1&quot; -&gt; &quot;node1=yhat&quot; &quot;x&quot; -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Identity&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Identity 1 1 node1=yhat node1=yhat 1&#45;&gt;node1=yhat x x x&#45;&gt;node1=yhat gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;x&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x&quot; -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Identity&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Identity x x node1=yhat node1=yhat x&#45;&gt;node1=yhat - 예제2의 아키텍처 . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;x&quot; &quot;x**2&quot; &quot;x**3&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x&quot; -&gt; &quot;node1=yhat&quot; &quot;x**2&quot; -&gt; &quot;node1=yhat&quot; &quot;x**3&quot; -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Identity&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Identity x x node1=yhat node1=yhat x&#45;&gt;node1=yhat x**2 x**2 x**2&#45;&gt;node1=yhat x**3 x**3 x**3&#45;&gt;node1=yhat &#54400;&#51060;3&#51060; &#49892;&#54056;&#54624; &#49688;&#46020; &#51080;&#51020; . - 아키텍처 + 옵티마이저 . torch.manual_seed(40352) ## 초기가중치를 동일하게 layer1 = torch.nn.Linear(in_features=1,out_features=2,bias=False) activation1 = torch.nn.ReLU() layer2 = torch.nn.Linear(in_features=2,out_features=1,bias=False) net3 = torch.nn.Sequential(layer1,activation1,layer2) optimizer3= torch.optim.SGD(net3.parameters(),lr=0.1) . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net3(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9513f0bcd0&gt;] . - Step 1~4 . for epoc in range(10000): ## 1 yhat=net3(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer3.step() net3.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9513cbecd0&gt;] . - 왜 가중치가 변하지 않는가? (이것보다 더 좋은 fitting이 있음을 우리는 이미 알고있는데..) . W1=(layer1.weight.data).T W2=(layer2.weight.data).T W1,W2 . (tensor([[5.2157e-04, 1.8399e+00]]), tensor([[0.0722], [1.9172]])) . u1=X@W1 plt.plot(u1) #plt.plot(X@W1) . [&lt;matplotlib.lines.Line2D at 0x7f9513c404f0&gt;, &lt;matplotlib.lines.Line2D at 0x7f9513c40520&gt;] . v1=activation1(u1) plt.plot(v1) #plt.plot(activation1(X@W1)) . [&lt;matplotlib.lines.Line2D at 0x7f9513ba52e0&gt;, &lt;matplotlib.lines.Line2D at 0x7f9513ba5310&gt;] . _yhat=v1@W2 plt.plot(X,y,&#39;.&#39;) plt.plot(X,_yhat,&#39;--&#39;) #plt.plot(X,activation1(X@W1)@W2,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9513b164c0&gt;] . - 고약한 상황에 빠졌음. . &#54400;&#51060;4: &#45331;&#51008; &#49888;&#44221;&#47581; . - Custom Activation Function . def mooyaho(input): return torch.sigmoid(200*input) class MOOYAHO(torch.nn.Module): def __init__(self): super().__init__() # init the base class def forward(self, input): return mooyaho(input) # simply apply already implemented SiLU . _x=torch.linspace(-10,10,100) plt.plot(_x,mooyaho(_x)) . [&lt;matplotlib.lines.Line2D at 0x7f951355cd90&gt;] . - 아키텍처 . torch.manual_seed(1) # 초기가중치를 똑같이 하기 위해서.. layer1=torch.nn.Linear(in_features=1,out_features=500,bias=True) activation1=MOOYAHO() layer2=torch.nn.Linear(in_features=500,out_features=1,bias=True) net4=torch.nn.Sequential(layer1,activation1,layer2) optimizer4=torch.optim.SGD(net4.parameters(),lr=0.001) . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net4(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9513488250&gt;] . - step1~4 . for epoc in range(5000): # 1 yhat=net4(X) # 2 loss=loss_fn(yhat,y) # 3 loss.backward() # 4 optimizer4.step() net4.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f951373b3d0&gt;] . - 넓은 신경망은 과적합을 하는 경우가 종종있다. . - 무엇이든 맞출 수 있음 . torch.manual_seed(43052) __X = torch.linspace(-1,1,100).reshape(100,1) __y = torch.randn(100,1) . plt.plot(__X,__y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f95137ed340&gt;] . torch.manual_seed(1) # 초기가중치를 똑같이 하기 위해서.. layer1=torch.nn.Linear(in_features=1,out_features=500,bias=True) activation1=MOOYAHO() layer2=torch.nn.Linear(in_features=500,out_features=1,bias=True) net4=torch.nn.Sequential(layer1,activation1,layer2) optimizer4=torch.optim.SGD(net4.parameters(),lr=0.001) . - step1~4 . for epoc in range(5000): # 1 __yhat=net4(__X) # 2 loss=loss_fn(__yhat,__y) # 3 loss.backward() # 4 optimizer4.step() net4.zero_grad() . - result . plt.plot(__X,__y,) plt.plot(__X,__yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9510145d00&gt;] . loss_fn(__y,__y*0), loss_fn(__y,__yhat.data) . (tensor(1.1437), tensor(0.7460)) . &#49689;&#51228; . - 예제2: polynomial regression 에서 . optimizer= torch.optim.SGD(net.parameters(),lr=0.01) . 대신에 . optimizer= torch.optim.SGD(net.parameters(),lr=0.1) . 로 변경하여 학습하고 결과를 관찰할것. . 설명: 위와 같은 결과가 나온 이유는... .",
            "url": "https://guebin.github.io/2021BDA/2021/10/05/(4%EC%A3%BC%EC%B0%A8)-10%EC%9B%945%EC%9D%BC.html",
            "relUrl": "/2021/10/05/(4%EC%A3%BC%EC%B0%A8)-10%EC%9B%945%EC%9D%BC.html",
            "date": " • Oct 5, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "(4주차) 9월30일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/4) Step 1~2 요약 (1) . - (2/4) Step 1~2 요약 (2), Step 3: derivation . - (3/4) Step 4: update (1) . - (4/4) Step 4: update (2), Step 1~4 를 for 문으로 처리 . import torch import numpy as np . Data . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(43052) n=100 ones= torch.ones(n) x,_ = torch.randn(n).sort() X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ ytrue = X@W . step1~2 &#50836;&#50557; . &#48169;&#48277;1: &#47784;&#45944;&#51012; &#51649;&#51217;&#49440;&#50616; + loss&#54632;&#49688;&#46020; &#51649;&#51217;&#49440;&#50616; . What1=torch.tensor([-5.0,10.0],requires_grad=True) yhat1=X@What1 loss1=torch.mean((y-yhat1)**2) loss1 . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;2: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=False) + loss &#51649;&#51217;&#49440;&#50616; . net2=torch.nn.Linear(in_features=2,out_features=1,bias=False) net2.weight.data= torch.tensor([[-5.0,10.0]]) yhat2=net2(X) loss2=torch.mean((y.reshape(100,1)-yhat2)**2) loss2 . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;3: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=True) + loss &#51649;&#51217;&#49440;&#50616; . net3=torch.nn.Linear(in_features=1,out_features=1,bias=True) net3.weight.data= torch.tensor([[10.0]]) net3.bias.data= torch.tensor([[-5.0]]) yhat3=net3(x.reshape(100,1)) loss3=torch.mean((y.reshape(100,1)-yhat3)**2) loss3 . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;4: &#47784;&#45944;&#49885;&#51012; &#51649;&#51217;&#49440;&#50616; + loss&#54632;&#49688;&#45716; torch.nn.MSELoss() . What4=torch.tensor([-5.0,10.0],requires_grad=True) yhat4=X@What4 lossfn=torch.nn.MSELoss() loss4=lossfn(y,yhat4) loss4 . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . &#48169;&#48277;5: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=False) + loss&#54632;&#49688;&#45716; torch.nn.MSELoss() . net5=torch.nn.Linear(in_features=2,out_features=1,bias=False) net5.weight.data= torch.tensor([[-5.0,10.0]]) yhat5=net5(X) #lossfn=torch.nn.MSELoss() loss5=lossfn(y.reshape(100,1),yhat5) loss5 . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . &#48169;&#48277;6: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=True) + loss&#54632;&#49688;&#45716; torch.nn.MSELoss() . net6=torch.nn.Linear(in_features=1,out_features=1,bias=True) net6.weight.data= torch.tensor([[10.0]]) net6.bias.data= torch.tensor([[-5.0]]) yhat6=net6(x.reshape(100,1)) loss6=lossfn(y.reshape(100,1),yhat6) loss6 . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . step3: derivation . loss1 . loss1.backward() . What1.grad.data . tensor([-13.4225, 11.8893]) . 이것이 손계산을 통한 이론적인 미분값과 일치함은 이전시간에 확인하였음. | . loss2 . loss2.backward() . net2.weight.grad . tensor([[-13.4225, 11.8893]]) . loss3 . loss3.backward() . net3.bias.grad,net3.weight.grad . (tensor([[-13.4225]]), tensor([[11.8893]])) . loss4 . loss4.backward() . What4.grad.data . tensor([-13.4225, 11.8893]) . loss5 . loss5.backward() . net5.weight.grad . tensor([[-13.4225, 11.8893]]) . loss6 . loss6.backward() . net6.bias.grad,net6.weight.grad . (tensor([[-13.4225]]), tensor([[11.8893]])) . step4: update . loss1 . What1.data ## update 전 . tensor([-5., 10.]) . lr=0.1 What1.data = What1.data - lr*What1.grad.data ## update 후 What1 . tensor([-3.6577, 8.8111], requires_grad=True) . loss2 . net2.weight.data . tensor([[-5., 10.]]) . optmz2 = torch.optim.SGD(net2.parameters(),lr=0.1) . optmz2.step() ## update . net2.weight.data ## update 후 . tensor([[-3.6577, 8.8111]]) . loss3 . net3.bias.data,net3.weight.data . (tensor([[-5.]]), tensor([[10.]])) . optmz3 = torch.optim.SGD(net3.parameters(),lr=0.1) . optmz3.step() . net3.bias.data,net3.weight.data . (tensor([[-3.6577]]), tensor([[8.8111]])) . list(net3.parameters()) . [Parameter containing: tensor([[8.8111]], requires_grad=True), Parameter containing: tensor([[-3.6577]], requires_grad=True)] . loss4 . What4.data ## update 전 . tensor([-5., 10.]) . lr=0.1 What4.data = What4.data - lr*What4.grad.data ## update 후 What4 . tensor([-3.6577, 8.8111], requires_grad=True) . loss5 . net5.weight.data . tensor([[-5., 10.]]) . optmz5 = torch.optim.SGD(net5.parameters(),lr=0.1) . optmz5.step() ## update . net5.weight.data ## update 후 . tensor([[-3.6577, 8.8111]]) . loss6 . net6.bias.data,net6.weight.data . (tensor([[-5.]]), tensor([[10.]])) . optmz6 = torch.optim.SGD(net6.parameters(),lr=0.1) . optmz6.step() . net6.bias.data,net6.weight.data . (tensor([[-3.6577]]), tensor([[8.8111]])) . step1~4&#47484; &#48152;&#48373;&#54616;&#47732;&#46108;&#45796;. . net=torch.nn.Linear(in_features=2,out_features=1,bias=False) ## 모형정의 optmz=torch.optim.SGD(net.parameters(),lr=0.1) mseloss=torch.nn.MSELoss() for epoc in range(100): # step1: yhat yhat=net(X) ## yhat 계산 # step2: loss loss=mseloss(y.reshape(100,1),yhat) # step3: derivation loss.backward() # step4: update optmz.step() optmz.zero_grad() ## 외우세요.. . list(net.parameters()) . [Parameter containing: tensor([[2.4459, 4.0043]], requires_grad=True)] . &#49689;&#51228; . 아래를 실행해보고 결과를 관찰하라. . net=torch.nn.Linear(in_features=2,out_features=1,bias=False) ## 모형정의 optmz=torch.optim.SGD(net.parameters(),lr=0.1) mseloss=torch.nn.MSELoss() for epoc in range(100): # step1: yhat yhat=net(X) ## yhat 계산 # step2: loss loss=mseloss(y.reshape(100,1),yhat) # step3: derivation loss.backward() # step4: update optmz.step() .",
            "url": "https://guebin.github.io/2021BDA/2021/09/30/(4%EC%A3%BC%EC%B0%A8)-9%EC%9B%9430%EC%9D%BC.html",
            "relUrl": "/2021/09/30/(4%EC%A3%BC%EC%B0%A8)-9%EC%9B%9430%EC%9D%BC.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "(3주차) 9월28일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/3): 9월14-16일 강의노트의 일부내용 추가설명 . - (2/3): torch.nn.Linear()를 사용하여 yhat을 계산하기, torch.nn.MSELoss()를 이용하여 loss를 계산하기 . - (3/3): 과제설명 . Import . import torch import numpy as np . Data . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(43052) n=100 ones= torch.ones(n) x,_ = torch.randn(n).sort() X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ ytrue = X@W . &#51060;&#51204;&#48169;&#48277;&#50836;&#50557; . - step1: yhat . - step2: loss . - step3: derivation . - step4: update . step1: yhat . - feedforward 신경망을 설계하는 과정 . - 이 단계가 잘 완료되었다면, 임의의 ${ bf hat{W}}$을 넣었을 때 $ bf hat{y}$를 계산할 수 있어야 함 . &#48169;&#48277;1: &#51649;&#51217;&#49440;&#50616; (&#45236;&#44032; &#44277;&#49885;&#51012; &#50508;&#44256; &#51080;&#50612;&#50556; &#54620;&#45796;) . What=torch.tensor([-5.0,10.0],requires_grad=True) . yhat1=X@What . yhat1 . tensor([-29.8211, -28.6215, -24.9730, -21.2394, -19.7919, -19.6354, -19.5093, -19.4352, -18.7223, -18.0793, -16.9040, -16.0918, -16.0536, -15.8746, -14.4690, -14.3193, -13.6426, -12.8578, -12.5486, -12.4213, -11.9484, -11.1034, -10.8296, -10.6210, -10.5064, -10.0578, -9.8063, -9.7380, -9.7097, -9.6756, -8.8736, -8.7195, -8.6880, -8.1592, -7.7752, -7.7716, -7.7339, -7.7208, -7.6677, -7.1551, -7.0004, -6.8163, -6.7081, -6.5655, -6.4480, -6.3612, -6.0566, -5.6031, -5.5589, -5.2137, -4.3446, -4.3165, -3.8047, -3.5801, -3.4793, -3.4325, -2.3545, -2.3440, -1.8434, -1.7799, -1.5386, -1.0161, -0.8103, 0.4426, 0.5794, 0.9125, 1.1483, 1.4687, 1.4690, 1.5234, 1.6738, 2.0592, 2.1414, 2.8221, 3.1536, 3.6682, 4.2907, 4.8037, 4.8531, 4.9414, 5.3757, 5.3926, 5.6973, 6.0239, 6.1261, 6.5317, 7.2891, 8.4032, 8.4936, 9.2794, 9.9943, 10.0310, 10.4369, 11.7886, 15.8323, 17.4440, 18.9350, 21.0560, 21.0566, 21.6324], grad_fn=&lt;MvBackward&gt;) . &#48169;&#48277;2: torch.nn.Linear() &#49324;&#50857; . net = torch.nn.Linear(in_features=2 ,out_features=1, bias=False) . net.weight.data . tensor([[0.3320, 0.1982]]) . net.weight.data=torch.tensor([[-5.0,10.0]]) . net.weight.data . tensor([[-5., 10.]]) . net(X) . tensor([[-29.8211], [-28.6215], [-24.9730], [-21.2394], [-19.7919], [-19.6354], [-19.5093], [-19.4352], [-18.7223], [-18.0793], [-16.9040], [-16.0918], [-16.0536], [-15.8746], [-14.4690], [-14.3193], [-13.6426], [-12.8578], [-12.5486], [-12.4213], [-11.9484], [-11.1034], [-10.8296], [-10.6210], [-10.5064], [-10.0578], [ -9.8063], [ -9.7380], [ -9.7097], [ -9.6756], [ -8.8736], [ -8.7195], [ -8.6880], [ -8.1592], [ -7.7752], [ -7.7716], [ -7.7339], [ -7.7208], [ -7.6677], [ -7.1551], [ -7.0004], [ -6.8163], [ -6.7081], [ -6.5655], [ -6.4480], [ -6.3612], [ -6.0566], [ -5.6031], [ -5.5589], [ -5.2137], [ -4.3446], [ -4.3165], [ -3.8047], [ -3.5801], [ -3.4793], [ -3.4325], [ -2.3545], [ -2.3440], [ -1.8434], [ -1.7799], [ -1.5386], [ -1.0161], [ -0.8103], [ 0.4426], [ 0.5794], [ 0.9125], [ 1.1483], [ 1.4687], [ 1.4690], [ 1.5234], [ 1.6738], [ 2.0592], [ 2.1414], [ 2.8221], [ 3.1536], [ 3.6682], [ 4.2907], [ 4.8037], [ 4.8531], [ 4.9414], [ 5.3757], [ 5.3926], [ 5.6973], [ 6.0239], [ 6.1261], [ 6.5317], [ 7.2891], [ 8.4032], [ 8.4936], [ 9.2794], [ 9.9943], [ 10.0310], [ 10.4369], [ 11.7886], [ 15.8323], [ 17.4440], [ 18.9350], [ 21.0560], [ 21.0566], [ 21.6324]], grad_fn=&lt;MmBackward&gt;) . yhat2=net(X) . &#48169;&#48277;3: torch.nn.Linear()&#49324;&#50857;, bias=True . net = torch.nn.Linear(in_features=1 ,out_features=1, bias=True) . net.weight.data . tensor([[0.3480]]) . net.weight.data=torch.tensor([[10.0]]) . net.bias.data=torch.tensor([-5.0]) . net.weight,net.bias . (Parameter containing: tensor([[10.]], requires_grad=True), Parameter containing: tensor([-5.], requires_grad=True)) . net(x.reshape(100,1)) . tensor([[-29.8211], [-28.6215], [-24.9730], [-21.2394], [-19.7919], [-19.6354], [-19.5093], [-19.4352], [-18.7223], [-18.0793], [-16.9040], [-16.0918], [-16.0536], [-15.8746], [-14.4690], [-14.3193], [-13.6426], [-12.8578], [-12.5486], [-12.4213], [-11.9484], [-11.1034], [-10.8296], [-10.6210], [-10.5064], [-10.0578], [ -9.8063], [ -9.7380], [ -9.7097], [ -9.6756], [ -8.8736], [ -8.7195], [ -8.6880], [ -8.1592], [ -7.7752], [ -7.7716], [ -7.7339], [ -7.7208], [ -7.6677], [ -7.1551], [ -7.0004], [ -6.8163], [ -6.7081], [ -6.5655], [ -6.4480], [ -6.3612], [ -6.0566], [ -5.6031], [ -5.5589], [ -5.2137], [ -4.3446], [ -4.3165], [ -3.8047], [ -3.5801], [ -3.4793], [ -3.4325], [ -2.3545], [ -2.3440], [ -1.8434], [ -1.7799], [ -1.5386], [ -1.0161], [ -0.8103], [ 0.4426], [ 0.5794], [ 0.9125], [ 1.1483], [ 1.4687], [ 1.4690], [ 1.5234], [ 1.6738], [ 2.0592], [ 2.1414], [ 2.8221], [ 3.1536], [ 3.6682], [ 4.2907], [ 4.8037], [ 4.8531], [ 4.9414], [ 5.3757], [ 5.3926], [ 5.6973], [ 6.0239], [ 6.1261], [ 6.5317], [ 7.2891], [ 8.4032], [ 8.4936], [ 9.2794], [ 9.9943], [ 10.0310], [ 10.4369], [ 11.7886], [ 15.8323], [ 17.4440], [ 18.9350], [ 21.0560], [ 21.0566], [ 21.6324]], grad_fn=&lt;AddmmBackward&gt;) . . step2: loss . &#48169;&#48277;1: &#49552;&#49892;&#54632;&#49688;&#47484; &#51649;&#51217;&#51221;&#51032;&#54616;&#45716; &#48169;&#48277; . loss=torch.mean((y-yhat1)**2) loss . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . loss=torch.mean((y-yhat2)**2) loss . tensor(176.2661, grad_fn=&lt;MeanBackward0&gt;) . 176.2661? 이건 잘못된 결과임 | . loss=torch.mean((y.reshape(100,1)-yhat2)**2) loss . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;2: torch.nn.MSELoss()&#47484; &#49324;&#50857;&#54616;&#50668; &#49552;&#49892;&#54632;&#49688;&#47484; &#51221;&#51032;&#54616;&#45716; &#48169;&#48277; . lossfn=torch.nn.MSELoss() . loss=lossfn(y,yhat1) loss . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . loss=lossfn(y.reshape(100,1),yhat2) loss . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . &#49689;&#51228; . - model: $y_i= w_0+w_1 x_{i1}+w_2 x_{i2} + epsilon_i = 2.5 + 4x_{1i} + -2x_{2i}+ epsilon_i, quad i=1,2, dots,n$ . torch.manual_seed(43052) n=100 ones= torch.ones(n) x1,_ = torch.randn(n).sort() x2,_ = torch.randn(n).sort() X = torch.vstack([ones,x1,x2]).T W = torch.tensor([2.5,4,-2]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ ytrue = X@W . X . tensor([[ 1.0000, -2.4821, -2.3721], [ 1.0000, -2.3621, -2.3032], [ 1.0000, -1.9973, -2.2271], [ 1.0000, -1.6239, -2.0301], [ 1.0000, -1.4792, -1.9157], [ 1.0000, -1.4635, -1.8241], [ 1.0000, -1.4509, -1.6696], [ 1.0000, -1.4435, -1.6675], [ 1.0000, -1.3722, -1.4723], [ 1.0000, -1.3079, -1.4405], [ 1.0000, -1.1904, -1.4111], [ 1.0000, -1.1092, -1.3820], [ 1.0000, -1.1054, -1.3803], [ 1.0000, -1.0875, -1.3456], [ 1.0000, -0.9469, -1.3255], [ 1.0000, -0.9319, -1.2860], [ 1.0000, -0.8643, -1.2504], [ 1.0000, -0.7858, -1.2095], [ 1.0000, -0.7549, -1.1498], [ 1.0000, -0.7421, -1.1151], [ 1.0000, -0.6948, -1.0980], [ 1.0000, -0.6103, -1.0609], [ 1.0000, -0.5830, -0.9825], [ 1.0000, -0.5621, -0.9672], [ 1.0000, -0.5506, -0.9396], [ 1.0000, -0.5058, -0.9208], [ 1.0000, -0.4806, -0.8768], [ 1.0000, -0.4738, -0.7517], [ 1.0000, -0.4710, -0.7091], [ 1.0000, -0.4676, -0.7027], [ 1.0000, -0.3874, -0.6918], [ 1.0000, -0.3719, -0.6561], [ 1.0000, -0.3688, -0.6153], [ 1.0000, -0.3159, -0.5360], [ 1.0000, -0.2775, -0.4784], [ 1.0000, -0.2772, -0.3936], [ 1.0000, -0.2734, -0.3763], [ 1.0000, -0.2721, -0.3283], [ 1.0000, -0.2668, -0.3227], [ 1.0000, -0.2155, -0.2860], [ 1.0000, -0.2000, -0.2842], [ 1.0000, -0.1816, -0.2790], [ 1.0000, -0.1708, -0.2472], [ 1.0000, -0.1565, -0.2199], [ 1.0000, -0.1448, -0.2170], [ 1.0000, -0.1361, -0.1952], [ 1.0000, -0.1057, -0.1886], [ 1.0000, -0.0603, -0.1829], [ 1.0000, -0.0559, -0.1447], [ 1.0000, -0.0214, -0.0723], [ 1.0000, 0.0655, -0.0667], [ 1.0000, 0.0684, -0.0625], [ 1.0000, 0.1195, -0.0539], [ 1.0000, 0.1420, -0.0356], [ 1.0000, 0.1521, 0.0306], [ 1.0000, 0.1568, 0.0783], [ 1.0000, 0.2646, 0.1328], [ 1.0000, 0.2656, 0.1925], [ 1.0000, 0.3157, 0.2454], [ 1.0000, 0.3220, 0.2519], [ 1.0000, 0.3461, 0.3517], [ 1.0000, 0.3984, 0.3816], [ 1.0000, 0.4190, 0.3831], [ 1.0000, 0.5443, 0.3850], [ 1.0000, 0.5579, 0.4247], [ 1.0000, 0.5913, 0.4431], [ 1.0000, 0.6148, 0.4589], [ 1.0000, 0.6469, 0.4709], [ 1.0000, 0.6469, 0.4711], [ 1.0000, 0.6523, 0.4944], [ 1.0000, 0.6674, 0.4969], [ 1.0000, 0.7059, 0.5234], [ 1.0000, 0.7141, 0.5614], [ 1.0000, 0.7822, 0.5874], [ 1.0000, 0.8154, 0.5899], [ 1.0000, 0.8668, 0.6259], [ 1.0000, 0.9291, 0.6296], [ 1.0000, 0.9804, 0.7098], [ 1.0000, 0.9853, 0.7154], [ 1.0000, 0.9941, 0.7437], [ 1.0000, 1.0376, 0.7786], [ 1.0000, 1.0393, 0.8346], [ 1.0000, 1.0697, 0.8432], [ 1.0000, 1.1024, 0.8558], [ 1.0000, 1.1126, 0.8803], [ 1.0000, 1.1532, 0.9951], [ 1.0000, 1.2289, 1.0430], [ 1.0000, 1.3403, 1.0580], [ 1.0000, 1.3494, 1.0685], [ 1.0000, 1.4279, 1.1723], [ 1.0000, 1.4994, 1.2669], [ 1.0000, 1.5031, 1.3621], [ 1.0000, 1.5437, 1.3738], [ 1.0000, 1.6789, 1.4183], [ 1.0000, 2.0832, 1.4193], [ 1.0000, 2.2444, 1.5095], [ 1.0000, 2.3935, 1.6424], [ 1.0000, 2.6056, 1.8131], [ 1.0000, 2.6057, 2.0058], [ 1.0000, 2.6632, 2.2810]]) . - torch.nn.Linear() 를 이용하여 $ bf{ hat{W}}= begin{bmatrix}1 1 1 end{bmatrix}$ 에 대한 $ hat{y}$를 구하라. .",
            "url": "https://guebin.github.io/2021BDA/2021/09/28/(3%EC%A3%BC%EC%B0%A8)-9%EC%9B%9428%EC%9D%BC.html",
            "relUrl": "/2021/09/28/(3%EC%A3%BC%EC%B0%A8)-9%EC%9B%9428%EC%9D%BC.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "(2주차) 9월14일, 9월16일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/5) 회귀모형 소개, 손실 함수 . - (2/5) 경사하강법, 경사하강법을 이용하여 회귀계수 1회 업데이트 . - (3/5) 회귀계수 반복 업데이트 . - (4/5) 학습률 . - (5/5) 사과영상 . import . import torch import numpy as np import matplotlib.pyplot as plt . &#47196;&#46300;&#47605; . - 회귀분석 $ to$ 로지스틱 $ to$ 심층신경망(DNN) $ to$ 합성곱신경망(CNN) . Data . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(43052) n=100 ones= torch.ones(n) x,_ = torch.randn(n).sort() X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ ytrue = X@W . plt.plot(x,y,&#39;o&#39;) plt.plot(x,ytrue,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa8326671f0&gt;] . &#54617;&#49845;&#51060;&#46976;? . - 파란점만 주어졌을때, 주황색 점선을 추론하는것. 좀 더 정확하게 말하면 given data로 $ begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$를 최대한 $ begin{bmatrix} 2.5 4 end{bmatrix}$와 비슷하게 찾는것. . given data : $ big {(x_i,y_i) big }_{i=1}^{n}$ . | parameter: ${ bf W}= begin{bmatrix} w_0 w_1 end{bmatrix}$ . | estimated parameter: ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$ . | . - 더 쉽게 말하면 아래의 그림을 보고 적당한 추세선을 찾는것이다. . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa831e07b80&gt;] . - 시도: $( hat{w}_0, hat{w}_1)=(-5,10)$을 선택하여 선을 그려보고 적당한지 판단. . $ hat{y}_i=-5 +10 x_i$ 와 같이 $y_i$의 값을 적합시키겠다는 의미 | . plt.plot(x,y,&#39;o&#39;) plt.plot(x,-5+10*x,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa8318d3400&gt;] . - 벡터표현으로 주황색점선을 계산 . What=torch.tensor([-5.0,10.0]) plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@What,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa831252070&gt;] . &#54028;&#46972;&#47700;&#53552;&#47484; &#54617;&#49845;&#54616;&#45716; &#48169;&#48277; (&#51201;&#45817;&#54620; &#49440;&#51004;&#47196; &#50629;&#45936;&#51060;&#53944; &#54616;&#45716; &#48169;&#48277;) . - 이론적으로 추론 &lt;- 회귀분석시간에 배운것 . - 컴퓨터의 반복계산을 이용하여 추론 (경사하강법) &lt;- 우리가 오늘 파이토치로 실습해볼 내용. . (1) initial value: 임의의 선을 일단 그어본다. . What= torch.tensor([-5.0,10.0],requires_grad=True) What . tensor([-5., 10.], requires_grad=True) . 처음에는 ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}= begin{bmatrix} -5 10 end{bmatrix} $ 를 대입해서 주황색 점선을 적당히 그려보자는 의미 . | 끝에 requires_grad=True는 나중에 미분을 위한 것 . | . yhat=X@What yhat . tensor([-29.8211, -28.6215, -24.9730, -21.2394, -19.7919, -19.6354, -19.5093, -19.4352, -18.7223, -18.0793, -16.9040, -16.0918, -16.0536, -15.8746, -14.4690, -14.3193, -13.6426, -12.8578, -12.5486, -12.4213, -11.9484, -11.1034, -10.8296, -10.6210, -10.5064, -10.0578, -9.8063, -9.7380, -9.7097, -9.6756, -8.8736, -8.7195, -8.6880, -8.1592, -7.7752, -7.7716, -7.7339, -7.7208, -7.6677, -7.1551, -7.0004, -6.8163, -6.7081, -6.5655, -6.4480, -6.3612, -6.0566, -5.6031, -5.5589, -5.2137, -4.3446, -4.3165, -3.8047, -3.5801, -3.4793, -3.4325, -2.3545, -2.3440, -1.8434, -1.7799, -1.5386, -1.0161, -0.8103, 0.4426, 0.5794, 0.9125, 1.1483, 1.4687, 1.4690, 1.5234, 1.6738, 2.0592, 2.1414, 2.8221, 3.1536, 3.6682, 4.2907, 4.8037, 4.8531, 4.9414, 5.3757, 5.3926, 5.6973, 6.0239, 6.1261, 6.5317, 7.2891, 8.4032, 8.4936, 9.2794, 9.9943, 10.0310, 10.4369, 11.7886, 15.8323, 17.4440, 18.9350, 21.0560, 21.0566, 21.6324], grad_fn=&lt;MvBackward&gt;) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa8310cae80&gt;] . (2) 첫번째 수정: 적당한 선의 &#39;적당한 정도&#39;를 판단하고 더 적당한 선으로 업데이트 한다. . - &#39;적당한 정도&#39;를 판단하기 위한 장치: loss function 도입! . $loss= sum_{i=1}^{n}(y_i- hat{y}_i)^2= sum_{i=1}^{n}(y_i-( hat{w}_0+ hat{w}_1x_i))^2$ . $=({ bf y}-{ bf hat{y}})^ top({ bf y}-{ bf hat{y}})=({ bf y}-{ bf X}{ bf hat{W}})^ top({ bf y}-{ bf X}{ bf hat{W}})$ . - loss 함수의 특징 . $y_i approx hat{y}_i$ 일수록 loss값이 작다. | $y_i approx hat{y}_i$ 이 되도록 $( hat{w}_0, hat{w}_1)$을 잘 찍으면 loss값이 작다. | (중요) 주황색 점선이 &#39;적당할 수록&#39; loss값이 작다. | . loss=torch.sum((y-yhat)**2) loss . tensor(8587.6875, grad_fn=&lt;SumBackward0&gt;) . - 우리의 목표: 이 loss(=8587.6875)을 더 줄이자. $ to$ 아예 모든 조합 $( hat{w}_0, hat{w}_1)$에 대하여 가장 작은 loss를 찾으면 좋겠다. . - 문제의 치환: 생각해보니까 우리의 문제는 아래와 같이 수학적으로 단순화 되었다. . 적당해보이는 주황색 선을 찾자 $ to$ $loss(w_0,w_1)$를 최소로하는 $(w_0,w_1)$의 값을 찾자. | . - 수정된 목표: $loss(w_0,w_1)$를 최소로 하는 $(w_0,w_1)$을 구하라. . 단순한 수학문제가 되었다. 마치 $loss(w)=w^2-2w+3$ 을 최소화하는 $w$를 찾으라는 것과 같음. | . - 우리의 무기: 경사하강법, 벡터미분 . . ($ ast$) &#51104;&#49884; &#44221;&#49324;&#54616;&#44053;&#48277;&#51012; &#47532;&#48624;&#54616;&#51088;. . 경사하강법 아이디어 (1차원) . (step 1) 임의의 점을 찍는다. . (step 2) 그 점에서 순간기울기를 구한다. (접선) &lt;-- 미분 . (step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 움직인다. (순간기울기와 같은 방향으로 움직이면 점점 커질테니까) . (팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 조절한다. . 경사하강법 아이디어 (2차원) . - 경사하강법 아이디어 (1차원) . (step 1) 임의의 점을 찍는다. . (step 2) 그 점에서 순간기울기를 구한다. (접평면) &lt;-- 편미분 . (step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 각각 움직인다. (순간기울기와 같은 방향으로 움직이면 점점 커질테니까) . (팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 각각 조절한다. . loss를 줄이도록 ${ bf W}$를 개선하는 방법 . - $수정값 leftarrow 원래값 - 기울어진크기(=미분계수) times alpha $ . 여기에서 $ alpha$는 전체적인 보폭의 크기를 결정한다. 즉 $ alpha$값이 클수록 한번의 update에 움직이는 양이 크다. | . - ${ bf W} leftarrow { bf W} - alpha times frac{ partial}{ partial { bf W}}loss(w_0,w_1)$ . 마이너스의 의미: 기울기의 부호를 보고 반대방향으로 움직여라. . | $ frac{ partial}{ partial { bf W}}loss(w_0,w_1):$ 기울기의 절대값 크기와 비례하여 움직이는 정도를 조정하라. . | $ alpha$의 의미: 전체적인 보폭의 속도를 조절, $ alpha$가 크면 전체적으로 빠르게 움직인다. 다리의 길이로 비유할 수 있다. . | . . - 우리의 목표: loss=8587.6875 인데, 이걸 줄이는 것이 목표라고 했었음. 이것을 줄이는 방법이 경사하강법이다. . - 경사하강법으로 loss를 줄이기 위해서는 $ frac{ partial}{ partial { bf W}}loss(w_0,w_1)$의 계산이 필요한데, 이를 위해서 벡터미분이 필요하다. . loss.backward() . 미분해라! 뭘로? requires_grad=True를 가진 텐서로!!loss=torch.sum((y-yhat)**2)= torch.sum((y-X@What)**2) # 이었고 What=torch.tensor([-5.0,10.0],requires_grad=True) # 이므로 결국 What으로 미분하라는 의미. # 미분한 식이 나오는 것이 아니고, # 그 식에 (-5.0, 10.0)을 대입한 계수값이 계산됨. . | . 정확하게 말하면 미분을 활용하여 $(-5,10)$에서의 순간기울기를 구했다는 의미임. | . What.grad.data . tensor([-1342.2523, 1188.9307]) . 이것이 의미하는건 $(-5,10)$에서의 순간기울기가 $(-1342.2523, 1188.9307)$ 이라는 의미 | . - 잘계산한것이 맞는가? 손계산으로 검증하여 보자. . $loss(w_0,w_1)=(y- hat{y})^ top (y- hat{y})=(y-XW)^ top (y-XW)$ . | $ frac{ partial}{ partial W}loss(w_0,w_1)=-2X^ top y+2X^ top X W$ . | . - 2 * X.T @ y + 2 * X.T @ X @ What . tensor([-1342.2522, 1188.9305], grad_fn=&lt;AddBackward0&gt;) . alpha=0.001 print(&#39;수정전: &#39; + str(What.data)) print(&#39;수정하는폭: &#39; +str(-alpha * What.grad.data)) print(&#39;수정후: &#39; +str(What.data-alpha * What.grad.data)) print(&#39;*참값: (2.5,4)&#39; ) . 수정전: tensor([-5., 10.]) 수정하는폭: tensor([ 1.3423, -1.1889]) 수정후: tensor([-3.6577, 8.8111]) *참값: (2.5,4) . Wbefore = What.data Wafter = What.data-alpha * What.grad.data Wbefore, Wafter . (tensor([-5., 10.]), tensor([-3.6577, 8.8111])) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@Wbefore,&#39;--&#39;,color=&#39;b&#39;) #수정전: 파란점선 plt.plot(x,X@Wafter,&#39;--&#39;,color=&#39;r&#39;) #수정후: 빨간점선 plt.title(&quot;before: blue // after: red&quot;) . Text(0.5, 1.0, &#39;before: blue // after: red&#39;) . (3) Learn (=estimate $ bf hat{W})$: . What= torch.tensor([-5.0,10.0],requires_grad=True) . alpha=0.001 for epoc in range(30): What.grad=None yhat=X@What loss=torch.sum((y-yhat)**2) loss.backward() What.data = What.data-alpha * What.grad.data . What.data ## true: (2.5,4) . tensor([2.4290, 4.0144]) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@What.data),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa8310056a0&gt;] . &#54028;&#46972;&#47700;&#53552;&#51032; &#49688;&#51221;&#44284;&#51221;&#51012; &#44288;&#52272;&#54624; &#49688; &#50630;&#51012;&#44620;? (&#54617;&#49845;&#44284;&#51221; &#47784;&#45768;&#53552;&#47553;) . - 기록을 해보자. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . What= torch.tensor([-5.0,10.0],requires_grad=True) alpha=0.001 for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . - $ hat{y}$ 관찰 . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[3],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa830fd2f40&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[10],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa830f31b50&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[15],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fa830f07c70&gt;] . - $ hat{ bf W}$ . Whats . [[-5.0, 10.0], [-3.657747745513916, 8.81106948852539], [-2.554811716079712, 7.861191749572754], [-1.649186372756958, 7.101552963256836], [-0.9060714244842529, 6.49347448348999], [-0.29667866230010986, 6.006272315979004], [0.2027742564678192, 5.615575313568115], [0.6119104623794556, 5.302003383636475], [0.9469034671783447, 5.050129413604736], [1.2210699319839478, 4.847657680511475], [1.4453645944595337, 4.684779167175293], [1.6287915706634521, 4.553659439086914], [1.778746247291565, 4.448036193847656], [1.90129816532135, 4.3628973960876465], [2.0014259815216064, 4.294229507446289], [2.0832109451293945, 4.238814353942871], [2.149996757507324, 4.194070339202881], [2.204521894454956, 4.157923698425293], [2.249027729034424, 4.128708839416504], [2.285348415374756, 4.105085849761963], [2.31498384475708, 4.0859761238098145], [2.339160442352295, 4.070511341094971], [2.3588807582855225, 4.057991027832031], [2.3749637603759766, 4.0478515625], [2.3880786895751953, 4.039637088775635], [2.3987717628479004, 4.032979965209961], [2.40748929977417, 4.027583599090576], [2.414595603942871, 4.023208141326904], [2.4203879833221436, 4.019659042358398], [2.4251089096069336, 4.016779899597168]] . plt.plot(losses) . [&lt;matplotlib.lines.Line2D at 0x7fa830e58be0&gt;] . Animation . plt.rcParams[&#39;figure.figsize&#39;] = (10,4) plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . from matplotlib import animation fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect $ alpha$&#50640; &#45824;&#54616;&#50668; ($ alpha$&#45716; &#54617;&#49845;&#47456;) . (1) $ alpha$가 너무 작다면? $ to$ 비효율적이다. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.0001 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (2) $ alpha$가 크다면? $ to$ 다른의미에서 비효율적이다 + 위험하다.. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.0083 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (3) $ alpha=0.0085$ . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.0085 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (4) $ alpha=0.01$ . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.01 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#49689;&#51228; . - 학습률($ alpha$)를 조정하며 실습해보고 스크린샷 제출 . &#45796;&#47336;&#44592; &#49899;&#51648;&#47564; &#54644;&#50556;&#54616;&#45716; &#49324;&#49548;&#54620; &#47928;&#51228;&#46308; . (A1) &#49552;&#49892;&#54632;&#49688; . - $ sum_{i=1}^{n}(y_i- hat{y}_i)^2$ 대신에 . $ frac{1}{n} sum_{i=1}^{n}(y_i- hat{y}_i)^2$ | $ frac{1}{2n} sum_{i=1}^{n}(y_i- hat{y}_i)^2$ | . 중 하나를 사용하여도 상관없다. . (A2) &#48324;&#54364;&#47196; &#54364;&#49884;&#46108; &#51216;&#51060; &#51221;&#47568; $(2.5,4.0)$&#51068;&#44620;? $ Longleftrightarrow$ l&#51060; &#51221;&#47568; $w_0=2.5$, $w_1=4.0$&#50640;&#49436; &#52572;&#49548;&#54868; &#46104;&#45716;&#44032;? . - np.argmin 소개 . _a=np.array([0,2,5,2,3,4]) np.argmin(_a) . 0 . np.argmin(l) . 598 . 이건 무슨 값이지?? . - 왜 이런일이 생기는가? . _X=np.array([[1,6,3],[1,-5,5]]) . _X . array([[ 1, 6, 3], [ 1, -5, 5]]) . np.argmin(_X) . 4 . - array의 구조가 너무 컴퓨터 위주의 숫자임.. $ to$ np.unravel_index() 함수사용 . np.unravel_index(4,_X.shape) . (1, 1) . - 이것을 응용하면 . np.unravel_index(np.argmin(l),l.shape) . (17, 20) . _w0[17],_w1[20] . (2.5, 4.0) . - (2.5,4.0)에서 l이 최소값을 가지는 것이 맞긴함 . - 그런데 이론적으로 그래야 하는 것은 아님. . torch.sum((y-2.5-4.0*x)**2) . tensor(26.6494) . XX=np.matrix(X) yy=np.matrix(y).T . (XX.T*XX).I * XX.T * yy . matrix([[2.4458692], [4.004343 ]], dtype=float32) . torch.sum((y-2.4458692-4.004343*x)**2) . tensor(26.3600) . 진짜로 (2.4458692,4.004343) 에서의 로스가 더 작음 | . - $n$이 커질수록 (2.4458692, 4.004343) 의 값은 점점 (2.5,4.0)의 값에 가까워 진다. . (A3) &#54665;&#48289;&#53552;&#50752; &#50676;&#48289;&#53552; . - 아래의 매트릭스를 관찰하자. . XX . matrix([[ 1. , -2.482113 ], [ 1. , -2.3621461 ], [ 1. , -1.9972954 ], [ 1. , -1.6239362 ], [ 1. , -1.4791915 ], [ 1. , -1.4635365 ], [ 1. , -1.450925 ], [ 1. , -1.4435216 ], [ 1. , -1.3722302 ], [ 1. , -1.3079282 ], [ 1. , -1.1903973 ], [ 1. , -1.109179 ], [ 1. , -1.1053556 ], [ 1. , -1.0874591 ], [ 1. , -0.94689655], [ 1. , -0.9319339 ], [ 1. , -0.8642649 ], [ 1. , -0.78577816], [ 1. , -0.7548619 ], [ 1. , -0.74213064], [ 1. , -0.6948388 ], [ 1. , -0.610345 ], [ 1. , -0.5829591 ], [ 1. , -0.56210476], [ 1. , -0.55064297], [ 1. , -0.50577736], [ 1. , -0.48062643], [ 1. , -0.4737953 ], [ 1. , -0.47096547], [ 1. , -0.46755713], [ 1. , -0.3873588 ], [ 1. , -0.37194738], [ 1. , -0.3687963 ], [ 1. , -0.31592152], [ 1. , -0.27751535], [ 1. , -0.27715707], [ 1. , -0.27338728], [ 1. , -0.27207515], [ 1. , -0.2667671 ], [ 1. , -0.21550845], [ 1. , -0.20004053], [ 1. , -0.18163072], [ 1. , -0.17081414], [ 1. , -0.1565458 ], [ 1. , -0.14479806], [ 1. , -0.13611706], [ 1. , -0.10566129], [ 1. , -0.06031348], [ 1. , -0.05588722], [ 1. , -0.02136729], [ 1. , 0.06554431], [ 1. , 0.06835173], [ 1. , 0.11953046], [ 1. , 0.14198998], [ 1. , 0.15207446], [ 1. , 0.15675156], [ 1. , 0.26455274], [ 1. , 0.26559785], [ 1. , 0.3156574 ], [ 1. , 0.32201108], [ 1. , 0.346143 ], [ 1. , 0.39839193], [ 1. , 0.4189721 ], [ 1. , 0.5442578 ], [ 1. , 0.557936 ], [ 1. , 0.591254 ], [ 1. , 0.61482644], [ 1. , 0.64686656], [ 1. , 0.64689904], [ 1. , 0.6523392 ], [ 1. , 0.6673753 ], [ 1. , 0.7059195 ], [ 1. , 0.7141374 ], [ 1. , 0.78221494], [ 1. , 0.8153611 ], [ 1. , 0.8668233 ], [ 1. , 0.9290748 ], [ 1. , 0.98036987], [ 1. , 0.9853081 ], [ 1. , 0.99413556], [ 1. , 1.0375688 ], [ 1. , 1.039256 ], [ 1. , 1.0697267 ], [ 1. , 1.1023871 ], [ 1. , 1.112612 ], [ 1. , 1.1531745 ], [ 1. , 1.2289088 ], [ 1. , 1.3403202 ], [ 1. , 1.3493598 ], [ 1. , 1.4279404 ], [ 1. , 1.4994265 ], [ 1. , 1.503098 ], [ 1. , 1.5436871 ], [ 1. , 1.6788615 ], [ 1. , 2.083233 ], [ 1. , 2.2444 ], [ 1. , 2.393501 ], [ 1. , 2.6056044 ], [ 1. , 2.605658 ], [ 1. , 2.66324 ]], dtype=float32) . - 두번째 col을 선택하고 싶다. . XX[:,1] . matrix([[-2.482113 ], [-2.3621461 ], [-1.9972954 ], [-1.6239362 ], [-1.4791915 ], [-1.4635365 ], [-1.450925 ], [-1.4435216 ], [-1.3722302 ], [-1.3079282 ], [-1.1903973 ], [-1.109179 ], [-1.1053556 ], [-1.0874591 ], [-0.94689655], [-0.9319339 ], [-0.8642649 ], [-0.78577816], [-0.7548619 ], [-0.74213064], [-0.6948388 ], [-0.610345 ], [-0.5829591 ], [-0.56210476], [-0.55064297], [-0.50577736], [-0.48062643], [-0.4737953 ], [-0.47096547], [-0.46755713], [-0.3873588 ], [-0.37194738], [-0.3687963 ], [-0.31592152], [-0.27751535], [-0.27715707], [-0.27338728], [-0.27207515], [-0.2667671 ], [-0.21550845], [-0.20004053], [-0.18163072], [-0.17081414], [-0.1565458 ], [-0.14479806], [-0.13611706], [-0.10566129], [-0.06031348], [-0.05588722], [-0.02136729], [ 0.06554431], [ 0.06835173], [ 0.11953046], [ 0.14198998], [ 0.15207446], [ 0.15675156], [ 0.26455274], [ 0.26559785], [ 0.3156574 ], [ 0.32201108], [ 0.346143 ], [ 0.39839193], [ 0.4189721 ], [ 0.5442578 ], [ 0.557936 ], [ 0.591254 ], [ 0.61482644], [ 0.64686656], [ 0.64689904], [ 0.6523392 ], [ 0.6673753 ], [ 0.7059195 ], [ 0.7141374 ], [ 0.78221494], [ 0.8153611 ], [ 0.8668233 ], [ 0.9290748 ], [ 0.98036987], [ 0.9853081 ], [ 0.99413556], [ 1.0375688 ], [ 1.039256 ], [ 1.0697267 ], [ 1.1023871 ], [ 1.112612 ], [ 1.1531745 ], [ 1.2289088 ], [ 1.3403202 ], [ 1.3493598 ], [ 1.4279404 ], [ 1.4994265 ], [ 1.503098 ], [ 1.5436871 ], [ 1.6788615 ], [ 2.083233 ], [ 2.2444 ], [ 2.393501 ], [ 2.6056044 ], [ 2.605658 ], [ 2.66324 ]], dtype=float32) . 정상적을 잘 선택되었다. | . - 이제 XX에서 첫번째 row를 선택하고 싶다면? . XX[0,:] . matrix([[ 1. , -2.482113]], dtype=float32) . - X에 관심을 가져보자. . - 첫번째 row를 뽑고싶다면? . X[0,:] . tensor([ 1.0000, -2.4821]) . - 두번째 col을 뽑고 싶다면? . X[:,1] . tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435, -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319, -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621, -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719, -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155, -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603, -0.0559, -0.0214, 0.0655, 0.0684, 0.1195, 0.1420, 0.1521, 0.1568, 0.2646, 0.2656, 0.3157, 0.3220, 0.3461, 0.3984, 0.4190, 0.5443, 0.5579, 0.5913, 0.6148, 0.6469, 0.6469, 0.6523, 0.6674, 0.7059, 0.7141, 0.7822, 0.8154, 0.8668, 0.9291, 0.9804, 0.9853, 0.9941, 1.0376, 1.0393, 1.0697, 1.1024, 1.1126, 1.1532, 1.2289, 1.3403, 1.3494, 1.4279, 1.4994, 1.5031, 1.5437, 1.6789, 2.0832, 2.2444, 2.3935, 2.6056, 2.6057, 2.6632]) . - shape을 비교하여 보자. . XX.shape, (XX[0,:]).shape, (XX[:,1]).shape . ((100, 2), (1, 2), (100, 1)) . 이게 상식적임 | . X.shape, (X[0,:]).shape, (X[:,1]).shape . (torch.Size([100, 2]), torch.Size([2]), torch.Size([100])) . row-vec, col-vec의 구분없이 그냥 길이2인 벡터, 길이가 100인 벡터로 고려됨 | row-vec, col-vec의 구분을 하려면 2차원이 필요한데 1차원으로 축소가 되면서 생기는 현상 | 대부분의 경우 별로 문제가 되지 않음. | 수학적으로는 col-vec, row-vec를 엄밀하게 구분하는 것이 좋지만, 프로그래밍 효율을 생각하면 떄로는 구분이 모호한게 유리할 수도 있다. | .",
            "url": "https://guebin.github.io/2021BDA/2021/09/16/(2-3%EC%A3%BC%EC%B0%A8)-9%EC%9B%9414,-9%EC%9B%9416%EC%9D%BC.html",
            "relUrl": "/2021/09/16/(2-3%EC%A3%BC%EC%B0%A8)-9%EC%9B%9414,-9%EC%9B%9416%EC%9D%BC.html",
            "date": " • Sep 16, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "(2주차) 9월9일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/4) Path 설명 . - (2/4) 이미지 크롤링 . - (3/4) 모형학습 및 결과분석 . - (4/4) 테스트 . import . from fastai.data.all import * from fastai.vision.all import * . Path . - 기능: 현재폴더, 혹은 그 하위폴더들에 속한 파일의 목록을 볼 수 있다. . path=Path() # Path클래스에서 인스턴스생성 . (path/&#39;ghtop_images&#39;).ls() . (#2) [Path(&#39;ghtop_images/token.png&#39;),Path(&#39;ghtop_images/sparknb.gif&#39;)] . - Path(...)에서 ...에 무엇을 넣느냐에 따라 원하는 경로를 설정할 수 있다. . path=Path(&#39;/home&#39;) . path.ls() . (#1) [Path(&#39;/home/cgb4&#39;)] . - 폴더를 만들수 있다. . path=Path() . (path/&#39;asdf&#39;).mkdir() . (path/&#39;asdf&#39;).ls() . (#0) [] . - 이미 폴더가 존재할 때는 아래와 같이 에러가 발생 . (path/&#39;asdf&#39;).mkdir() . FileExistsError Traceback (most recent call last) /tmp/ipykernel_258436/283275367.py in &lt;module&gt; -&gt; 1 (path/&#39;asdf&#39;).mkdir() ~/anaconda3/envs/bda2021/lib/python3.8/pathlib.py in mkdir(self, mode, parents, exist_ok) 1286 self._raise_closed() 1287 try: -&gt; 1288 self._accessor.mkdir(self, mode) 1289 except FileNotFoundError: 1290 if not parents or self.parent == self: FileExistsError: [Errno 17] File exists: &#39;asdf&#39; . (path/&#39;asdf&#39;).mkdir(exist_ok=True) . - 생성한 폴더를 지우는 방법 . (path/&#39;asdf&#39;).rmdir() . &#51060;&#48120;&#51648; &#53356;&#47204;&#47553; . - 이미지 크롤링은 (1) 검색 (2) 이미지 주소를 찾음 (3) 해당주소로 이동하여 저장하는 과정을 반복하면 된다. . - 교재: 빙을 이용하여 이미지 크롤링 . 단점: 애져에 가입, 완전무료가 아님 (학생에게 1년간 무료) | . - 다른방법: 덕덕고를 이용한 이미지 크롤링 . ref: https://github.com/fastai/fastbook/blob/master/utils.py | . def search_images_ddg(key,max_n=200): &quot;&quot;&quot;Search for &#39;key&#39; with DuckDuckGo and return a unique urls of &#39;max_n&#39; images (Adopted from https://github.com/deepanprabhu/duckduckgo-images-api) &quot;&quot;&quot; url = &#39;https://duckduckgo.com/&#39; params = {&#39;q&#39;:key} res = requests.post(url,data=params) searchObj = re.search(r&#39;vqd=([ d-]+) &amp;&#39;,res.text) if not searchObj: print(&#39;Token Parsing Failed !&#39;); return requestUrl = url + &#39;i.js&#39; headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0&#39;} params = ((&#39;l&#39;,&#39;us-en&#39;),(&#39;o&#39;,&#39;json&#39;),(&#39;q&#39;,key),(&#39;vqd&#39;,searchObj.group(1)),(&#39;f&#39;,&#39;,,,&#39;),(&#39;p&#39;,&#39;1&#39;),(&#39;v7exp&#39;,&#39;a&#39;)) urls = [] while True: try: res = requests.get(requestUrl,headers=headers,params=params) data = json.loads(res.text) for obj in data[&#39;results&#39;]: urls.append(obj[&#39;image&#39;]) max_n = max_n - 1 if max_n &lt; 1: return L(set(urls)) # dedupe if &#39;next&#39; not in data: return L(set(urls)) requestUrl = url + data[&#39;next&#39;] except: pass . - search_images_ddg(검색어)를 이용하여 검색어에 해당하는 url을 얻는다. . search_images_ddg(&#39;hynn&#39;,max_n=5) . (#5) [&#39;https://yt3.ggpht.com/a/AGF-l7_1jF579BUaWHBEpY95iZAb0WI2SC4vykeo3A=s900-c-k-c0xffffffff-no-rj-mo&#39;,&#39;http://talkimg.imbc.com/TVianUpload/tvian/TViews/image/2020/03/21/GRMTjLNM9a88637203974033409433.jpg&#39;,&#39;https://images.genius.com/a37e8f087886e8a9f1f1d4d4d02aba44.960x960x1.jpg&#39;,&#39;https://www.nautiljon.com/images/people/01/59/hynn_99095.jpg?0&#39;,&#39;https://lastfm.freetls.fastly.net/i/u/770x0/f6744fc617da497938bf0560c82fe0d2.jpg#f6744fc617da497938bf0560c82fe0d2&#39;] . - download_images(저장하고싶은폴더위치, url의리스트)를 이용하여 url에 해당하는 이미지를 저장하고 싶은 폴더에 저장. . path=Path() . path.ls() . (#14) [Path(&#39;2021-09-06-cat2.jpeg&#39;),Path(&#39;2021-09-06-hani03.jpg&#39;),Path(&#39;2021-09-06-hani01.jpeg&#39;),Path(&#39;ghtop_images&#39;),Path(&#39;2021-09-07-(1주차) 9월7일.ipynb&#39;),Path(&#39;Untitled.ipynb&#39;),Path(&#39;2021-08-17-(A1) 깃허브와 fastpages를 이용하여 블로그 개설하기.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2021-09-02-(1주차) 9월2일.ipynb&#39;),Path(&#39;2021-09-06-cat1.png&#39;)...] . download_images(path,urls=search_images_ddg(&#39;hynn&#39;,max_n=5)) . 현재 working dir에 5개의 이미지가 저장된다. | . keywords = &#39;hynn&#39;, &#39;iu&#39; path=Path(&#39;singer&#39;) . if not path.exists(): # 현재폴더에 singer라는 폴더가 있는지 체크 path.mkdir() # 현재폴더에 singer라는 폴더가 만들어짐 for keyword in keywords: # keyword=&#39;hynn&#39;, keyword=&#39;iu&#39; 일때 아래내용을 반복 lastpath=path/keyword # ./singer/hynn or ./singer/iu lastpath.mkdir(exist_ok=True) # make ./singer/hynn or ./singer/iu urls=search_images_ddg(keyword) # &#39;hynn&#39; 검색어로 url들의 리스트를 얻음 download_images(lastpath,urls=urls) # 그 url에 해당하는 이미지들을 ./singer/hynn or ./singer/iu 에 저장 . Cleaning Data . - 탐색기로 파일들을 살펴보니 조금 이상한 확장자도 있음. . - 조금 이상해보이는 확장자도 열리기는 함. . PILImage.create(&#39;./singer/iu/00000006.jpg:large&#39;) . verify_images(get_image_files(path)) . (#4) [Path(&#39;singer/iu/00000041.jpg&#39;),Path(&#39;singer/iu/00000029.jpg&#39;),Path(&#39;singer/iu/00000125.jpg&#39;),Path(&#39;singer/hynn/00000077.png&#39;)] . - 위에 해당하는 이미지를 수동으로 지워줌. . - csv을 받았으면 df를 만들어야 하듯이, 이미지 파일들을 받았으면 dls를 만들어야 fastai가 지원하는 함수로 분석하기 좋다. . dls = ImageDataLoaders.from_folder( path, train=&#39;singer&#39;, valid_pct=0.2, item_tfms=Resize(224)) . dls.show_batch(max_n=16) . - 모형을 만들고 학습을 시키자. . learn=cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(7) . epoch train_loss valid_loss error_rate time . 0 | 1.069038 | 0.753938 | 0.264706 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.638990 | 0.531955 | 0.220588 | 00:04 | . 1 | 0.498534 | 0.338006 | 0.147059 | 00:04 | . 2 | 0.392531 | 0.268666 | 0.132353 | 00:04 | . 3 | 0.313377 | 0.214198 | 0.102941 | 00:04 | . 4 | 0.262075 | 0.227022 | 0.088235 | 00:04 | . 5 | 0.216234 | 0.228273 | 0.088235 | 00:04 | . 6 | 0.192656 | 0.218852 | 0.088235 | 00:04 | . learn.show_results(max_n=16) . &#50724;&#45813;&#48516;&#49437; . interp = Interpretation.from_learner(learn) interp.plot_top_losses(16) . - 수동으로 특정 observation에 대한 예측결과를 확인하여 보자. . dls.train_ds . (#272) [(PILImage mode=RGB size=960x960, TensorCategory(1)),(PILImage mode=RGB size=540x793, TensorCategory(1)),(PILImage mode=RGB size=800x1200, TensorCategory(1)),(PILImage mode=RGB size=720x960, TensorCategory(1)),(PILImage mode=RGB size=500x500, TensorCategory(0)),(PILImage mode=RGB size=1418x2000, TensorCategory(1)),(PILImage mode=RGB size=1920x1280, TensorCategory(1)),(PILImage mode=RGB size=480x360, TensorCategory(0)),(PILImage mode=RGB size=630x1045, TensorCategory(0)),(PILImage mode=RGB size=799x1200, TensorCategory(1))...] . training set | . dls.train_ds[0] . (PILImage mode=RGB size=960x960, TensorCategory(1)) . dls.train_ds[0] 가 의미하는 것은 첫번쨰 observation을 의미함. 즉 $(x_1,y_1)$ | $x_1=$PILImage mode=RGB size=960x960 | $y_1=$TensorCategory(1) | . dls.train_ds[210][0] . $x_{211}$=위의 이미지 | . dls.train_ds[210][1] . TensorCategory(0) . $y_{211}=$TensorCategory(0) | . x210=dls.train_ds[210][0] . learn.predict(x210) . (&#39;hynn&#39;, tensor(0), tensor([0.8893, 0.1107])) . Test . path = Path() . if not (path/&#39;test&#39;).exists(): (path/&#39;test&#39;).mkdir() . urls=search_images_ddg(&#39;hynn 박혜원&#39;,max_n=20) download_images(path/&#39;test&#39;,urls=urls) testset=get_image_files(path/&#39;test&#39;) testset . (#20) [Path(&#39;test/00000010.jpg&#39;),Path(&#39;test/00000005.jpg&#39;),Path(&#39;test/00000013.jpg&#39;),Path(&#39;test/00000011.jpg&#39;),Path(&#39;test/00000003.jpg&#39;),Path(&#39;test/00000000.jpg&#39;),Path(&#39;test/00000015.png&#39;),Path(&#39;test/00000004.jpg&#39;),Path(&#39;test/00000012.jpg&#39;),Path(&#39;test/00000006.jpg&#39;)...] . for i in range(len(testset)): print(learn.predict(PILImage.create(testset[i]))) . (&#39;hynn&#39;, tensor(0), tensor([1.0000e+00, 1.5190e-06])) . (&#39;hynn&#39;, tensor(0), tensor([0.9516, 0.0484])) . (&#39;hynn&#39;, tensor(0), tensor([0.9904, 0.0096])) . (&#39;hynn&#39;, tensor(0), tensor([9.9952e-01, 4.7845e-04])) . (&#39;hynn&#39;, tensor(0), tensor([0.9990, 0.0010])) . (&#39;hynn&#39;, tensor(0), tensor([0.9983, 0.0017])) . (&#39;hynn&#39;, tensor(0), tensor([0.9923, 0.0077])) . (&#39;iu&#39;, tensor(1), tensor([0.1120, 0.8880])) . (&#39;hynn&#39;, tensor(0), tensor([0.9949, 0.0051])) . (&#39;hynn&#39;, tensor(0), tensor([0.9982, 0.0018])) . (&#39;hynn&#39;, tensor(0), tensor([0.9940, 0.0060])) . (&#39;hynn&#39;, tensor(0), tensor([1.0000e+00, 8.8760e-07])) . (&#39;hynn&#39;, tensor(0), tensor([0.9963, 0.0037])) . (&#39;hynn&#39;, tensor(0), tensor([9.9975e-01, 2.5230e-04])) . (&#39;hynn&#39;, tensor(0), tensor([0.7672, 0.2328])) . (&#39;hynn&#39;, tensor(0), tensor([9.9982e-01, 1.8401e-04])) . (&#39;hynn&#39;, tensor(0), tensor([1.0000e+00, 3.9835e-06])) . (&#39;hynn&#39;, tensor(0), tensor([1.0000e+00, 6.5406e-07])) . (&#39;hynn&#39;, tensor(0), tensor([0.9253, 0.0747])) . (&#39;iu&#39;, tensor(1), tensor([0.1957, 0.8043])) . 결과를 보니까 hynn이 많음 $ to$ 어느정도 맞추는것 같긴하다. | . PILImage.create(testset[7]) . 실제로는 박혜원인데 아이유로 예측한 사진 | . path = Path() . if not (path/&#39;test2&#39;).exists(): (path/&#39;test2&#39;).mkdir() . urls=search_images_ddg(&#39;iu 아이유&#39;,max_n=20) download_images(path/&#39;test2&#39;,urls=urls) testset=get_image_files(path/&#39;test2&#39;) testset . (#20) [Path(&#39;test2/00000010.jpg&#39;),Path(&#39;test2/00000005.jpg&#39;),Path(&#39;test2/00000013.jpg&#39;),Path(&#39;test2/00000011.jpg&#39;),Path(&#39;test2/00000003.jpg&#39;),Path(&#39;test2/00000000.jpg&#39;),Path(&#39;test2/00000004.jpg&#39;),Path(&#39;test2/00000016.jpg&#39;),Path(&#39;test2/00000009.jpeg&#39;),Path(&#39;test2/00000012.jpg&#39;)...] . for i in range(len(testset)): print(learn.predict(PILImage.create(testset[i]))) . (&#39;iu&#39;, tensor(1), tensor([0.0051, 0.9949])) . (&#39;iu&#39;, tensor(1), tensor([8.7392e-06, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0895, 0.9105])) . (&#39;iu&#39;, tensor(1), tensor([0.0011, 0.9989])) . (&#39;iu&#39;, tensor(1), tensor([1.0321e-05, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0211, 0.9789])) . (&#39;iu&#39;, tensor(1), tensor([4.9877e-05, 9.9995e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0031, 0.9969])) . (&#39;iu&#39;, tensor(1), tensor([0.0011, 0.9989])) . (&#39;iu&#39;, tensor(1), tensor([1.5381e-05, 9.9998e-01])) . (&#39;iu&#39;, tensor(1), tensor([7.1447e-05, 9.9993e-01])) . (&#39;iu&#39;, tensor(1), tensor([1.3296e-04, 9.9987e-01])) . (&#39;hynn&#39;, tensor(0), tensor([0.9982, 0.0018])) . (&#39;iu&#39;, tensor(1), tensor([2.5169e-05, 9.9997e-01])) . (&#39;iu&#39;, tensor(1), tensor([1.2726e-05, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([7.9650e-05, 9.9992e-01])) . (&#39;iu&#39;, tensor(1), tensor([3.0283e-04, 9.9970e-01])) . (&#39;iu&#39;, tensor(1), tensor([6.8668e-05, 9.9993e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0034, 0.9966])) . (&#39;iu&#39;, tensor(1), tensor([0.0052, 0.9948])) . 결과를 보니 아이유 역시 잘 맞추는 듯 보인다. | . - 정확률이 아쉽긴 하지만 어느정도 유의미한 결과를 얻었다. . &#49689;&#51228; . - 원하는 검색어로 이미지를 모은 뒤 결과를 제출 .",
            "url": "https://guebin.github.io/2021BDA/2021/09/09/(2%EC%A3%BC%EC%B0%A8)-9%EC%9B%949%EC%9D%BC.html",
            "relUrl": "/2021/09/09/(2%EC%A3%BC%EC%B0%A8)-9%EC%9B%949%EC%9D%BC.html",
            "date": " • Sep 9, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "(1주차) 9월7일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/6): 아나콘다 가상환경 만들기, 파이토치 설치, 주피터랩 설치, conda install 과 pip install 의 차이 . - (2/6): 이미지 분석을 위한 데이터셋 준비 및 정리 . - (3/6): 학습 및 예측 . - (4/6): 코랩설명 + 깃허브/블로그 (뒷부분은 화면전환 오류로 설명이 부실함) . - (5/6): 코랩설명 + 깃허브/블로그 . - (6/6): 우리강아지 이미지를 활용한 예측, 과제설명 . import . from fastai.vision.all import * . &#45936;&#51060;&#53552;&#51200;&#51109;, &#45936;&#51060;&#53552;&#47196;&#45908;&#49828; &#49373;&#49457;&#54980; dls&#47196; &#51200;&#51109; . path=untar_data(URLs.PETS)/&#39;images&#39; . files=get_image_files(path) # 이미지파일들의 이름을 모두 복붙하여 리스트를 만든뒤에 files.txt로 저장하는 과정으로 비유할 수 있음 . files[2] # txt 파일의 3번째 목록 . Path(&#39;/home/cgb4/.fastai/data/oxford-iiit-pet/images/leonberger_5.jpg&#39;) . def label_func(f): if f[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . label_func(&#39;asdf&#39;) . &#39;dog&#39; . dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(224)) . dls.show_batch(max_n=16) . &#54617;&#49845; . learn=cnn_learner(dls,resnet34,metrics=error_rate) . Downloading: &#34;https://download.pytorch.org/models/resnet34-b627a593.pth&#34; to C: Users cgb/.cache torch hub checkpoints resnet34-b627a593.pth . ImportError Traceback (most recent call last) ~ AppData Local Temp/ipykernel_29964/2234347239.py in &lt;module&gt; -&gt; 1 learn=cnn_learner(dls,resnet34,metrics=error_rate) ~ anaconda3 envs bda2021 lib site-packages fastai vision learner.py in cnn_learner(dls, arch, normalize, n_out, pretrained, config, loss_func, opt_func, lr, splitter, cbs, metrics, path, model_dir, wd, wd_bn_bias, train_bn, moms, **kwargs) 177 if n_out is None: n_out = get_c(dls) 178 assert n_out, &#34;`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`&#34; --&gt; 179 model = create_cnn_model(arch, n_out, pretrained=pretrained, **kwargs) 180 181 splitter=ifnone(splitter, meta[&#39;split&#39;]) ~ anaconda3 envs bda2021 lib site-packages fastai vision learner.py in create_cnn_model(arch, n_out, pretrained, cut, n_in, init, custom_head, concat_pool, **kwargs) 141 &#34;Create custom convnet architecture&#34; 142 meta = model_meta.get(arch, _default_meta) --&gt; 143 body = create_body(arch, n_in, pretrained, ifnone(cut, meta[&#39;cut&#39;])) 144 if custom_head is None: 145 nf = num_features_model(nn.Sequential(*body.children())) ~ anaconda3 envs bda2021 lib site-packages fastai vision learner.py in create_body(arch, n_in, pretrained, cut) 63 def create_body(arch, n_in=3, pretrained=True, cut=None): 64 &#34;Cut off the body of a typically pretrained `arch` as determined by `cut`&#34; &gt; 65 model = arch(pretrained=pretrained) 66 _update_first_layer(model, n_in, pretrained) 67 #cut = ifnone(cut, cnn_config(arch)[&#39;cut&#39;]) ~ anaconda3 envs bda2021 lib site-packages torchvision models resnet.py in resnet34(pretrained, progress, **kwargs) 286 progress (bool): If True, displays a progress bar of the download to stderr 287 &#34;&#34;&#34; --&gt; 288 return _resnet(&#39;resnet34&#39;, BasicBlock, [3, 4, 6, 3], pretrained, progress, 289 **kwargs) 290 ~ anaconda3 envs bda2021 lib site-packages torchvision models resnet.py in _resnet(arch, block, layers, pretrained, progress, **kwargs) 260 model = ResNet(block, layers, **kwargs) 261 if pretrained: --&gt; 262 state_dict = load_state_dict_from_url(model_urls[arch], 263 progress=progress) 264 model.load_state_dict(state_dict) ~ anaconda3 envs bda2021 lib site-packages torch hub.py in load_state_dict_from_url(url, model_dir, map_location, progress, check_hash, file_name) 551 r = HASH_REGEX.search(filename) # r is Optional[Match[str]] 552 hash_prefix = r.group(1) if r else None --&gt; 553 download_url_to_file(url, cached_file, hash_prefix, progress=progress) 554 555 if _is_legacy_zip_format(cached_file): ~ anaconda3 envs bda2021 lib site-packages torch hub.py in download_url_to_file(url, dst, hash_prefix, progress) 436 if hash_prefix is not None: 437 sha256 = hashlib.sha256() --&gt; 438 with tqdm(total=file_size, disable=not progress, 439 unit=&#39;B&#39;, unit_scale=True, unit_divisor=1024) as pbar: 440 while True: ~ anaconda3 envs bda2021 lib site-packages tqdm notebook.py in __init__(self, *args, **kwargs) 240 unit_scale = 1 if self.unit_scale is True else self.unit_scale or 1 241 total = self.total * unit_scale if self.total else self.total --&gt; 242 self.container = self.status_printer(self.fp, total, self.desc, self.ncols) 243 self.container.pbar = proxy(self) 244 self.displayed = False ~ anaconda3 envs bda2021 lib site-packages tqdm notebook.py in status_printer(_, total, desc, ncols) 113 # Prepare IPython progress bar 114 if IProgress is None: # #187 #451 #558 #872 --&gt; 115 raise ImportError( 116 &#34;IProgress not found. Please update jupyter and ipywidgets.&#34; 117 &#34; See https://ipywidgets.readthedocs.io/en/stable&#34; ImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html . - 에러를 해결하기 위해서는 아래를 설치하면 된다. . !conda install -c conda-forge jupyterlab_widgets -y !conda install -c conda-forge ipywidgets -y !conda install -c conda-forge nodejs -y . - 위를 설치하고 커널을 재시작하면 정상적으로 모형이 만들어진다. . learn=cnn_learner(dls,resnet34,metrics=error_rate) . /home/cgb4/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /tmp/pip-req-build-pma2oi4d/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.148441 | 0.018828 | 0.006766 | 00:12 | . epoch train_loss valid_loss error_rate time . 0 | 0.040593 | 0.014769 | 0.002706 | 00:11 | . &#50696;&#52769; . learn.predict(files[0]) . (&#39;dog&#39;, tensor(1), tensor([6.1421e-07, 1.0000e+00])) . learn.show_results() . &#50724;&#45813;&#48516;&#49437; . interp = Interpretation.from_learner(learn) . interp.plot_top_losses(16) . &#51652;&#51676; &#51096;&#46104;&#45716;&#44172; &#47582;&#45716;&#44148;&#44032;? . PILImage.create(&#39;2021-09-06-cat1.png&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-cat1.png&#39;)) . (&#39;cat&#39;, tensor(0), tensor([1.0000e+00, 1.7844e-07])) . - 헷갈리는 고양이 사진인데 잘 구분한다. . PILImage.create(&#39;2021-09-06-cat2.jpeg&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-cat2.jpeg&#39;)) . (&#39;cat&#39;, tensor(0), tensor([9.9984e-01, 1.6283e-04])) . PILImage.create(&#39;2021-09-06-hani01.jpeg&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-hani01.jpeg&#39;)) . (&#39;dog&#39;, tensor(1), tensor([5.0984e-04, 9.9949e-01])) . PILImage.create(&#39;2021-09-06-hani02.jpeg&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-hani02.jpeg&#39;)) . (&#39;dog&#39;, tensor(1), tensor([7.1694e-06, 9.9999e-01])) . PILImage.create(&#39;2021-09-06-hani03.jpg&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-hani03.jpg&#39;)) . (&#39;dog&#39;, tensor(1), tensor([4.5399e-04, 9.9955e-01])) . &#45796;&#51020;&#49884;&#44036; . 이미지 크롤링 --&gt; 데이터셋트 --&gt; A,B 구분하는 모델 | . &#49689;&#51228; . 위의 사진들 이외에 사진들을 바탕으로 예측을 하는 모형구축. | 예측결과를 스샷으로 저장하여 제출 (이미지도 함께 스샷할것) | . &#49689;&#51228;&#52280;&#44256;&#51088;&#47308; . import PIL urls=&#39;https://t1.daumcdn.net/cfile/tistory/9925F03C5AD486B033&#39; urllib.request.urlretrieve(urls,&#39;temp.png&#39;) learn.predict(PILImage.create(&#39;temp.png&#39;)) . (&#39;dog&#39;, tensor(1), tensor([6.6117e-04, 9.9934e-01])) .",
            "url": "https://guebin.github.io/2021BDA/2021/09/07/(1%EC%A3%BC%EC%B0%A8)-9%EC%9B%947%EC%9D%BC.html",
            "relUrl": "/2021/09/07/(1%EC%A3%BC%EC%B0%A8)-9%EC%9B%947%EC%9D%BC.html",
            "date": " • Sep 7, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "(1주차) 9월2일",
            "content": "&#44053;&#51032;&#50689;&#49345; . - (1/3): 과목소개 . - (2/3): 카카오톡 채널 소개 . - (3/3): 텐서플로우-케라스 vs 파이토치-fastai, 과제안내 . . &#47112;&#54252;&#53944; . - 카카오톡 스샷제출 .",
            "url": "https://guebin.github.io/2021BDA/2021/09/02/(1%EC%A3%BC%EC%B0%A8)-9%EC%9B%942%EC%9D%BC.html",
            "relUrl": "/2021/09/02/(1%EC%A3%BC%EC%B0%A8)-9%EC%9B%942%EC%9D%BC.html",
            "date": " • Sep 2, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "(A1) 깃허브와 fastpages를 이용하여 블로그 개설하기",
            "content": "About this doc . - 본 포스트는 2021년 1학기 Python 입문 강의내용중 일부를 업로드 하였음. . - Github, fastpages를 사용하여 블로그를 개설하고 관리하는 방법에 대한 설명임. . .",
            "url": "https://guebin.github.io/2021BDA/2021/08/17/(A1)-%EA%B9%83%ED%97%88%EB%B8%8C%EC%99%80-fastpages%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EA%B0%9C%EC%84%A4%ED%95%98%EA%B8%B0.html",
            "relUrl": "/2021/08/17/(A1)-%EA%B9%83%ED%97%88%EB%B8%8C%EC%99%80-fastpages%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EA%B0%9C%EC%84%A4%ED%95%98%EA%B8%B0.html",
            "date": " • Aug 17, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "최규빈 . guebin@jbnu.ac.kr | 자연과학대학교 본관 205호 | 카카오톡 오픈채널 1 | . 2021년 2학기 종료후 폐쇄예정 &#8617; . |",
          "url": "https://guebin.github.io/2021BDA/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://guebin.github.io/2021BDA/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}