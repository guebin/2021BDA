{
  
    
        "post0": {
            "title": "(12주차) 11월30일",
            "content": "&#44053;&#51032;&#50689;&#49345; . &#44592;&#50872;&#44592;&#49548;&#47736; . &#44256;&#50836;&#49549;&#51032; &#50808;&#52840; . - https://www.youtube.com/watch?v=ouitOnaDtFY . - 중간에 한명이라도 잘못 말한다면.. . &#51221;&#51032; . - In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation. . &#51060;&#54644; . - 당연한것 아닌가? . 그레디언트 기반의 학습 (그레디언트 기반의 옵티마이저): 손실함수의 기울기를 통하여 업데이트 하는 방식 | 역전파: 손실함수의 기울기를 구하는 테크닉 (체인룰 + $ alpha$). 구체적으로는 (1) 손실함수를 여러단계로 쪼개고 (2) 각 단계의 미분값을 각각 구하고 (3) 그것들을 모두 곱하여 기울기를 계산한다. | 0 근처의 숫자를 계속 곱하면 터지거나 0으로 간다. (사실 안정적인 기울기가 나올 것이라고 생각하는것 자체가 사실 이상함) | . import numpy as np . grads = np.random.uniform(low=-2,high=2,size=100) grads . array([-1.2167476 , -1.12064134, 1.00393117, 1.40056194, 1.92882907, -0.09171322, 0.30572762, 1.56992117, 1.32243833, -0.92432003, -0.50248891, 1.66943431, -1.60460124, 0.35229854, 1.2274395 , 1.37389022, 0.29056595, 0.98707261, -1.2251744 , 1.80257753, 0.71537336, -1.15877464, -0.83494789, 1.64240574, -1.32446422, -0.89313588, -1.51753367, -0.98460436, -1.56692271, -1.46442158, 0.68353486, 1.72871458, -0.2368262 , -1.63323852, 0.2627509 , 0.5877175 , -0.00262332, 0.11535328, -1.50135131, 1.66543865, -1.89538258, -1.28429374, 1.02493065, -0.32715551, 1.30725127, -0.33500648, -1.71877426, 1.88252857, 0.06692171, -1.63322342, -1.74570482, 1.62121875, 1.97338714, -0.81678251, -0.85935999, -0.09854208, 0.49107484, -0.73930724, -0.90959205, -1.94458844, -0.29952569, 0.88053565, -0.02598754, 0.58950273, -0.74198508, 1.79651259, -0.19150611, 0.05414303, -0.69770824, 1.80695205, 1.70329315, -1.92801617, -0.08804641, -0.14812397, -0.34009265, 0.82033119, -0.50359616, -1.9665786 , 1.94747694, -0.00885402, -0.79864724, -0.6043695 , -0.41644225, 0.73273884, 0.6724115 , -1.52320254, -1.27154839, 0.25046107, 1.02911645, -1.67979034, -0.38374368, 1.32331248, 0.63000883, 1.46149912, 0.98874506, -1.97213467, -0.28766756, 0.11726286, 1.61055069, 1.06425548]) . grads.prod() . -4.8946716297614096e-17 . 기울기가 소멸함 | . grads = np.random.uniform(low=-5,high=5,size=100) grads.prod() . -2.0889172052036512e+33 . 기울기가 폭발함. | . grads = np.random.uniform(low=-1,high=3.5,size=100) grads.prod() . 16364179.47497398 . - 도깨비: 기울기가 소멸하기도 하고 터지기도 한다. . &#54644;&#44208;&#52293; (&#44592;&#50872;&#44592; &#49548;&#47736;&#50640; &#45824;&#54620; &#54644;&#44208;&#52293;) . . Note: 잘 정리되어있는것이 없어서 제 머리속에서 정리했습니다. 부정확할수도 있어요 . - 개념 . 데이터 $ to$ (아키텍처,손실함수,역전파,업데이트) | . - Multi-level hierarchy . 여러층을 쪼개서 학습하자 $ to$ 어떻게? 사전학습, 층벼학습 | 기울기소실문제를 해결하여 딥러닝을 유행시킨 태초의(?) 방법임. | 결국 입력자료를 바꾼뒤에 학습하는 형태 | . - Faster hardware . GPU를 중심으로 한 테크닉 | 근본적인 문제해결책은 아니라는 힌튼의 비판 | CPU를 쓸때보다 GPU를 쓰면 약간 더 깊은 모형을 학습할 수 있다 정도? | . - Residual networks . 훌륭한 접근법중 하나임 | 아키텍처를 변경하는 방법이지만, 사실상 손실함수를 부드럽게 만드는 기법으로 이해해도 된다. | 솟컷이라는 아키텍처를 추가하여 이리저리 실험해보니까 손실함수가 부드러워졌다. &lt; 이런게 아니고 | 손실함수를 부드럽게 하기 위해서는 층별의차이(residual)를 학습하는게 유리할 것 같다. 그런데 이것을 위한 효과를 주기 위해서는 단지 아키텍처에 숏만만 추가하면 되겠다. &lt; 이런 모티브였을 것이다. | . - Other activation functions . 렐루의 개발 | . - 배치정규화 . 어쩌다보니 되는것. | 배치정규화는 원래 공변량 쉬프트를 잡기 위한 방법임. 그런데 기울기 소멸에도 효과가 있음. 현재는 기울기소멸문제에 대한 해결책으로 빠짐없이 언급되고 있음. 2015년의 원래 논문에는 기울기소멸에 대한 언급은 없었음. (https://arxiv.org/pdf/1502.03167.pdf) | 심지어 배치정규화는 오버피팅을 잡는효과도 있음 (이것은 논문에 언급했음) | . - 기울기를 안구하면 안되나?` . 베이지안 최적화기법: (https://arxiv.org/pdf/1807.02811.pdf) $ to$ GPU를 어떻게 쓰지? $ to$ 느리다 | . &#52628;&#52380;&#49884;&#49828;&#53596; . import . import pandas as pd import torch from fastai.collab import * from fastai.tabular.all import * . Data (&#49884;&#48044;&#47112;&#51060;&#49496;) . df=pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021BDA/master/_notebooks/2021-11-30-recommend.csv&#39;) df . user item rating item_name . 0 1 | 15 | 1.084308 | 홍차5 | . 1 1 | 1 | 4.149209 | 커피1 | . 2 1 | 11 | 1.142659 | 홍차1 | . 3 1 | 5 | 4.033415 | 커피5 | . 4 1 | 4 | 4.078139 | 커피4 | . ... ... | ... | ... | ... | . 995 100 | 18 | 4.104276 | 홍차8 | . 996 100 | 17 | 4.164773 | 홍차7 | . 997 100 | 14 | 4.026915 | 홍차4 | . 998 100 | 4 | 0.838720 | 커피4 | . 999 100 | 7 | 1.094826 | 커피7 | . 1000 rows × 4 columns . user-item matrix . - 아래와 같은 매트릭스를 고려하자. . df2 = pd.DataFrame([[None]*20]*100,columns=[&#39;커피&#39;+str(i) for i in range(1,11)]+[&#39;홍차&#39;+str(i) for i in range(1,11)]) df2.index = pd.Index([&#39;user&#39;+str(i) for i in range(1,101)]) df2 . 커피1 커피2 커피3 커피4 커피5 커피6 커피7 커피8 커피9 커피10 홍차1 홍차2 홍차3 홍차4 홍차5 홍차6 홍차7 홍차8 홍차9 홍차10 . user1 None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . user2 None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . user3 None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . user4 None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . user5 None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . user96 None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . user97 None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . user98 None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . user99 None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . user100 None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . 100 rows × 20 columns . for (i,j) in zip(df.user.to_list(), df.item.to_list()): df2.iloc[i-1,j-1]=df.query(&#39;user == @i and item == @j&#39;)[&#39;rating&#39;].to_list()[0] . df2.iloc[:5] . 커피1 커피2 커피3 커피4 커피5 커피6 커피7 커피8 커피9 커피10 홍차1 홍차2 홍차3 홍차4 홍차5 홍차6 홍차7 홍차8 홍차9 홍차10 . user1 4.149209 | None | None | 4.078139 | 4.033415 | 4.071871 | None | None | None | None | 1.142659 | 1.109452 | None | 0.603118 | 1.084308 | None | 0.906524 | None | None | 0.903826 | . user2 4.031811 | None | None | 3.822704 | None | None | None | 4.07141 | 3.996206 | None | None | 0.839565 | 1.011315 | None | 1.120552 | 0.91134 | None | 0.860954 | 0.871482 | None | . user3 4.082178 | 4.196436 | None | 3.956876 | None | None | None | 4.450931 | 3.97209 | None | None | None | None | 0.983838 | None | 0.918576 | 1.206796 | 0.913116 | None | 0.956194 | . user4 None | 4.000621 | 3.89557 | None | 3.838781 | 3.967183 | None | None | None | 4.105741 | 1.147554 | None | 1.34686 | None | 0.614099 | 1.297301 | None | None | None | 1.147545 | . user5 None | None | None | None | 3.888208 | None | 3.97033 | 3.97949 | None | 4.010982 | None | 0.920995 | 1.081111 | 0.999345 | None | 1.195183 | None | 0.818332 | 1.236331 | None | . user1~user5는 대체로 커피를 선호한다. | user1은 커피 2,3,7,8,9,10을 먹지는 않았는데, 아마 먹었다면 높은 점수를 주었을 것임. | . df2.iloc[47:53] . 커피1 커피2 커피3 커피4 커피5 커피6 커피7 커피8 커피9 커피10 홍차1 홍차2 홍차3 홍차4 홍차5 홍차6 홍차7 홍차8 홍차9 홍차10 . user48 None | None | 3.718709 | None | None | None | 4.484278 | 4.242403 | None | 3.654447 | 0.981285 | 1.089422 | 0.945297 | None | 1.367151 | 0.984146 | None | 0.779323 | None | None | . user49 4.012018 | 4.347253 | 3.752401 | None | None | None | 4.182699 | 3.763614 | 4.112049 | None | None | 0.921155 | 0.947049 | 0.937485 | None | None | 1.046802 | None | None | None | . user50 4.208589 | None | 3.995654 | 3.85952 | 4.186735 | None | None | 3.955416 | 4.017231 | 4.312376 | None | None | 1.148465 | None | None | None | 1.2535 | None | None | 0.985061 | . user51 None | None | 1.158754 | None | None | 0.961621 | 0.958685 | 0.986134 | 1.207339 | 0.799213 | None | 4.053459 | None | None | 4.214456 | 3.94822 | None | 4.168223 | None | None | . user52 0.794371 | None | None | None | None | 0.72175 | None | 0.99541 | None | None | None | None | 4.713239 | 4.061688 | 3.791844 | 4.008471 | 3.96703 | 4.420732 | 4.055309 | None | . user53 None | 0.586384 | 1.170372 | 0.939492 | None | None | 1.299762 | None | None | 0.818039 | 4.155103 | 4.131003 | None | 3.839847 | 3.859583 | None | None | 3.873978 | None | None | . user1 $ sim$ user50은 커피를 선호하고 user51 $ sim$ user100은 홍차를 선호함 | . df2.iloc[94:] . 커피1 커피2 커피3 커피4 커피5 커피6 커피7 커피8 커피9 커피10 홍차1 홍차2 홍차3 홍차4 홍차5 홍차6 홍차7 홍차8 홍차9 홍차10 . user95 1.190142 | None | 0.818878 | None | 1.150194 | None | None | 1.287971 | None | 0.586373 | None | None | 4.06455 | 4.399665 | None | None | 3.573139 | 3.944602 | None | 4.012274 | . user96 0.511905 | 1.066144 | None | 1.31543 | None | 1.285778 | None | 0.6784 | 1.02302 | 0.886803 | None | 4.055996 | None | None | 4.156489 | 4.127622 | None | None | None | None | . user97 None | 1.035022 | None | 1.085834 | None | 0.812558 | None | 1.074543 | None | 0.852806 | 3.894772 | None | 4.071385 | 3.935935 | None | None | 3.989815 | None | None | 4.267142 | . user98 None | 1.115511 | None | 1.101395 | 0.878614 | None | None | None | 1.329319 | None | 4.12519 | None | 4.354638 | 3.811209 | 4.144648 | None | None | 4.116915 | 3.887823 | None | . user99 None | 0.850794 | None | None | 0.927884 | 0.669895 | None | None | 0.665429 | 1.387329 | None | None | 4.329404 | 4.111706 | 3.960197 | None | None | None | 3.725288 | 4.122072 | . user100 None | None | 1.413968 | 0.83872 | None | None | 1.094826 | 0.987888 | None | 1.177387 | 3.957383 | 4.136731 | None | 4.026915 | None | None | 4.164773 | 4.104276 | None | None | . &#50696;&#52769;? . - None의 값을 추론하는 방법? 머리속으로는 알고있음. 그런데 어떻게 수식화 할지? . df2.loc[&#39;user1&#39;] . 커피1 4.149209 커피2 None 커피3 None 커피4 4.078139 커피5 4.033415 커피6 4.071871 커피7 None 커피8 None 커피9 None 커피10 None 홍차1 1.142659 홍차2 1.109452 홍차3 None 홍차4 0.603118 홍차5 1.084308 홍차6 None 홍차7 0.906524 홍차8 None 홍차9 None 홍차10 0.903826 Name: user1, dtype: object . - 유저의 취향은 예를들면 아래와 같이 표현할 수 있다. . user1의 취향 = (커피좋아함, 홍차싫어함) = (0.8, 0.21) | . - 코딩해보자. . user1_prprnc = (0.8,0.21) user1_prprnc . (0.8, 0.21) . - 여기에서 (0.8,0.21)은 각각 (커피음료,홍차음료)와 같은 음료의 특징으로 해석가능. 현재는 단순하게 원소가 2인 벡터이지만 다양한 feature도 가능하다. . 음료의특징: (커피음료, 홍차음료, 우유포함, 설탕포함정도, 아이스여부, 행사여부) | . - 아이템4와 아이템11의 특징은 예를들면 아래와 같이 표현할 수 있다. . item4_ftr = (0.71, 0.03) # 커피4 item11_ftr = (0.1, 0.88) # 홍차1 . - 유저의 선호도와 아이템의 feature를 내적하여보자. . np.array(user1_prprnc) @ np.array(item4_ftr) . 0.5742999999999999 . np.array(user1_prprnc) @ np.array(item11_ftr) . 0.26480000000000004 . - 유저의 선호도와 item의 feature가 비슷할수록 값이 높게 나온다. 실제로 user1이 커피4와 홍차1에 매긴 평점은 (4.078139,1.142659) 인데 (0.5743, 0.2648) 가 실제평점과 비슷하게 나오도록 하면 좋겠다. . - 적당히 값을 조정하여 보자. . user1_prprnc = (0.8,0.21) item4_ftr = (5, 0.03) # 커피4 item11_ftr = (0.1, 4.8) # 홍차1 . np.array(user1_prprnc) @ np.array(item4_ftr) . 4.0063 . np.array(user1_prprnc) @ np.array(item11_ftr) . 1.088 . - 신기한것은 이 파라메터들이 있다면 None에 대한 추론도 가능하다는 것이다. 예를들어서 user2는 홍차1을 먹지 않았지만 item11_ftr(홍차1)의 계수값이 다른유저를 통해 학습되었다면 user2가 홍차1을 먹고 내릴 평점이 예측가능하다. . df2.loc[&#39;user2&#39;] . 커피1 4.031811 커피2 None 커피3 None 커피4 3.822704 커피5 None 커피6 None 커피7 None 커피8 4.07141 커피9 3.996206 커피10 None 홍차1 None 홍차2 0.839565 홍차3 1.011315 홍차4 None 홍차5 1.120552 홍차6 0.91134 홍차7 None 홍차8 0.860954 홍차9 0.871482 홍차10 None Name: user2, dtype: object . user2_prprnc = (0.77,0.18) . np.array(user2_prprnc) @ np.array(item11_ftr) . 0.9410000000000001 . - 생각해보니까 user1_prprnc, ... , user100_prprnc, item1_ftr, ... , item20_ftr를 ${ bf W}$라고 해석하면 우리가 ${ bf W}$ 바꿈에 따라 예측값과 손실이 결정되므로 네트워크 구조가 만들어진다. . - 정리하면 적당한 아래와 같은 매트릭스를 정의하여 . ${ bf U} = begin{bmatrix} tt user1~ prprnc dots tt user100~prprnc end{bmatrix}$ $ quad 100 times 2$ mat | . ${ bf V} = begin{bmatrix} tt item1~ ftr dots tt item20~ftr end{bmatrix}$ $ quad 20 times 2$ mat | . ${ bf U}{ bf V}^ top approx $ df2 가 만족하도록 하면 된다. (None은 무시) . fastai&#47484; &#54617;&#49845;: df $ to$ dls . dls=CollabDataLoaders.from_df(df,bs=100) # df의 형식: #첫번째열은 사용자이 인덱스, #두번째열은 아이템의 인덱스, #세번쨰열은 rating, #네번째열은 아이템의 이름 . dls.items . user item rating item_name . 405 41 | 2 | 3.870817 | 커피2 | . 65 7 | 6 | 3.680798 | 커피6 | . 988 99 | 15 | 3.960197 | 홍차5 | . 842 85 | 2 | 1.184458 | 커피2 | . 227 23 | 10 | 4.035678 | 커피10 | . ... ... | ... | ... | ... | . 512 52 | 8 | 0.995410 | 커피8 | . 620 63 | 2 | 0.907010 | 커피2 | . 231 24 | 2 | 4.431236 | 커피2 | . 252 26 | 12 | 1.127234 | 홍차2 | . 580 59 | 3 | 1.198462 | 커피3 | . 800 rows × 4 columns . fastai&#47484; &#53685;&#54620; &#54617;&#49845;: learn . lrnr = collab_learner(dls,n_factors=2,y_range=(0,5)) lrnr.fit(30,0.01) . epoch train_loss valid_loss time . 0 | 2.332310 | 2.324714 | 00:00 | . 1 | 2.301398 | 2.314132 | 00:00 | . 2 | 2.255456 | 2.235110 | 00:00 | . 3 | 2.180693 | 2.062851 | 00:00 | . 4 | 2.068622 | 1.799302 | 00:00 | . 5 | 1.918429 | 1.467824 | 00:00 | . 6 | 1.734515 | 1.099332 | 00:00 | . 7 | 1.529240 | 0.753254 | 00:00 | . 8 | 1.318664 | 0.473745 | 00:00 | . 9 | 1.118887 | 0.281612 | 00:00 | . 10 | 0.941217 | 0.170759 | 00:00 | . 11 | 0.790015 | 0.113465 | 00:00 | . 12 | 0.664468 | 0.087310 | 00:00 | . 13 | 0.561132 | 0.076604 | 00:00 | . 14 | 0.476078 | 0.072097 | 00:00 | . 15 | 0.405709 | 0.069820 | 00:00 | . 16 | 0.347135 | 0.068991 | 00:00 | . 17 | 0.298197 | 0.068708 | 00:00 | . 18 | 0.257201 | 0.068535 | 00:00 | . 19 | 0.222736 | 0.068374 | 00:00 | . 20 | 0.193690 | 0.067657 | 00:00 | . 21 | 0.169164 | 0.066795 | 00:00 | . 22 | 0.148427 | 0.065751 | 00:00 | . 23 | 0.130861 | 0.064602 | 00:00 | . 24 | 0.115963 | 0.064182 | 00:00 | . 25 | 0.103277 | 0.063416 | 00:00 | . 26 | 0.092521 | 0.062831 | 00:00 | . 27 | 0.083378 | 0.062791 | 00:00 | . 28 | 0.075562 | 0.062564 | 00:00 | . 29 | 0.068923 | 0.061822 | 00:00 | . lrnr.show_results() . user item rating rating_pred . 0 64.0 | 4.0 | 1.193997 | 1.018805 | . 1 65.0 | 14.0 | 3.956821 | 3.988665 | . 2 59.0 | 2.0 | 1.161738 | 0.927356 | . 3 36.0 | 6.0 | 4.028756 | 4.073104 | . 4 78.0 | 10.0 | 0.968873 | 0.834417 | . 5 1.0 | 17.0 | 0.906524 | 0.973807 | . 6 96.0 | 6.0 | 1.285778 | 0.798828 | . 7 91.0 | 10.0 | 1.453194 | 1.013060 | . 8 25.0 | 10.0 | 4.342545 | 4.081783 | . - 학습이 잘 되었음. . &#52628;&#52380;: &#47560;&#51648;&#47561;&#50976;&#51200;&#50640; &#45824;&#54620; &#50696;&#52769;&#44208;&#44284; (&#50500;&#47560; &#54861;&#52264;&#47484; &#51339;&#50500;&#54624;&#46319;) . X,y = dls.one_batch() . X[:5] . tensor([[94, 3], [46, 12], [61, 19], [47, 11], [87, 20]]) . y[:5] . tensor([[1.1616], [0.6749], [4.1603], [0.6241], [4.3710]]) . torch.tensor([[94,3]]) . tensor([[94, 3]]) . lrnr.model(torch.tensor([[94,3]]).to(&quot;cuda:0&quot;)) . tensor([1.2002], device=&#39;cuda:0&#39;, grad_fn=&lt;AddBackward0&gt;) . x100 = torch.tensor([[100,j] for j in range(1,21) ]) . lrnr.model(x100.to(&quot;cuda:0&quot;)) . tensor([1.0416, 1.0414, 1.0742, 1.1004, 1.0125, 1.0141, 1.1448, 1.0488, 1.1155, 1.0428, 3.9658, 4.0532, 4.0562, 3.9919, 4.0497, 4.0288, 3.9983, 4.0984, 3.9943, 4.0803], device=&#39;cuda:0&#39;, grad_fn=&lt;AddBackward0&gt;) . - 예상대로 홍차를 매우 좋아함. .",
            "url": "https://guebin.github.io/2021BDA/2021/11/30/(12%EC%A3%BC%EC%B0%A8)-11%EC%9B%9430%EC%9D%BC.html",
            "relUrl": "/2021/11/30/(12%EC%A3%BC%EC%B0%A8)-11%EC%9B%9430%EC%9D%BC.html",
            "date": " • Nov 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "(11주차) 11월18일, 11월23일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/4) 체인룰을 이용한 미분계수 계산, 이론 (1) . - (2/4) 체인룰을 이용한 미분계수 계산, 이론 (2) . - (3/4) 체인룰을 이용한 미분계수 계산, 실습 . - (4/4) 과제설명 . - (5/8) 체인룰을 이용한 미분계수 계산, 실습 (2) . - (6/8) 체인룰을 이용한 미분계수 계산, 실습 (3) / back propagation (1) . - (7/8) back propagation (2) . - (8/8) some comments . import . import torch . &#51648;&#45212;&#49884;&#44036; &#48373;&#49845; . - 회귀분석에서 손실함수에 대한 미분은 아래와 같은 과정으로 계산할 수 있다. . $loss = ({ bf y}-{ bf X}{ bf W})^ top ({ bf y}-{ bf X}{ bf W})={ bf y}^ top { bf y} - { bf y}^ top { bf X}{ bf W} - { bf W}^ top { bf X}^ top { bf y} + { bf W}^ top { bf X}^ top { bf X} { bf W}$ | . $ frac{ partial }{ partial { bf W}}loss = -2{ bf X}^ top { bf y} +2 { bf X}^ top { bf X} { bf W}$ | . &#50724;&#45720;&#49688;&#50629;&#51032; &#47785;&#54364; . - 체인룰을 이해하자. . &#52404;&#51064;&#47344; (&#50612;&#47140;&#50868; &#54616;&#45208;&#51032; &#48120;&#48516;&#51012; &#49552;&#49772;&#50868; &#50668;&#47084;&#44060;&#51032; &#48120;&#48516;&#51004;&#47196; &#45208;&#45572;&#45716; &#44592;&#48277;) . - 손실함수가 사실 아래와 같은 변환을 거쳐서 계산되었다고 볼 수 있다. . ${ bf X} to { bf X}{ bf W} to { bf y} -{ bf X}{ bf W} to ({ bf y}-{ bf X}{ bf W})^ top ({ bf y}-{ bf X}{ bf W})$ | . - 위의 과정을 수식으로 정리해보면 아래와 같다. . ${ bf u}={ bf X}{ bf W}$, $ quad { bf u}: n times 1$ . | ${ bf v} = { bf y}- { bf u},$ $ quad { bf v}: n times 1$ . | $loss={ bf v}^ top { bf v},$ $ quad loss: 1 times 1 $ . | . - 손실함수에 대한 미분은 아래와 같다. . $$ frac{ partial }{ partial { bf W}} loss = frac{ partial }{ partial { bf W}} { bf v}^ top { bf v} $$ . (그런데 이걸 어떻게 계산함?) . - 계산할 수 있는것들의 모음.. . $ frac{ partial}{ partial { bf v}} loss = 2{ bf v} $ $ quad to$ (n,1) 벡터 | . $ frac{ partial }{ partial { bf u}} { bf v}^ top = -{ bf I}$ $ quad to $ (n,n) 매트릭스 | . $ frac{ partial }{ partial bf W}{ bf u}^ top = { bf X}^ top $ $ quad to$ (p,n) 매트릭스 | . - 혹시.. 아래와 같이 쓸 수 있을까? . $$ left( frac{ partial }{ partial bf W}{ bf u}^ top right) left( frac{ partial }{ partial bf u}{ bf v}^ top right) left( frac{ partial }{ partial bf v}loss right) = frac{ partial { bf u}^ top}{ partial bf W} frac{ partial { bf v}^ top}{ partial bf u} frac{ partial loss}{ partial bf v} $$ . 가능할것 같다. 뭐 기호야 정의하기 나름이니까! | . - 그렇다면 혹시 아래와 같이 쓸 수 있을까? . $$ frac{ partial { bf u}^ top}{ partial bf W} frac{ partial { bf v}^ top}{ partial bf u} frac{ partial loss}{ partial bf v} = frac{ partial loss }{ partial bf W}= frac{ partial }{ partial bf W} loss $$ 이건 선을 넘는 것임. | 그런데 어떠한 공식에 의해서 가능함. 그 공식 이름이 체인룰이다. | . - 결국 정리하면 아래의 꼴이 되었다. . $$ left( frac{ partial }{ partial bf W}{ bf u}^ top right) left( frac{ partial }{ partial bf u}{ bf v}^ top right) left( frac{ partial }{ partial bf v}loss right) = frac{ partial }{ partial bf W}loss $$ . - 그렇다면? . $$ left({ bf X}^ top right) left(-{ bf I} right) left(2{ bf v} right) = frac{ partial }{ partial bf W}loss $$ . 그런데, ${ bf v}={ bf y}-{ bf u}={ bf y} -{ bf X}{ bf W}$ 이므로 . $$-2{ bf X}^ top left({ bf y}-{ bf X}{ bf W} right) = frac{ partial }{ partial bf W}loss $$ . 정리하면 . $$ frac{ partial }{ partial bf W}loss = -2{ bf X}^ top{ bf y}+2{ bf X}^ top { bf X}{ bf W}$$ . &#50696;&#49884;: &#51473;&#44036;&#44256;&#49324; &#47928;&#51228;.. . - 미분계수를 계산하는 문제였음.. . - 체인룰을 이용하여 미분계수를 계산하여 보자. . ones= torch.ones(5) x = torch.tensor([11.0,12.0,13.0,14.0,15.0]) X = torch.vstack([ones,x]).T y = torch.tensor([17.7,18.5,21.2,23.6,24.2]) . W = torch.tensor([3.0,3.0]) . u = X@W v = y-u loss = v.T @ v . loss . tensor(2212.1799) . - $ frac{ partial}{ partial bf W}loss $ 의 계산 . X.T @ -torch.eye(5) @ (2*v) . tensor([ 209.6000, 2748.5999]) . - 참고로 중간고사 답은 . X.T @ -torch.eye(5)@ (2*v) / 5 . tensor([ 41.9200, 549.7200]) . 입니다. . - 확인 . _W = torch.tensor([3.0,3.0],requires_grad=True) . _loss = (y-X@_W).T @ (y-X@_W) . _loss.backward() . _W.grad.data . tensor([ 209.6000, 2748.5999]) . - $ frac{ partial}{ partial bf v} loss= 2{ bf v}$ 임을 확인하라. . v . tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000]) . _v= torch.tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000],requires_grad=True) . _loss = _v.T @ _v . _loss.backward() . _v.grad.data, v . (tensor([-36.6000, -41.0000, -41.6000, -42.8000, -47.6000]), tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000])) . - $ frac{ partial }{ partial { bf u}}{ bf v}^ top$ 의 계산 . _u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True) _u . tensor([36., 39., 42., 45., 48.], requires_grad=True) . _v = y - _u ### 이전의 _v와 또다른 임시 _v . (_v.T).backward() . RuntimeError Traceback (most recent call last) /tmp/ipykernel_2147125/3227738085.py in &lt;module&gt; -&gt; 1 (_v.T).backward() ~/anaconda3/envs/py38r40/lib/python3.8/site-packages/torch/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs) 305 create_graph=create_graph, 306 inputs=inputs) --&gt; 307 torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs) 308 309 def register_hook(self, hook): ~/anaconda3/envs/py38r40/lib/python3.8/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs) 148 149 grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors)) --&gt; 150 grad_tensors_ = _make_grads(tensors, grad_tensors_) 151 if retain_graph is None: 152 retain_graph = create_graph ~/anaconda3/envs/py38r40/lib/python3.8/site-packages/torch/autograd/__init__.py in _make_grads(outputs, grads) 49 if out.requires_grad: 50 if out.numel() != 1: &gt; 51 raise RuntimeError(&#34;grad can be implicitly created only for scalar outputs&#34;) 52 new_grads.append(torch.ones_like(out, memory_format=torch.preserve_format)) 53 else: RuntimeError: grad can be implicitly created only for scalar outputs . 사실 토치에서는 스칼라아웃풋에 대해서만 미분을 계산할 수 있음 | . 그런데 $ frac{ partial}{ partial { bf u}}{ bf v}^ top= frac{ partial}{ partial { bf u}}(v_1,v_2,v_3,v_4,v_5)= big( frac{ partial}{ partial { bf u}}v_1, frac{ partial}{ partial { bf u}}v_2, frac{ partial}{ partial { bf u}}v_3, frac{ partial}{ partial { bf u}}v_4, frac{ partial}{ partial { bf u}}v_5 big)$ 이므로 . 조금 귀찮은 과정을 거친다면 아래와 같은 알고리즘으로 계산할 수 있다. . (0) $ frac{ partial }{ partial { bf u}} { bf v}^ top$의 결과를 저장할 매트릭스를 만든다. 적당히 A라고 만들자. . (1) _u 하나를 임시로 만든다. 그리고 $v_1$을 _u로 미분하고 그 결과를 A의 첫번째 칼럼에 기록한다. . (2) _u를 또하나 임시로 만들고 $v_2$를 _u로 미분한뒤 그 결과를 A의 두번째 칼럼에 기록한다. . (3) (1)-(2)와 같은 작업을 $v_5$까지 반복한다. . (0)을 수행 . A = torch.zeros((5,5)) A . tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) . (1)을 수행 . u,v . (tensor([36., 39., 42., 45., 48.]), tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000])) . _u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True) v1 = (y-_u)[0] . 이때 $v_1=g(f({ bf u}))$와 같이 표현할 수 있다. 여기에서 $f((u_1, dots,u_5)^ top)=(y_1-u_1, dots,y_5-u_5)^ top$, 그리고 $g((v_1, dots,v_n)^ top)=v_1$ 라고 생각한다. 즉 $f$는 벡터 뺄셈을 수행하는 함수이고, $g$는 프로젝션 함수이다. 즉 $f: mathbb{R}^5 to mathbb{R}^5$인 함수이고, $g: mathbb{R}^5 to mathbb{R}$인 함수이다. | . v1.backward() . _u.grad.data . tensor([-1., -0., -0., -0., -0.]) . A[:,0]= _u.grad.data . A . tensor([[-1., 0., 0., 0., 0.], [-0., 0., 0., 0., 0.], [-0., 0., 0., 0., 0.], [-0., 0., 0., 0., 0.], [-0., 0., 0., 0., 0.]]) . (2)를 수행 . _u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True) v2 = (y-_u)[1] . v2.backward() . _u.grad.data . tensor([-0., -1., -0., -0., -0.]) . A[:,1]= _u.grad.data A . tensor([[-1., -0., 0., 0., 0.], [-0., -1., 0., 0., 0.], [-0., -0., 0., 0., 0.], [-0., -0., 0., 0., 0.], [-0., -0., 0., 0., 0.]]) . (3)을 수행 // 그냥 (1)~(2)도 새로 수행하자. . for i in range(5): _u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True) _v = (y-_u)[i] _v.backward() A[:,i]= _u.grad.data . A . tensor([[-1., -0., -0., -0., -0.], [-0., -1., -0., -0., -0.], [-0., -0., -1., -0., -0.], [-0., -0., -0., -1., -0.], [-0., -0., -0., -0., -1.]]) . 이론적인 결과인 $-{ bf I}$와 일치한다. | . - $ frac{ partial }{ partial { bf W}}{ bf u}^ top$의 계산 . $ frac{ partial }{ partial { bf W}}{ bf u}^ top = frac{ partial }{ partial { bf W}}(u_1, dots,u_5)= big( frac{ partial }{ partial { bf W}}u_1, dots, frac{ partial }{ partial { bf W}}u_5 big) $ . B = torch.zeros((2,5)) B . tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) . W . tensor([3., 3.]) . _W = torch.tensor([3., 3.],requires_grad=True) _W . tensor([3., 3.], requires_grad=True) . for i in range(5): _W = torch.tensor([3., 3.],requires_grad=True) _u = (X@_W)[i] _u.backward() B[:,i]= _W.grad.data . B # X의 트랜스포즈 . tensor([[ 1., 1., 1., 1., 1.], [11., 12., 13., 14., 15.]]) . X . tensor([[ 1., 11.], [ 1., 12.], [ 1., 13.], [ 1., 14.], [ 1., 15.]]) . 이론적인 결과와 일치 | . &#51104;&#44624; &#49373;&#44033;&#54644;&#48372;&#51088;.. . - 결국 위의 예제에 한정하여 임의의 ${ bf hat{W}}$에 대한 $ frac{ partial}{ partial { bf hat W}}loss$는 아래와 같이 계산할 수 있다. . (단계1) $2{ bf v}$를 계산하고 | (단계2) (단계1)의 결과 앞에 $-{ bf I}$를 곱하고 | (단계3) (단계2)의 결과 앞에 ${ bf X}^ top$를 곱한다. | . - step1에서 ${ bf v}$는 어떻게 알지? . X $ to$ u=X@W $ to$ v = y-u | . 그런데 이것은 우리가 loss를 구하기 위해서 이미 계산해야 하는것 아니었나? | step1: yhat, step2: loss, step3: derivate, step4: update | . - (중요) step2에서 loss만 구해서 저장할 생각 하지말고 중간과정을 다 저장해라. (그중에 v와 같이 필요한것이 있을테니까) 그리고 그걸 적당한 방법을 통하여 이용하여 보자. . backprogation &#50508;&#44256;&#47532;&#51608; &#47784;&#54000;&#48652; . - 아래와 같이 함수의 변환을 아키텍처로 이해하자. (함수의입력=레이어의입력, 함수의출력=레이어의출력) . ${ bf X} overset{l1}{ to} { bf X}{ bf W} overset{l2}{ to} { bf y} -{ bf X}{ bf W} overset{l3}{ to} ({ bf y}-{ bf X}{ bf W})^ top ({ bf y}-{ bf X}{ bf W})$ | . - 그런데 위의 계산과정을 아래와 같이 요약할 수도 있다. (${ bf X} to { bf hat y} to loss$가 아니라 ${ bf W} to loss({ bf W})$로 생각해보세요) . ${ bf W} overset{l1}{ to} { bf u} overset{l2}{ to} { bf v} overset{l3}{ to} loss$ | . - 그렇다면 아래와 같은 사실을 관찰할 수 있다. . (단계1) $2{ bf v}$는 function of ${ bf v}$이고, ${ bf v}$는 l3의 입력 (혹은 l2의 출력) | (단계2) $-{ bf I}$는 function of ${ bf u}$이고, ${ bf u}$는 l2의 입력 (혹은 l1의 출력) | (단계3) 마찬가지의 논리로 ${ bf X}^ top$는 function of ${ bf W}$로 해석할 수 있다. | . - 요약: $2{ bf v},-{ bf I}, { bf X}^ top$와 같은 핵심적인 값들이 사실 각 층의 입/출력 값들의 함수꼴로 표현가능하다. $ to$ 각 층의 입/출력 값들을 모두 기록하면 미분계산을 유리하게 할 수 있다. . 문득의문: 각 층의 입출력값 ${ bf v}, { bf u}, { bf W}$로 부터 $2{ bf v}, -{ bf I}, { bf X}^ top$ 를 만들어내는 방법을 모른다면 헛수고 아닌가? | 의문해결: 어차피 우리가 쓰는 층은 선형+(렐루, 시그모이드, ...) 정도가 전부임. 따라서 변환규칙은 미리 계산할 수 있음. | . - 결국 . (1) 순전파를 하면서 입출력값을 모두 저장하고 . (2) 그에 대응하는 층별 미분계수값 $2{ bf v}, -{ bf I}, { bf X}^ top$ 를 구하고 . (3) 층별미분계수값을 다시 곱하면 (그러니까 ${ bf X}^ top (-{ bf I}) 2{ bf v}$ 를 계산) 된다. . backpropagation . (1) 순전파를 계산하고 각 층별 입출력 값을 기록 . yhat = net(X) | loss = loss_fn(yhat,y) | . (2) 역전파를 수행하여 손실함수의 미분값을 계산 . loss.backward() | . - 참고로 (1)에서 층별 입출력값은 GPU의 메모리에 기록된다.. 무려 GPU 메모리.. . - 작동원리를 GPU의 관점에서 요약 (슬기로운 GPU 활용) . gpu특징: 큰 차원의 매트릭스 곱셈 전문가 (원리? 어마어마한 코어숫자) . 아키텍처 설정: 모형의 파라메터값을 GPU 메모리에 올림 // net.to(&quot;cuda:0&quot;) | 순전파 계산: 중간 계산결과를 모두 GPU메모리에 저장 (순전파 계산을 위해서라면 굳이 GPU에 있을 필요는 없으나 후에 역전파를 계산하기 위한 대비) // net(X) | 오차 및 손실함수 계산: loss = loss_fn(yhat,y) | 역전파 계산: 순전파단계에서 저장된 계산결과를 활용하여 손실함수의 미분값을 계산 // loss.backward() | 다음 순전파 계산: 이전값은 삭제하고 새로운 중간계산결과를 GPU메모리에 올림 | 반복. | . some comments . - 역전파기법은 체인룰 + $ alpha$ 이다. . - 오차역전파기법이라는 용어를 쓰는 사람도 있다. . - 이미 훈련한 네트워크에 입력 $X$를 넣어 결과값만 확인하고 싶을 경우 순전파만 사용하면 되고, 이 상황에서는 좋은 GPU가 필요 없다. .",
            "url": "https://guebin.github.io/2021BDA/2021/11/18/(11%EC%A3%BC%EC%B0%A8)-11%EC%9B%9418%EC%9D%BC-23%EC%9D%BC.html",
            "relUrl": "/2021/11/18/(11%EC%A3%BC%EC%B0%A8)-11%EC%9B%9418%EC%9D%BC-23%EC%9D%BC.html",
            "date": " • Nov 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "(10주차) 11월16일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/5) 벡터와 매트릭스의 미분 (1) . - (2/5) 벡터와 매트릭스의 미분 (2) . - (3/5) 벡터와 매트릭스의 미분 (3) . - (4/5) 벡터와 매트릭스의 미분 (4) . - (5/5) 벡터와 매트릭스의 미분 (5) . . Note: 영상에 해당하는 강의자료는 LMS에 있습니다. (pdf파일참고) .",
            "url": "https://guebin.github.io/2021BDA/2021/11/16/(10%EC%A3%BC%EC%B0%A8)-11%EC%9B%9416%EC%9D%BC.html",
            "relUrl": "/2021/11/16/(10%EC%A3%BC%EC%B0%A8)-11%EC%9B%9416%EC%9D%BC.html",
            "date": " • Nov 16, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "2021년 2학기 빅데이터분석 중간고사",
            "content": "1. &#53356;&#47204;&#47553;&#51012; &#53685;&#54620; &#51060;&#48120;&#51648; &#48516;&#49437; &#48143; CAM (40&#51216;) . (a) &#46160;&#44032;&#51648; &#53412;&#50892;&#46300;&#47196; &#53356;&#47204;&#47553;&#54616;&#50668; &#51060;&#48120;&#51648; &#51088;&#47308;&#47484; &#47784;&#51004;&#44256; CNN &#47784;&#54805;&#51012; &#54876;&#50857;&#54616;&#50668; &#48516;&#49437;&#54616;&#46972;. . 2주차-hw1의 분석코드를 그대로 활용하여도 무방함 | . (b) CAM&#51012; &#51060;&#50857;&#54616;&#50668; CNN&#51032; &#54032;&#45800;&#44540;&#44144;&#47484; &#49884;&#44033;&#54868;&#54616;&#46972;. . 2. &#50500;&#47000;&#50752; &#44057;&#51008; 5&#44060;&#51032; &#51088;&#47308;&#47484; &#44288;&#52769;&#54616;&#50688;&#45796;&#44256; &#44032;&#51221;&#54616;&#51088;. (15&#51216;) . x y . 0 | 11 | 17.7 | . 1 | 12 | 18.5 | . 2 | 13 | 21.2 | . 3 | 14 | 23.6 | . 4 | 15 | 24.2 | . (a) &#47784;&#54805; $y_i= beta_0+ beta_1 x_i$&#50640; &#54644;&#45817;&#54616;&#45716; &#45348;&#53944;&#50892;&#53356;&#47484; &#54028;&#51060;&#53664;&#52824;&#47484; &#51060;&#50857;&#54616;&#50668; &#49444;&#44228;&#54616;&#44256; &#49552;&#49892;&#54632;&#49688;&#47484; &#51221;&#51032;&#54616;&#46972;. $( beta_0, beta_1)=(3,3)$&#51068; &#44221;&#50864;&#51032; loss&#47484; &#44228;&#49328;&#54616;&#46972;. . 손실함수는 MSELoss를 활용한다. | . (b) $( beta_0, beta_1)=(3,3)$&#50640;&#49436; &#49552;&#49892;&#54632;&#49688;&#51032; &#48120;&#48516;&#44228;&#49688;&#47484; &#44228;&#49328;&#54616;&#46972;. . (c) &#44221;&#49324;&#54616;&#44053;&#48277;&#51012; &#53685;&#54616;&#50668; $( beta_0, beta_1)=(3,3)$&#51032; &#44050;&#51012; 1&#54924; update&#54616;&#46972;. &#50668;&#44592;&#50640;&#49436; &#54617;&#49845;&#47456;&#51008; 0.01&#47196; &#49444;&#51221;&#54620;&#45796;. . 3. &#54028;&#51060;&#53664;&#52824;&#47484; &#51060;&#50857;&#54620; &#47784;&#54805;&#44396;&#52629; &#48143; &#54617;&#49845; (20&#51216;) . 아래와 같은 모형에서 시뮬레이션 된 자료가 있다고 하자. . $$y_i= beta_0 + beta_1 exp(-x_i)+ epsilon_i$$ . 여기에서 $ epsilon_i overset{iid} sim N(0,0.1^2)$ 이다. 시뮬레이션된 자료는 아래의 코드를 통하여 얻을 수 있다. . import pandas as pd df=pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021BDA/master/_notebooks/2021-11-06-prob3.csv&#39;) df.head() . x y . 0 0.0 | 4.962202 | . 1 0.1 | 4.889815 | . 2 0.2 | 4.605782 | . 3 0.3 | 4.491711 | . 4 0.4 | 4.344537 | . 자료를 시각화 하면 아래와 같다. . plt.plot(df.x,df.y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc16f7dcd60&gt;] . 파이토치를 이용하여 적절한 $ beta_0, beta_1$의 값을 구하여라. (손실함수는 MSEloss를 사용한다.) . 4. &#45796;&#51020;&#51012; &#51096; &#51069;&#44256; &#47932;&#51020;&#50640; O/X&#47196; &#45813;&#54616;&#46972;. (25&#51216;) . (1) 경사하강법은 손실함수와 상관없이 언제나 전역최소해를 찾을 수 있다. . (2) 확률적경사하강법은 손실함수와 상관없이 언제 전역최소해를 찾을 수 있다. . (3) 일반근사정리(universal approximation theorem)는 충분히 깊은 신경망이 어떠한 함수든 표현할 수 있다는 내용의 이론이다. . (4) $y_i= beta_0+ beta_1 x_i+ epsilon_i$ 와 같은 형태의 단순회귀모형은 학습해야할 파라메터가 2개이다. . (5) 참모형(true model)이 단순회귀모형일 경우, 비선형 활성화 함수를 사용한 깊은신경망으로 모형을 적합시키면 오히려 적합력이 떨어진다. . (6) 확률적 경사하강법은 관측자료에 임의의 오차항을 더하여 학습시키는 방법이다. . (7) 경사하강법은 손실함수가 convex일 경우 언제나 전역최소해를 찾을 수 있다. . (8) 로지스틱 모형에서 MSEloss를 사용하더라도 전역최소해를 찾는 경우가 있다. 즉 시그모이드 활성화 함수와 MSEloss를 사용한다고 하여도 항상 전역최소해를 찾지 못하는 것은 아니다. . (9) 로지스틱 모형에서 MLELoss를 사용하면 옵티마이저를 Adam으로 선택하고 BCELoss를 사용하면 확률적 경사하강법을 사용한다. . (10) 확률적 경사하강법은 컴퓨터의 자원을 효율적으로 활용할 수 있도록 도와준다. . (11) 학습할 파라메터가 많을수록 GPU의 학습속도가 CPU의 학습속도 보다 빠르다. . (12) GPU는 언제나 CPU보다 빠르게 모형을 학습한다. . (13) CNN 모형에서 에서 2D콘볼루션은 비선형 변환이다. . (14) 드랍아웃은 결측치를 제거하는 기법이다. . (15) 모든 관측치를 활용하지 않고 일부의 관측치만 활용하여 학습하는 기법을 드랍아웃이라 한다. . (16) 확률적 경사하강법은 드랍아웃과 같이 사용할 수 없다. . (17) MLP의 모든 활성화 함수가 선형이라면 은닉층(Hidden Layer)을 아무리 추가하여도 모형의 표현력이 향상되지 않는다. . (18) 학습할 파라메터수가 증가하면 언더피팅의 위험이 있다. . (19) CAM은 CNN의 모든층에서 사용가능하다. . (20) CAM은 CNN모형의 일부를 수정해야 한다는 점에서 단점이 있다. . (21) CNN은 이미지 자료만 분석할 수 있다. . (22) 드랍아웃은 과적합을 방지하는 효과가 있다. . (23) 예측 및 적합을 할때는 네트워크에서 드랍아웃층을 제거해야 한다. . (24) BCELoss는 Softmax 활성화 함수와 잘 어울린다. . (25) 파이토치에서 미분을 수행하는 메소드는 backward() 이다. .",
            "url": "https://guebin.github.io/2021BDA/2021/11/09/mid.html",
            "relUrl": "/2021/11/09/mid.html",
            "date": " • Nov 9, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "(A2) 빅데이터분석 중간고사 대비문제",
            "content": "1. &#53356;&#47204;&#47553;&#51012; &#53685;&#54620; &#51060;&#48120;&#51648; &#48516;&#49437; &#48143; CAM (40&#51216;) . (a) &#46160;&#44032;&#51648; &#53412;&#50892;&#46300;&#47196; &#53356;&#47204;&#47553;&#54616;&#50668; &#51060;&#48120;&#51648; &#51088;&#47308;&#47484; &#47784;&#51004;&#44256; CNN &#47784;&#54805;&#51012; &#54876;&#50857;&#54616;&#50668; &#48516;&#49437;&#54616;&#46972;. . 2주차-hw1의 분석코드를 그대로 활용하여도 무방함 | . (b) CAM&#51012; &#51060;&#50857;&#54616;&#50668; CNN&#51032; &#54032;&#45800;&#44540;&#44144;&#47484; &#49884;&#44033;&#54868;&#54616;&#46972;. . 2. &#50500;&#47000;&#50752; &#44057;&#51008; 5&#44060;&#51032; &#51088;&#47308;&#47484; &#44288;&#52769;&#54616;&#50688;&#45796;&#44256; &#44032;&#51221;&#54616;&#51088;. (15&#51216;) . x y . 0 | 11 | 17.7 | . 1 | 12 | 18.5 | . 2 | 13 | 21.2 | . 3 | 14 | 23.6 | . 4 | 15 | 24.2 | . (a) &#47784;&#54805; $y_i= beta_0+ beta_1 x_i$&#50640; &#54644;&#45817;&#54616;&#45716; &#45348;&#53944;&#50892;&#53356;&#47484; &#54028;&#51060;&#53664;&#52824;&#47484; &#51060;&#50857;&#54616;&#50668; &#49444;&#44228;&#54616;&#44256; &#49552;&#49892;&#54632;&#49688;&#47484; &#51221;&#51032;&#54616;&#46972;. $( beta_0, beta_1)=(3,3)$&#51068; &#44221;&#50864;&#51032; loss&#47484; &#44228;&#49328;&#54616;&#46972;. . 손실함수는 MSELoss를 활용한다. | . (b) $( beta_0, beta_1)=(3,3)$&#50640;&#49436; &#49552;&#49892;&#54632;&#49688;&#51032; &#48120;&#48516;&#44228;&#49688;&#47484; &#44228;&#49328;&#54616;&#46972;. . (c) &#44221;&#49324;&#54616;&#44053;&#48277;&#51012; &#53685;&#54616;&#50668; $( beta_0, beta_1)=(3,3)$&#51032; &#44050;&#51012; 1&#54924; update&#54616;&#46972;. &#50668;&#44592;&#50640;&#49436; &#54617;&#49845;&#47456;&#51008; 0.01&#47196; &#49444;&#51221;&#54620;&#45796;. . 3. &#54028;&#51060;&#53664;&#52824;&#47484; &#51060;&#50857;&#54620; &#47784;&#54805;&#44396;&#52629; &#48143; &#54617;&#49845; (20&#51216;) . 아래와 같은 모형에서 시뮬레이션 된 자료가 있다고 하자. . $$y_i= beta_0 + beta_1 exp(-x_i)+ epsilon_i$$ . 여기에서 $ epsilon_i overset{iid} sim N(0,0.1^2)$ 이다. 시뮬레이션된 자료는 아래의 코드를 통하여 얻을 수 있다. . import pandas as pd df=pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/2021BDA/master/_notebooks/2021-11-06-prob3.csv&#39;) df.head() . x y . 0 0.0 | 4.962202 | . 1 0.1 | 4.889815 | . 2 0.2 | 4.605782 | . 3 0.3 | 4.491711 | . 4 0.4 | 4.344537 | . 자료를 시각화 하면 아래와 같다. . plt.plot(df.x,df.y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc16f7dcd60&gt;] . 파이토치를 이용하여 적절한 $ beta_0, beta_1$의 값을 구하여라. (손실함수는 MSEloss를 사용한다.) . 4. &#45796;&#51020;&#51012; &#51096; &#51069;&#44256; &#47932;&#51020;&#50640; O/X&#47196; &#45813;&#54616;&#46972;. (25&#51216;) . - 확률적 경사하강법은 손실함수의 모양과 상관없이 언제나 전역최소해를 찾을 수 있다. .",
            "url": "https://guebin.github.io/2021BDA/2021/11/06/(A2)-%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC-%EB%8C%80%EB%B9%84%EB%AC%B8%EC%A0%9C.html",
            "relUrl": "/2021/11/06/(A2)-%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC-%EB%8C%80%EB%B9%84%EB%AC%B8%EC%A0%9C.html",
            "date": " • Nov 6, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "(9주차) 11월4일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/5) 컨볼루션 연산 . - (2/5) 파이토치를 이용한 컨볼루션 연산의 구현, feature extraction (1) . - (3/5) feature extraction (2), padding, stride . - (4/5) CNN 아키텍처 . - (5/5) Batch normalization, Discussion about CNN . import . import torch import matplotlib.pyplot as plt . &#52968;&#48380;&#47336;&#49496; . &#52968;&#48380;&#47336;&#49496; &#50672;&#49328; . - 아래는 (5,5) 이미지에 (3,3) 커널을 적용하여 컨볼루션을 수행한 결과이다. . . https://arxiv.org/abs/1603.07285 | . - 위의 연산을 구현하면 아래와 같다. . img = torch.Tensor([[3.0, 3.0, 2.0, 1.0, 0.0], [0.0, 0.0, 1.0, 3.0, 1.0], [3.0, 1.0, 2.0, 2.0, 3.0], [2.0, 0.0, 0.0, 2.0, 2.0], [2.0, 0.0, 0.0, 0.0, 1.0]]) . img.shape . torch.Size([5, 5]) . img=img.reshape(1,1,5,5) . conv=torch.nn.Conv2d(in_channels=1,out_channels=1, kernel_size=(3,3), bias=False) . conv.weight . Parameter containing: tensor([[[[-0.2265, -0.0131, -0.2056], [-0.0014, 0.2152, 0.1334], [-0.3204, -0.0991, -0.0626]]]], requires_grad=True) . conv.weight.data = torch.Tensor([[[[0.0, 1.0, 2.0], [2.0, 2.0, 0.0], [0.0, 1.0, 2.0]]]]) . conv.weight . Parameter containing: tensor([[[[0., 1., 2.], [2., 2., 0.], [0., 1., 2.]]]], requires_grad=True) . conv(img) . tensor([[[[12., 12., 17.], [10., 17., 19.], [ 9., 6., 14.]]]], grad_fn=&lt;ThnnConv2DBackward0&gt;) . 현재는 입력채널이 1개 출력채널이 1개인 경우이다. | 입력채널이 3개인경우 (3,3) 커널이 3개 필요하다. | 입력채널이 3개이고 출력채널이 16개인 경우라면 (3,3) 커널이 48개(3$ times 16$) 필요하다. | . &#53945;&#51669;&#52628;&#52636;&#44592;&#45733; (feature extraction) . - 컨볼루션연산은 커널의 계수값에 따라서 이미지의 특징을 추출하는 역할을 하는데 그것을 알아보기 위해 아래와 같이 Conv2d 층을 하나 생성하자. . conv = torch.nn.Conv2d(in_channels=1, out_channels=2, kernel_size=2, bias=False) . - 필터값확인 . conv.weight . Parameter containing: tensor([[[[-0.1265, -0.1014], [-0.0099, 0.0583]]], [[[-0.3847, -0.3246], [-0.0972, -0.3373]]]], requires_grad=True) . - 필터값을 내가 원하는 값으로 변경 . conv.weight.data= torch.Tensor([[[[0.25, 0.25], [0.25, 0.25]]], [[[-1.0,1.0], [-1.0,1.0]]]]) . 첫번째 필터는 평균필터, 두번째 필터는 엣지검출필터 | . - 입력데이터 생성 (단순한 흑백대비) . img = torch.Tensor([[.1, .1, .1, .0, .0, .0], [.1, .1, .1, .0, .0, .0], [.1, .1, .1, .0, .0, .0], [.1, .1, .1, .0, .0, .0], [.1, .1, .1, .0, .0, .0], [.1, .1, .1, .0, .0, .0]]).reshape(1,1,6,6) . img . tensor([[[[0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000], [0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000], [0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000], [0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000], [0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000], [0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000]]]]) . plt.imshow(img.squeeze(),cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f4a90bc5670&gt; . - 입력이미지에 컨볼루션 적용 . plt.imshow(conv(img)[0][0].data,cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f4a90b86a90&gt; . 첫번째 필터를 적용한 결과 | . plt.imshow(conv(img)[0][1].data,cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f4a90b4a2e0&gt; . - 요약 . 커널의 계수에 따라서 엣지를 검출하는 필터가 만들어지기도 하고, 스무딩을 하는 필터가 만들어지기도 한다. | 이들을 조합하면 다양한 특징을 검출할 수 있다. | . &#54056;&#46377;, &#49828;&#53944;&#46972;&#51060;&#46300; . - 패딩: 이미지의 가장자리에 적당한 값(예를들면 0)을 넣고 커널연산을 수행. 그래서 컨볼루션 연산 이후에도 이미지의 크기가 줄어들지 않도록 방지. . - 스트라이드: 윈도우가 1칸씩 이동하는 것이 아니라 2~3칸씩 이동함 (이미지 축소효과) . CNN &#50500;&#53412;&#53581;&#52376;&#51032; &#54364;&#54788;&#48169;&#48277; . - 왜 풀링을 하는가? 이미지를 줄이고 싶어서. . - 아래와 같이 아키텍처의 형태로 표현하고 굳이 이미지를 그리지않음 (출처: 위키피디아) . . &#48176;&#52824;&#45432;&#47568;&#46972;&#51060;&#51228;&#51060;&#49496; (&#50500;&#53412;&#53581;&#52376;&#47112;&#48296;&#51032; &#52968;&#53944;&#47204;) . ref: https://www.google.com/search?q=deep+learning+ian+goodfellow&amp;biw=1119&amp;bih=707&amp;tbm=bks&amp;sxsrf=AOaemvJfbtYOtnqia8NXVzJ03G1m-NuukQ%3A1635986386747&amp;ei=0iuDYbP-LInT-QbphZXABQ&amp;oq=Deep+Learning+i&amp;gs_l=psy-ab.3.0.0i19k1.10636.13294.0.14233.4.3.1.0.0.0.166.447.0j3.3.0....0...1c.1j4.64.psy-ab..0.4.451....0.dDa1TpYCPMs | . - 원래 목적은 최적화의 개선으로 개발된 방법이다. 역전파에 의해 발생하는 2~3차 사이드 이펙을 캔슬한다. (미분값을 잘 계산하기 위한 알고리즘) . - 하지만 부수적으로 과적합을 피하는 효과도 있다 (그래서 종종 드랍아웃을 쓸 필요가 없어진다.) . - 생각보다 어려운 개념이다. 그냥 이런것이 있다는 것 정도만 알아둘것 . discussion about CNN . - 격자형태로 배열된 자료를 처리하는데 특화된 신경망이다. . 시계열 (1차원 격자), 이미지(2차원 격자) | . - 신경과학의 원리가 심층학습에 영향을 미친 사례 . &#47784;&#54000;&#48652; . - 회소성 + 매개변수 공유 . 희소성: 이미지를 분석하여 특징을 뽑아낼 떄 부분부분의 특징만 뽑으면 된다는 의미 | 매개변수공유: 한 채널에는 하나의 역할을 하는 커널을 설계하면 된다는 의미 (스무딩이든 엣징이든). 즉 어떤지역은 스무딩, 어떤 지역은 엣징을 할 필요없이 한 채널은 모두 스무딩, 다른 채널은 모두 엣징만 하고 여러채널을 고려하여 이미지를 이해하면 된다는 의미 | . - 당연한 소리같지만 위의 원리로 인해서 파라메터 감소효과가 엄청남 . (예시) (1,6,6) $ to$ (2,5,5) . MLP 방식이라면 $36 times 50$의 파라메터가 필요함 | CNN은 8개 | . (예시) (3,244,244) $ to$ (16,224,244) . &#49888;&#44221;&#47581;&#51032; &#44396;&#51312; . - 컨볼루션 - 활성화 - 풀링 . - 풀링: 요약의 의미 . 요약을 굳이 왜 하는가? 어차피 결국 $y=0$ 아니면 $y=1$ 이니까 줄이긴 줄여야 함 | 요약을 하나도 안하고 있다가 나중에 한번에 하려면 힘들어요 | .",
            "url": "https://guebin.github.io/2021BDA/2021/11/04/(9%EC%A3%BC%EC%B0%A8)-11%EC%9B%944%EC%9D%BC.html",
            "relUrl": "/2021/11/04/(9%EC%A3%BC%EC%B0%A8)-11%EC%9B%944%EC%9D%BC.html",
            "date": " • Nov 4, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "시험관련 안내사항",
            "content": "&#49884;&#54744;&#50976;&#54805; . - 오픈북: 강의노트, 본인이 정리한 노트, 인터넷 검색 가능 . - 비대면: Zoom을 활용하여 응시 . &#49884;&#54744;&#49884;&#44036; . - 일시: LMS를 통해 공지한 날의 수업시간 . - 시험시간 중 처음 30분은 장비점검시간으로 활용함 (단, 모든 사람이 준비될 경우 30분을 기다리지 않고 시작) . . Note: 따라서 2시간 수업일 경우 실질적으로 문제를 풀고 답안을 제출할때 까지 쓸 수 있는 시간은 1시간30분입니다. . &#49884;&#54744;&#49892; &#51077;&#51109; . - LMS $ to$ 강의대화 $ to$ Zoom 화상강의 바로 가기로 입장 . &#49884;&#54744;&#47928;&#51228; &#44277;&#44060;&#48169;&#49885; . - LMS 공지사항을 통하여 시험문제의 URL을 공개 . &#51228;&#52636;&#48169;&#49885; . - LMS의 레포트 메뉴를 활용하여 답안지를 제출 (종료시간 이전에 미리 제출가능) . &#51456;&#48708;&#47932; . - 컴퓨터 및 노트북: 시험지 확인 및 문제풀이 용도 . - 핸드폰: Zoom을 통하여 주변상황을 및 컴퓨터 화면을 촬영하는 용도 . . Note: 중간에 핸드폰 및 노트북이 꺼지지 않도록 배터리 충전기를 준비한다. . &#49884;&#54744;&#51204; &#51456;&#48708;&#49324;&#54637; . - 시험준비시간 동안 핸드폰을 아래와 같이 배치하여 학생의 컴퓨터 화면 및 주변상황이 보이도록 함 . . Note: 적절한 각도를 설정하기 어려운 경우 주변환경보다 컴퓨터의 화면이 잘 보이도록 설정할 것 . - 학생증을 준비하여 시험 시작 직전에 본인의 얼굴과 학생증을 함께 촬영한다. (5초간) . &#50976;&#51032;&#49324;&#54637; . - 줌의 대화명은 이름과 학번을 모두 적는다. (예시: 최규빈_202143052) . . Note: 동명이인이 있을 수 있으므로 학번을 같이 적으세요 . - 질문은 카카오톡 채널 혹은 줌의 채팅기능을 이용한다. . - Zoom에서 스피커 음소거를 하지 않는다. (전체 공지사항등이 있을때 음성으로 공지함) . - 핸드폰으로 Zoom참가 중 전화가 오면 거절하고 받지 않는다. (전화통화시 Zoom연결이 종료되므로 부정행위로 의심할 수 있음) . &#44592;&#53440; &#52280;&#44256;&#49324;&#54637; . - 핸드폰과 피씨를 이용하여 줌에 동시접속할 경우 . 최규빈_202143052_핸드폰 | 최규빈_202143052_컴퓨터 | . 와 같이 기기를 분리하여 적는다. . - 시험문제는 코랩으로 풀어도 무방하며 시험문제를 다운받아 개인 주피터노트북 등으로 풀어도 무방하다. . - 제출형식은 주피터 노트북파일을 권장한다. 하지만 풀이 및 코드를 알아볼 수 있는 어떠한 형식으로 제출하여도 무방하다. (ex: txt, hwp, pdf..) .",
            "url": "https://guebin.github.io/2021BDA/2021/11/02/%EC%8B%9C%ED%97%98%EA%B4%80%EB%A0%A8-%EC%95%88%EB%82%B4%EC%82%AC%ED%95%AD.html",
            "relUrl": "/2021/11/02/%EC%8B%9C%ED%97%98%EA%B4%80%EB%A0%A8-%EC%95%88%EB%82%B4%EC%82%AC%ED%95%AD.html",
            "date": " • Nov 2, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "(8주차) 11월1일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/5) CAM (dog/cat) (1) . - (2/5) CAM (dog/cat) (2) . - (3/5) CAM (dog/cat) (3) . - (4/5) CAM (dog/cat) (4) . - (5/5) CAM (dog/cat) (5) . import . import torch from fastai.vision.all import * . data . path=untar_data(URLs.PETS)/&#39;images&#39; . files=get_image_files(path) . def label_func(f): if f[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) . learn . lrnr=cnn_learner(dls,resnet34,metrics=error_rate) lrnr.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.153076 | 0.027989 | 0.006766 | 00:34 | . epoch train_loss valid_loss error_rate time . 0 | 0.050782 | 0.004555 | 0.002030 | 00:41 | . &#47784;&#54805;&#46895;&#50612;&#48372;&#44592; . - 샘플로 하나의 관측치를 만든다. . get_image_files(path)[0] . Path(&#39;/home/cgb4/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_106.jpg&#39;) . img = PILImage.create(get_image_files(path)[0]) img . x, = first(dls.test_dl([img])) . - 전체네트워크를 1,2로 나눈다. . net1=lrnr.model[0] net2=lrnr.model[1] . - net2를 수정한다. . net1(x).shape . torch.Size([1, 512, 16, 16]) . net2 . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=2, bias=False) ) . net2 = torch.nn.Sequential( torch.nn.AdaptiveAvgPool2d(output_size=1), torch.nn.Flatten(), torch.nn.Linear(512,out_features=2,bias=False)) . - net1, net2를 묶어서 새로운 네트워크를 만들고 다시 학습 . net=torch.nn.Sequential(net1,net2) . lrnr2=Learner(dls,net,metrics=accuracy) . lrnr2.loss_func, lrnr.loss_func . (FlattenedLoss of CrossEntropyLoss(), FlattenedLoss of CrossEntropyLoss()) . lrnr2.fine_tune(5) . epoch train_loss valid_loss accuracy time . 0 | 0.232697 | 1.214378 | 0.788904 | 00:41 | . epoch train_loss valid_loss accuracy time . 0 | 0.127689 | 0.157893 | 0.937077 | 00:41 | . 1 | 0.130365 | 0.300920 | 0.862652 | 00:41 | . 2 | 0.090777 | 0.090363 | 0.967524 | 00:41 | . 3 | 0.047184 | 0.069729 | 0.978349 | 00:41 | . 4 | 0.024875 | 0.054623 | 0.984438 | 00:41 | . - 시각화 . net1(x).shape, net2[2].weight.shape . (torch.Size([1, 512, 16, 16]), torch.Size([2, 512])) . camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) . camimg.shape . torch.Size([2, 16, 16]) . 원래는 [1,7,7] 이었는데.. $ to$ 그래서 (7,7)를 평균내서 양인지 음인지 판단했고, 음이면 고양이 양수이면 강아지 와 같은 식으로 예측했음 (반대도가능) | 지금은 내가 데이터를 만들지 않았기 때문에 1을 고양이로 했는지 0을 강아지로 했는지 모르겠음 | 첫번째 차원이 왜 2인지도 클리어하지 않음 (마지막 활성화함수가 sigmoid가 아니고 softmax이기 때문이라는 것은 알고 있으나 명확하게 모르겠음) | . -- . &#49548;&#54532;&#53944;&#47589;&#49828; vs &#49884;&#44536;&#47784;&#51060;&#46300; . - 시그모이드 . y의 형태: 고양이=0, 개=1 . | 마지막 활성화함수: $u to frac{e^u}{1+e^u}$ 이때 $u$는 시그모이드층의 인풋 (=마지막 리니어층의 아웃풋) . | $u$의 값이 클수록 dog . | . - 소프트맥스 . $y$의 형태: 고양이=[1,0], 개=[0,1] . | 마지막 활성화함수: $(u_1,u_2) to big( frac{e^{u_1}}{e^{u_1}+e^{u_2}}, frac{e^{u_2}}{e^{u_1}+e^{u_2}} big)$, 이때 $(u_1,u_2)$는 소프트맥스의 인풋 (=마지막 리니어층의 아웃풋) . | $u_1$의 값이 클수록 cat, $u_2$의 값이 클수록 dog . | . - 참고로 $ big( frac{e^{u_1}}{e^{u_1}+e^{u_2}}, frac{e^{u_2}}{e^{u_1}+e^{u_2}} big)$에서 분자분모에 각각 $e^{-u_1}$을 곱하면 . $$ big( frac{1}{1+e^{u_2-u_1}}, frac{e^{u_2-u_1}}{1+e^{u_2-u_1}} big)$$ . 그리고 $u_2-u_1=u$라고 생각하면 . $$ big( frac{1}{1+e^{u}}, frac{e^{u}}{1+e^{u}} big)$$ . 이므로, 강아지라고 생각할 확률은 $ frac{e^u}{1+e^u}$, 고양이라고 생각할 확률은 $1- frac{e^u}{1+e^u}$이 되므로 시그모이드와 같아진다. . - 결국 이 경우 (2개의 클래스를 가지는 경우)는 똑같은 모형을 이득도 없이 파라메터만 더 써서 표현한 꼴임 . - 따라서 엄밀하게 따지면 이것은 파라메터의 낭비이다. 마치 . $$y_i = alpha_0 + beta_0 +( alpha_1+ beta_1)x_i+ epsilon_i$$ . 와 비슷함 . - 아래의 사례역시 유사하다. . 사례1: Ber(p) 대신 Ber(p,q)로 쓰는 꼴, (단 $p+q=1$) | 사례2: Bin(n,p) 대신 Bin(n, (p,q))로 쓰는 꼴, (단 $p+q=1$) | . - 하지만 위와 같은 표현식은 다차원으로 확장이 용이할 경우가 많다. . - 그리고 사실 파라메터를 몇개 더 써도 큰 문제는 아님 . - 전역최소해를 찾지 못할거라는 주장도 있지만 꼭 전역최소해를 찾야아하는 것도 아니다. . - 결론 . 소프트맥스는 시그모이드의 확장이다. | 클래스의 수가 2개일 경우에는 (Sigmoid, BCEloss) 조합을 사용해야 하고 클래스의 수가 2개보다 클 경우에는 (Softmax, CrossEntropyLoss) 를 사용해야 한다. | 그런데 사실 클래스의 수가 2개일 경우일때 (Softmax, CrossEntropyLoss)를 사용해도 그렇게 큰일나는것은 아니다. (흑백이미지를 칼라잉크로 출력하는 느낌) | 오히려 resnet 같이 최적화된 모형을 뜯어 고치면서 성능 저하시키는 것이 더 안좋을 수 있다. | -- . - 다시 돌아오자. camimg를 이미지를 AP layer에 통과시키자. . torch.nn.AdaptiveAvgPool2d(output_size=1)(camimg) . TensorImage([[[-8.4133]], [[ 8.4786]]], device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward0&gt;) . - $y approx[0,1]$ 임은 알겠는데 이것이 개인지 고양이인지는 모르겠음. . - dls에 코딩된 라벨을 확인 . dls.vocab . [&#39;cat&#39;, &#39;dog&#39;] . 뒷쪽값이 클수록 강아지이다. | . - 강아지라고 판단한 근거를 시각화하자. . plt.imshow(camimg[1].to(&quot;cpu&quot;).detach(),extent=(0,223,223,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) . &lt;matplotlib.image.AxesImage at 0x7fc8a36cce20&gt; . - 학습에 사용된 그림 . dls.train.decode((x,))[0].squeeze().show() . &lt;AxesSubplot:&gt; . - plot . fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) . &lt;matplotlib.image.AxesImage at 0x7fc8a3700c10&gt; . magma: 검-보-빨-노 순으로 값이 크다. | . - 오른쪽 그림에서 노란색으로 표현된 부분이 개라고 생각한 근거임 . 고양이가 아니라고 생각한 근거: 왼쪽그림의 보라색 | 강아지라고 생각한 근거: 오른쪽그램의 노란색 | . - (고양이,강아지)라고 생각한 확률 . a=net(x).tolist()[0][0] b=net(x).tolist()[0][1] np.exp(a)/(np.exp(a)+np.exp(b)), np.exp(b)/(np.exp(a)+np.exp(b)) . (4.613257284554693e-08, 0.9999999538674271) . &#54616;&#45768; . x, = first(dls.test_dl([PILImage.create(&#39;2021-09-06-hani01.jpeg&#39;)])) . a,b = net(x).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) . camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) fig, (ax1,ax2) = plt.subplots(1,2) # dls.train.decode((x,))[0].squeeze().show(ax=ax1) ax1.imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax1.set_title(&quot;cat(%s)&quot; % catprob.round(5)) # dls.train.decode((x,))[0].squeeze().show(ax=ax2) ax2.imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax2.set_title(&quot;dog(%s)&quot; % dogprob.round(5)) . Text(0.5, 1.0, &#39;dog(0.99997)&#39;) . CAM &#44208;&#44284; &#54869;&#51064; . fig, ax = plt.subplots(5,5) k=0 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;cat(%s)&quot; % catprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;dog(%s)&quot; % dogprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . fig, ax = plt.subplots(5,5) k=25 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;cat(%s)&quot; % catprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;dog(%s)&quot; % dogprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . fig, ax = plt.subplots(5,5) k=50 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;cat(%s)&quot; % catprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;dog(%s)&quot; % dogprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . fig, ax = plt.subplots(5,5) k=75 for i in range(5): for j in range(5): x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])])) camimg = torch.einsum(&#39;ij,jkl -&gt; ikl&#39;, net2[2].weight, net1(x).squeeze()) a,b = net(x).tolist()[0] catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) , np.exp(b)/ (np.exp(a)+np.exp(b)) if catprob&gt;dogprob: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[0].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;cat(%s)&quot; % catprob.round(5)) else: dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j]) ax[i][j].imshow(camimg[1].to(&quot;cpu&quot;).detach(),alpha=0.5,extent=(0,511,511,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) ax[i][j].set_title(&quot;dog(%s)&quot; % dogprob.round(5)) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() . discusstion about CAM . - 장점: CNN 모형의 판단근거를 시각화하기에 우수한 툴이다. . - 단점: 모형을 일부수정해야 한다. . - 단점2: 최종아웃풋에서만 시각화를 할 수 있음. .",
            "url": "https://guebin.github.io/2021BDA/2021/11/01/(8%EC%A3%BC%EC%B0%A8)-11%EC%9B%941%EC%9D%BC.html",
            "relUrl": "/2021/11/01/(8%EC%A3%BC%EC%B0%A8)-11%EC%9B%941%EC%9D%BC.html",
            "date": " • Nov 1, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "(7-8주차) 10월26일 10월28일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/10) 드랍아웃, 배치추가 (1) . - (2/10) 드랍아웃, 배치추가 (2) . - (3/10) resnet34 . - (4/10) 모형을 뜯어보는 방법 (직접만든모델) . - (5/10) resnet34 의 구조 (1) . - (6/10) resnet34 의 구조 (2) . - (7/10) $y$의 형태에 따라 달라지는 오차항가정/활성화함수/손실함수, 딥러닝의 연구의 4가지 축, 설명가능한 딥러닝의 필요성 . - (8/10) CAM (1) . - (9/10) CAM (2) . - (10/10) CAM (3) . import . import torch from fastai.vision.all import * . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . data . - download data . path = untar_data(URLs.MNIST_SAMPLE) . path.ls() . (#3) [Path(&#39;/home/cgb4/.fastai/data/mnist_sample/labels.csv&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_sample/train&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_sample/valid&#39;)] . - list . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . - list $ to$ image . Image.open(threes[4]) . - image $ to$ tensor . tensor(Image.open(threes[4])) . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 72, 156, 241, 254, 255, 188, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 168, 250, 232, 147, 79, 143, 254, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 109, 231, 164, 39, 0, 0, 0, 86, 251, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 40, 0, 0, 0, 0, 4, 200, 157, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 92, 249, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 221, 128, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 147, 185, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 137, 224, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 137, 239, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 239, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 245, 104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 179, 254, 224, 217, 147, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 44, 117, 117, 196, 237, 104, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 117, 246, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 85, 241, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 225, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 170, 131, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 104, 0, 0, 0, 0, 17, 234, 87, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 198, 179, 29, 0, 42, 199, 235, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 154, 236, 250, 252, 163, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8) . 여기에서 tensor는 파이토치가 아니라 fastai에서 구현한 함수임 | . - 여러개의 리스트를 모두 텐서로 바꿔보자. . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . - $X$와 $y$를 만들자. . seven_tensor.shape, three_tensor.shape . (torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28])) . y=torch.tensor([0.0]*6265+ [1.0]*6131).reshape(12396,1) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) . X.shape, y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . X=X.reshape(12396,1,28,28) . X.shape . torch.Size([12396, 1, 28, 28]) . 1. &#51648;&#45212;&#49884;&#44036;&#44620;&#51648;&#51032; &#47784;&#54805; (&#51649;&#51217;&#45348;&#53944;&#50892;&#53356;&#49444;&#44228;, pytorch) . 2d convolution with windowsize=5 . c1=torch.nn.Conv2d(1,16,5) # 입력채널=1 (흑백이므로), 출력채널=16, 윈도우크기5 . X.shape, c1(X).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24])) . MaxPool2d . m1=torch.nn.MaxPool2d(2) . X.shape,c1(X).shape,m1(c1(X)).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12])) . ReLU . a1=torch.nn.ReLU() . X.shape,c1(X).shape, m1(c1(X)).shape, a1(m1(c1(X))).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 16, 12, 12])) . flatten . class Flatten(torch.nn.Module): def forward(self,x): return x.reshape(12396,-1) . flatten=Flatten() . X.shape,c1(X).shape, m1(c1(X)).shape, a1(m1(c1(X))).shape, flatten(a1(m1(c1(X)))).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 2304])) . linear . l1=torch.nn.Linear(in_features=2304,out_features=1) . X.shape, c1(X).shape, m1(c1(X)).shape, a1(m1(c1(X))).shape, flatten(a1(m1(c1(X)))).shape, l1(flatten(a1(m1(c1(X))))).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 2304]), torch.Size([12396, 1])) . plt.plot(l1(flatten(a1(m1(c1(X))))).data) . [&lt;matplotlib.lines.Line2D at 0x7f3f9b27b310&gt;] . networks &#49444;&#44228; . net = nn.Sequential(c1,m1,a1,flatten,l1) ## 마지막의 sigmoid는 생략한다. torch.nn..BCEWithLogitsLoss()에 내장되어 있을것이므로 . - 손실함수와 옵티마이저 정의 . loss_fn=torch.nn.BCEWithLogitsLoss() optimizer= torch.optim.Adam(net.parameters()) . - step1~4 . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . a2= torch.nn.Sigmoid() . plt.plot(y) plt.plot(a2(yhat.data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3f9b25f7c0&gt;] . ypred=a2(yhat.data)&gt;0.5 . sum(ypred==y)/12396 . tensor([0.9938]) . 2. &#46300;&#46989;&#51060;&#50883;, &#48176;&#52824;&#52628;&#44032; (&#51649;&#51217;&#45348;&#53944;&#50892;&#53356;&#49444;&#44228;, pytorch+fastai) . step1: dls&#47484; &#47564;&#46308;&#51088;. . ds=torch.utils.data.TensorDataset(X,y) . ds.tensors[0].shape . torch.Size([12396, 1, 28, 28]) . ds1,ds2 = torch.utils.data.random_split(ds,[10000,2396]) . dl1 = torch.utils.data.DataLoader(ds1,batch_size=500) dl2 = torch.utils.data.DataLoader(ds2,batch_size=2396) . dls=DataLoaders(dl1,dl2) . step2: &#50500;&#53412;&#53581;&#52376;, &#49552;&#49892;&#54632;&#49688;, &#50741;&#54000;&#47560;&#51060;&#51200; . class Flatten(torch.nn.Module): def forward(self,x): return x.reshape(x.shape[0],-1) . net=torch.nn.Sequential( torch.nn.Conv2d(1,16,5), torch.nn.MaxPool2d(2), torch.nn.ReLU(), torch.nn.Dropout2d(), Flatten(), torch.nn.Linear(2304,1)) . loss_fn=torch.nn.BCEWithLogitsLoss() #optimizer= torch.optim.Adam(net.parameters()) . step3: lrnr &#49373;&#49457; &#54980; &#51201;&#54633; . lrnr1 = Learner(dls,net,opt_func=Adam,loss_func=loss_fn) . lrnr1.fit(10) . epoch train_loss valid_loss time . 0 | 0.430748 | 0.218082 | 00:00 | . 1 | 0.262507 | 0.093608 | 00:00 | . 2 | 0.178563 | 0.070512 | 00:00 | . 3 | 0.132288 | 0.061353 | 00:00 | . 4 | 0.104299 | 0.055355 | 00:00 | . 5 | 0.086001 | 0.050486 | 00:00 | . 6 | 0.073517 | 0.047254 | 00:00 | . 7 | 0.064433 | 0.044630 | 00:00 | . 8 | 0.057706 | 0.042601 | 00:00 | . 9 | 0.053289 | 0.040278 | 00:00 | . - 결과를 시각화하면 아래와 같다. . plt.plot(a2(net(X.to(&quot;cuda:0&quot;)).to(&quot;cpu&quot;).data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3f9b66d9d0&gt;] . - 빠르고 적합결과도 좋음 . 3. resnet34 (&#44592;&#51316;&#51032; &#45348;&#53944;&#50892;&#53356; &#49324;&#50857;, &#49692;&#49688; fastai) . - 데이터로부터 새로운 데이터로더스를 만들고 이를 dls2라고 하자. . path=untar_data(URLs.MNIST_SAMPLE) path . Path(&#39;/home/cgb4/.fastai/data/mnist_sample&#39;) . dls2=ImageDataLoaders.from_folder( path, train=&#39;train&#39;, valid_pct=0.2) . - 러너오브젝트를 생성하고 학습하자. . lrnr2=cnn_learner(dls2,resnet34,metrics=error_rate) lrnr2.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.284949 | 0.159780 | 0.055787 | 00:08 | . epoch train_loss valid_loss error_rate time . 0 | 0.042842 | 0.016358 | 0.006584 | 00:09 | . - 결과관찰 . lrnr2.show_results() . &#47784;&#54805;&#51012; &#46895;&#50612;&#48372;&#45716; &#48169;&#48277; (lrnr1.model) . - 우선 방법2로 돌아가자. . net(X.to(&quot;cuda:0&quot;)) . tensor([[-8.1382], [-6.9877], [ 0.7937], ..., [12.1038], [15.0634], [ 7.9055]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward&gt;) . - 네트워크 구조 . net . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) (4): Flatten() (5): Linear(in_features=2304, out_features=1, bias=True) ) . - 층별변환과정 . print(X.shape, &#39;--&gt; input image&#39;) print(net[0](X.to(&quot;cuda:0&quot;)).shape, &#39;--&gt; 2dConv&#39;) print(net[1](net[0](X.to(&quot;cuda:0&quot;))).shape, &#39;--&gt; MaxPool2d&#39;) print(net[2](net[1](net[0](X.to(&quot;cuda:0&quot;)))).shape, &#39;--&gt; ReLU&#39;) print(net[3](net[2](net[1](net[0](X.to(&quot;cuda:0&quot;))))).shape, &#39;--&gt; Dropout2d&#39;) print(net[4](net[3](net[2](net[1](net[0](X.to(&quot;cuda:0&quot;)))))).shape, &#39;--&gt; Flatten&#39;) print(net[5](net[4](net[3](net[2](net[1](net[0](X.to(&quot;cuda:0&quot;))))))).shape, &#39;--&gt; Linear&#39;) . torch.Size([12396, 1, 28, 28]) --&gt; input image torch.Size([12396, 16, 24, 24]) --&gt; 2dConv torch.Size([12396, 16, 12, 12]) --&gt; MaxPool2d torch.Size([12396, 16, 12, 12]) --&gt; ReLU torch.Size([12396, 16, 12, 12]) --&gt; Dropout2d torch.Size([12396, 2304]) --&gt; Flatten torch.Size([12396, 1]) --&gt; Linear . - 최종결과 . net[5](net[4](net[3](net[2](net[1](net[0](X.to(&quot;cuda:0&quot;))))))) . tensor([[-8.1382], [-6.9877], [ 0.7937], ..., [12.1038], [15.0634], [ 7.9055]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward&gt;) . net(X.to(&quot;cuda:0&quot;)) . tensor([[-8.1382], [-6.9877], [ 0.7937], ..., [12.1038], [15.0634], [ 7.9055]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward&gt;) . - lrnr1자체를 활용해도 층별변환과정을 추적할수 있음. (lrnr1.model = net 임을 이용) . lrnr1.model . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) (4): Flatten() (5): Linear(in_features=2304, out_features=1, bias=True) ) . lrnr1.model[0] . Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) . lrnr1.model(X.to(&quot;cuda:0&quot;)) . tensor([[-8.1382], [-6.9877], [ 0.7937], ..., [12.1038], [15.0634], [ 7.9055]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward&gt;) . print(X.shape, &#39;--&gt; input image&#39;) print(lrnr1.model[0](X.to(&quot;cuda:0&quot;)).shape, &#39;--&gt; 2dConv&#39;) print(lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;))).shape, &#39;--&gt; MaxPool2d&#39;) print(lrnr1.model[2](lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;)))).shape, &#39;--&gt; ReLU&#39;) print(lrnr1.model[3](lrnr1.model[2](lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;))))).shape, &#39;--&gt; Dropout2d&#39;) print(lrnr1.model[4](lrnr1.model[3](lrnr1.model[2](lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;)))))).shape, &#39;--&gt; Flatten&#39;) print(lrnr1.model[5](lrnr1.model[4](lrnr1.model[3](lrnr1.model[2](lrnr1.model[1](lrnr1.model[0](X.to(&quot;cuda:0&quot;))))))).shape, &#39;--&gt; Linear&#39;) . torch.Size([12396, 1, 28, 28]) --&gt; input image torch.Size([12396, 16, 24, 24]) --&gt; 2dConv torch.Size([12396, 16, 12, 12]) --&gt; MaxPool2d torch.Size([12396, 16, 12, 12]) --&gt; ReLU torch.Size([12396, 16, 12, 12]) --&gt; Dropout2d torch.Size([12396, 2304]) --&gt; Flatten torch.Size([12396, 1]) --&gt; Linear . - 정리: 모형은 항상 아래와 같이 2d-part 와 1d-part로 나누어진다. . torch.Size([12396, 1, 28, 28]) --&gt; input image torch.Size([12396, 16, 24, 24]) --&gt; 2dConv torch.Size([12396, 16, 12, 12]) --&gt; MaxPool2d torch.Size([12396, 16, 12, 12]) --&gt; ReLU torch.Size([12396, 16, 12, 12]) --&gt; Dropout2d =============================================================== torch.Size([12396, 2304]) --&gt; Flatten torch.Size([12396, 1]) --&gt; Linear . - 2d-part: . 2d선형변환: nn.torch.nn.Conv2d() | 2d비선형변환: torch.nn.MaxPool2d(), torch.nn.ReLU() | . - 1d-part: . 1d선형변환: torch.nn.Linear() | 1d비선형변환: torch.nn.ReLU() | . _net1=torch.nn.Sequential( net[0], net[1], net[2], net[3]) _net2=torch.nn.Sequential( net[4], net[5]) . _net1 . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) ) . _net2 . Sequential( (0): Flatten() (1): Linear(in_features=2304, out_features=1, bias=True) ) . _net=torch.nn.Sequential(_net1,_net2) . _net[1](_net[0](X.to(&#39;cuda:0&#39;))) . tensor([[-8.1382], [-6.9877], [ 0.7937], ..., [12.1038], [15.0634], [ 7.9055]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward&gt;) . lrnr2.model &#48516;&#49437; . - 아래의 모형은 현재 가장 성능이 좋은 모형(state of the art)중 하나인 resnet이다. . lrnr2.model . Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=2, bias=False) ) ) . - 특징 . 2d-part: 입력채널이3이다, Conv2d에 padding/stride의 옵션이 있다, 드랍아웃이 없다, 배치정규화가 있다. | 1d-part: 배치정규화가 있다, 출력의 차원이 2이다. | . DLS, Networks . 네트워크의 형태에 따라서 dls의 형태도 다르게 만들어야 한다. | MLP모형: 입력이 $784$, 첫 네트워크의 형태가 $784 to 30$ 인 torch.nn.Linear() | CNN모형: 입력이 $1 times 28 times 28$, 첫 네트워크의 형태가 $1 times 28 times 28 to 16 times 24 times 24$ 인 torch.nn.Conv2d() | Resnet34: 입력이 $3 times 28 times 28$, 첫 네트워크의 형태가 $3 times 28 times 28 to ??$ | . 참고 . $y$ 분포가정 마지막층의 활성화함수 손실함수(파이토치) . 3.45, 4.43, ... (연속형) | 정규분포 | Linear | MSEloss | . 0 or 1 | 이항분포(베르누이) | Sigmoid | BCEloss | . [0,0,1], [0,1,0], [1,0,0] | 다항분포 | Softmax | CrossEntropyLoss | . &#46373;&#47084;&#45789; &#50672;&#44396;&#51032; &#45348;&#44032;&#51648; &#52629; . (1) 아키텍처 $( star)$ . 한 영역의 전문적인 지식이 필요한 것이 아닌것 같다. | 끈기, 약간의 운, 직관, 좋은컴퓨터.. | . (2) 손실함수 . 통계적지식필요 // 기존의 손실함수를 변형하는 형태 (패널티텀활용) | . (3) 미분계산 . 병렬처리등에 대한 지식 필요 | . (4) 옵티마이저 . 최적화에 대한 이론적 토대 필요 | . - 딥러닝 이전까지의 아키텍처에 대한 연구 . 파라메트릭 모형: 전문가 | 넌파라메트릭 모형: 전문가 | 딥러닝: 상대적으로 비전문가 | . - 특징: 비전문가도 만들수 있다 + 블랙박스 (내부연산을 뜯어볼 수는 있지만 우리가 해석하기 어려움) . - 설명가능한 딥러닝에 대한 요구 (XAI) . &#49444;&#47749;&#44032;&#45733;&#54620; CNN&#47784;&#54805; . - 현재까지의 모형 . 1단계: 2d선형변환 $ to$ 2d비선형변환 | 2단계: Flatten $ to$ MLP | . - lrnr1(제가만들었던 모형)의 모형을 다시 복습 . lrnr1.model . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) (4): Flatten() (5): Linear(in_features=2304, out_features=1, bias=True) ) . net1=torch.nn.Sequential( lrnr1.model[0], lrnr1.model[1], lrnr1.model[2], lrnr1.model[3]) . net1(X.to(&#39;cuda:0&#39;)).shape . torch.Size([12396, 16, 12, 12]) . - 1단계까지의 출력결과를 시각화 . fig, axs = plt.subplots(4,4) k=0 for i in range(4): for j in range(4): axs[i,j].imshow(net1(X.to(&quot;cuda:0&quot;))[0][k].to(&quot;cpu&quot;).data) k=k+1 fig.set_figheight(8) fig.set_figwidth(8) fig.tight_layout() . net1&#51008; &#50976;&#51648;+ net2&#51032; &#44396;&#51312;&#47484; &#48320;&#44221;!! . lrnr1.model . Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) (4): Flatten() (5): Linear(in_features=2304, out_features=1, bias=True) ) . - 계획 . 변경전net2: $(n,16,12,12) overset{flatten}{ Longrightarrow} (n,?) overset{Linear(?,1)}{ Longrightarrow} (n,1)$ | 변경후net2: $(n,16,12,12) overset{gap+flatten}{ Longrightarrow} (n,16) overset{Linear(16,1)}{ Longrightarrow} (n,1)$ | . - gap: 12$ times$12 픽셀을 평균내서 하나의 값으로 대표하자 (왜?) . ap=torch.nn.AdaptiveAvgPool2d(output_size=1) . ap(net1(X.to(&quot;cuda:0&quot;))).shape . torch.Size([12396, 16, 1, 1]) . -- . 보충학습:ap는 그냥 평균 . torch.tensor([[0.1,0.2],[0.3,0.4]]) . tensor([[0.1000, 0.2000], [0.3000, 0.4000]]) . ap(torch.tensor([[0.1,0.2],[0.3,0.4]])) . tensor([[0.2500]]) . -- . - flatten . flatten(ap(net1(X.to(&quot;cuda:0&quot;)))).shape . torch.Size([12396, 16]) . - linear . _l1=torch.nn.Linear(16,1,bias=False) . _l1.to(&quot;cuda:0&quot;) . Linear(in_features=16, out_features=1, bias=False) . _l1(flatten(ap(net1(X.to(&quot;cuda:0&quot;))))).shape . torch.Size([12396, 1]) . - 이걸 net2로 구성하자. $ to$ (net1,net2)를 묶어서 하나의 새로운 네트워크를 만들자. . net2=torch.nn.Sequential( torch.nn.AdaptiveAvgPool2d(1), Flatten(), torch.nn.Linear(16,1,bias=False)) . net=torch.nn.Sequential(net1,net2) net . Sequential( (0): Sequential( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): ReLU() (3): Dropout2d(p=0.5, inplace=False) ) (1): Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten() (2): Linear(in_features=16, out_features=1, bias=False) ) ) . - 수정된 네트워크로 lrnr3을 만들고 재학습 . ds=torch.utils.data.TensorDataset(X,y) ds1,ds2=torch.utils.data.random_split(ds,[10000,2396]) dl1=torch.utils.data.DataLoader(ds1,batch_size=1000) dl2=torch.utils.data.DataLoader(ds2,batch_size=2396) dls=DataLoaders(dl1,dl2) . lrnr3=Learner(dls,net,opt_func=Adam,loss_func=loss_fn,lr=0.1) . lrnr3.fit(10) . epoch train_loss valid_loss time . 0 | 0.224906 | 0.109848 | 00:00 | . 1 | 0.224250 | 0.108319 | 00:00 | . 2 | 0.224349 | 0.101010 | 00:00 | . 3 | 0.225390 | 0.109090 | 00:00 | . 4 | 0.223415 | 0.098581 | 00:00 | . 5 | 0.219940 | 0.092961 | 00:00 | . 6 | 0.217830 | 0.105528 | 00:00 | . 7 | 0.215317 | 0.097136 | 00:00 | . 8 | 0.212853 | 0.094199 | 00:00 | . 9 | 0.212849 | 0.100530 | 00:00 | . CAM: observation&#51012; 1&#44060;&#47196; &#44256;&#51221;&#54616;&#44256; net2&#50640;&#49436; layer&#51032; &#49692;&#49436;&#47484; &#48148;&#45012;&#49436; &#49884;&#44033;&#54868; . - 계획 . 변경전net2: $(n,16,12,12) overset{flatten}{ Longrightarrow} (n,?) overset{Linear(?,1)}{ Longrightarrow} (n,1)$ | 변경후net2: $(n,16,12,12) overset{gap+flatten}{ Longrightarrow} (n,16) overset{Linear(16,1)}{ Longrightarrow} (n,1)$ | CAM: $(1,16,12,12) overset{Linear(16,1)+flatten}{ Longrightarrow} (12,12) overset{gap}{ Longrightarrow} 1$ | . - 준비과정1: 시각화할 샘플을 하나 준비하자. . x=X[100] X.shape,x.shape . (torch.Size([12396, 1, 28, 28]), torch.Size([1, 28, 28])) . 차원이 다르므로 나중에 네트워크에 넣을때 문제가 생길 수 있음 $ to$ 차원을 맞춰주자 | . x=x.reshape(1,1,28,28) . plt.imshow(x.squeeze()) . &lt;matplotlib.image.AxesImage at 0x7f4058887250&gt; . - 준비과정2: 계산과 시각화를 위해서 각 네트워크를 cpu로 옮기자. (fastai로 학습한 직후라 GPU에 있음) . net1.to(&#39;cpu&#39;) net2.to(&#39;cpu&#39;) . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten() (2): Linear(in_features=16, out_features=1, bias=False) ) . - forward확인: 이 값을 기억하자. . net2(net1(x)) ## 음수이므로 class=7 이라고 CNN이 판단 . tensor([[-5.3201]], grad_fn=&lt;MmBackward&gt;) . - net2를 수정하고 forward값 확인 . net2 . Sequential( (0): AdaptiveAvgPool2d(output_size=1) (1): Flatten() (2): Linear(in_features=16, out_features=1, bias=False) ) . net2에서 Linear와 AdaptiveAvgPool2d의 적용순서를 바꿔줌 | . 차원확인 . net1(x).squeeze().shape . torch.Size([16, 12, 12]) . net2[2].weight.squeeze().shape . torch.Size([16]) . Linear(in_features=16, out_features=1, bias=False) 를 적용: 16 $ times$ (16,12,12) $ to$ (12,12) . net2[2].weight.squeeze() @ net1(x).squeeze() . RuntimeError Traceback (most recent call last) /tmp/ipykernel_56901/2879013373.py in &lt;module&gt; -&gt; 1 net2[2].weight.squeeze() @ net1(x).squeeze() RuntimeError: mat1 and mat2 shapes cannot be multiplied (192x12 and 16x1) . 실패.. | . camimg=torch.einsum(&#39;i,ijk -&gt; jk&#39;,net2[2].weight.squeeze(), net1(x).squeeze()) camimg.shape . torch.Size([12, 12]) . 성공 | . AdaptiveAvgPool2d(output_size=1) 를 적용 . ap(camimg) . tensor([[-5.3201]], grad_fn=&lt;MeanBackward1&gt;) . !!!! 똑같다? . - 아래의 값이 같다. . net2(net1(x)),ap(camimg) . (tensor([[-5.3201]], grad_fn=&lt;MmBackward&gt;), tensor([[-5.3201]], grad_fn=&lt;MeanBackward1&gt;)) . - 왜냐하면 ap와 선형변환 모두 linear이므로 순서를 바꿔도 상관없음 . - 아래와 결국 같은 이치 . _x= np.array([1,2,3,4]) _x . array([1, 2, 3, 4]) . np.mean(_x*2+1) . 6.0 . 2*np.mean(_x)+1 . 6.0 . - 이제 camimg 에 관심을 가져보자. . camimg . tensor([[ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 5.9098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, -19.2289, 0.0000, 0.0000, 0.0000, 29.9900, 37.3474, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, -29.6655, 0.0000, 0.0000, 5.6040, 13.2785, 1.8883, 30.8203, 4.3253, 0.0000, 0.0000, 0.0000], [ 0.0000, -21.9075, 0.0000, 0.0000, -13.0436, 17.4574, 30.4819, 6.4937, 0.0000, 0.0000, 0.0000, -12.9101], [ 0.0000, -36.6223, 0.0000, 0.0000, -3.6448, 0.0000, 2.4763, 20.4865, 8.1199, 0.0000, 0.0000, -7.6647], [ -2.8867, -175.0864, -110.9451, -6.6493, 0.0000, 0.0000, 0.0000, -18.9344, -43.5822, 0.0000, -6.6248, 0.0000], [ 2.1586, -1.7907, -9.5646, -11.2632, 0.0000, 0.0000, 0.0000, -33.7909, 0.0000, 0.0000, -14.1396, 0.0000], [ 0.0000, 0.0000, -0.4893, 0.0000, 0.0000, 0.0000, -15.7184, -42.0344, 0.0000, -3.9603, -2.1219, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -34.1539, -15.9513, 0.0000, -12.8468, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -10.5112, -252.0293, -1.3688, -11.8025, 0.0000, 0.0000]], grad_fn=&lt;ViewBackward&gt;) . ap(camimg), torch.mean(camimg) . (tensor([[-5.3201]], grad_fn=&lt;MeanBackward1&gt;), tensor(-5.3201, grad_fn=&lt;MeanBackward0&gt;)) . 이미지의 값은 대부분0이지만 궁극적으로는 평균을 내서 음수의 값이 나와야 한다. | . - 결국 특정픽셀에서 큰 음의 값이 나오기 떄문에 궁극적으로는 평균이 음수가 된다. . 평균이 음수이다. $ leftrightarrow$ 이미지가 의미하는것이 7이다. | 특정픽셀이 큰 음수값을 가진다. $ leftrightarrow$ 그 픽셀에서 이미지가 7임을 뚜렷하게 알 수 있다. | . - 그 특정픽셀이 어딘가? . plt.imshow(camimg.data) . &lt;matplotlib.image.AxesImage at 0x7f4058863d00&gt; . 초록색으로 표현된 부분은 CNN모형이 이 숫자를 7이라고 생각한 근거가 된다. | . - 원래의 이미지와 비교 . plt.imshow(x.squeeze()) . &lt;matplotlib.image.AxesImage at 0x7f40587ce4c0&gt; . - 두 이미지를 겹쳐서 그리면 멋진 그림이 될 것 같다. . step1: 원래이미지를 흑백으로 그리자. . plt.imshow(x.squeeze(),cmap=&#39;gray&#39;,alpha=0.5) . &lt;matplotlib.image.AxesImage at 0x7f40587a7e20&gt; . - step2: 원래이미지는 (28,28)인데 camimg는 (12,12)픽셀 $ to$ camimg의 픽셀을 늘리자. . plt.imshow(camimg.data,alpha=0.5, extent=(0,27,27,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) . &lt;matplotlib.image.AxesImage at 0x7f405870e970&gt; . - step3: 합치자. . plt.imshow(x.squeeze(),cmap=&#39;gray&#39;,alpha=0.5) plt.imshow(camimg.data,alpha=0.5, extent=(0,27,27,0),interpolation=&#39;bilinear&#39;,cmap=&#39;magma&#39;) . &lt;matplotlib.image.AxesImage at 0x7f40587338e0&gt; . &#49689;&#51228; . - 숫자3이 그려진 이미지를 observation으로 선택하고 위와 같이 cam을 이용하여 시각화하라. .",
            "url": "https://guebin.github.io/2021BDA/2021/10/28/(7-8%EC%A3%BC%EC%B0%A8)-10%EC%9B%9426%EC%9D%BC,-10%EC%9B%9428%EC%9D%BC.html",
            "relUrl": "/2021/10/28/(7-8%EC%A3%BC%EC%B0%A8)-10%EC%9B%9426%EC%9D%BC,-10%EC%9B%9428%EC%9D%BC.html",
            "date": " • Oct 28, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "(7주차) 10월21일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/2) CNN 모형구축 (MNIST 3,7) . - (2/2) 과제설명 . import . import torch from fastai.vision.all import * . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . data . - download data . path = untar_data(URLs.MNIST_SAMPLE) . path.ls() . (#3) [Path(&#39;/home/cgb4/.fastai/data/mnist_sample/labels.csv&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_sample/train&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_sample/valid&#39;)] . - list . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . - list $ to$ image . Image.open(threes[4]) . - image $ to$ tensor . tensor(Image.open(threes[4])) . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 72, 156, 241, 254, 255, 188, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 168, 250, 232, 147, 79, 143, 254, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 109, 231, 164, 39, 0, 0, 0, 86, 251, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 40, 0, 0, 0, 0, 4, 200, 157, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 92, 249, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 221, 128, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 147, 185, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 137, 224, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 137, 239, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 239, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 245, 104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 179, 254, 224, 217, 147, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 44, 117, 117, 196, 237, 104, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 117, 246, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 85, 241, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 225, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 170, 131, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 104, 0, 0, 0, 0, 17, 234, 87, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 198, 179, 29, 0, 42, 199, 235, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 154, 236, 250, 252, 163, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8) . 여기에서 tensor는 파이토치가 아니라 fastai에서 구현한 함수임 | . - 여러개의 리스트를 모두 텐서로 바꿔보자. . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . - $X$와 $y$를 만들자. . seven_tensor.shape, three_tensor.shape . (torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28])) . y=torch.tensor([0.0]*6265+ [1.0]*6131).reshape(12396,1) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) . X.shape, y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . &#44592;&#51316;&#51032; MLP &#47784;&#54805; . ${ bf X} to { bf WX+b} to f({ bf WX+b}) to dots to { bf y}$ . ${ bf X}=12396 times 784$ matrix | ${ bf y}=12396 times 1$ (col) vector | . - 교재의 모형 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node30&quot; -&gt; &quot;y&quot; label = &quot;Layer 2: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2: Sigmoid x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y y node1&#45;&gt;y node2&#45;&gt;y ...&#45;&gt;y node30&#45;&gt;y - 왜 28$ times$28 이미지를 784개의 벡터로 만든 다음에 모형을 돌려야 하는가? . - 기존에 개발된 모형이 회귀분석 기반으로 되어있어서 결국 회귀분석 틀에 짜 맞추어서 이미지자료를 분석하는 느낌 . - observation의 차원은 $784$가 아니라 $1 times (28 times 28)$이 되어야 맞다. . X.shape . torch.Size([12396, 784]) . X=X.reshape(12396,1,28,28) . X.shape . torch.Size([12396, 1, 28, 28]) . plt.imshow(X[776][0]) . &lt;matplotlib.image.AxesImage at 0x7f63903c5730&gt; . &#49440;&#54805;&#48320;&#54872; &#45824;&#49888;&#50640; 2d convolution with windowsize=5 . c1=torch.nn.Conv2d(1,16,5) # 입력채널=1 (흑백이므로), 출력채널=16, 윈도우크기5 . X.shape, c1(X).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24])) . fig, axs = plt.subplots(4,4) k=0 for i in range(4): for j in range(4): axs[i,j].imshow(c1(X)[776][k].data) k=k+1 . fig.set_figheight(8) fig.set_figwidth(8) fig.tight_layout() fig . ReLU() &#45824;&#49888; MaxPool2d + ReLU . MaxPool2d . m1=torch.nn.MaxPool2d(2) . X.shape,c1(X).shape,m1(c1(X)).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12])) . fig, axs = plt.subplots(4,4) k=0 for i in range(4): for j in range(4): axs[i,j].imshow(m1(c1(X))[776][k].data) k=k+1 fig.set_figheight(8) fig.set_figwidth(8) fig.tight_layout() . ReLU . a1=torch.nn.ReLU() . X.shape,c1(X).shape, m1(c1(X)).shape, a1(m1(c1(X))).shape . (torch.Size([12396, 1, 28, 28]), torch.Size([12396, 16, 24, 24]), torch.Size([12396, 16, 12, 12]), torch.Size([12396, 16, 12, 12])) . fig, axs = plt.subplots(4,4) k=0 for i in range(4): for j in range(4): axs[i,j].imshow(a1(m1(c1(X)))[776][k].data) k=k+1 fig.set_figheight(8) fig.set_figwidth(8) fig.tight_layout() . torch.manual_seed(1) _A= torch.randn((3,3)) _A . tensor([[ 0.6614, 0.2669, 0.0617], [ 0.6213, -0.4519, -0.1661], [-1.5228, 0.3817, -1.0276]]) . a1(_A) . tensor([[0.6614, 0.2669, 0.0617], [0.6213, 0.0000, 0.0000], [0.0000, 0.3817, 0.0000]]) . &#50668;&#44592;&#50640;&#49436; &#44536;&#45285; &#49884;&#44536;&#47784;&#51060;&#46300;&#50640; &#53468;&#50864;&#51088;. . - 현재상황 . a1(m1(c1(X))).shape . torch.Size([12396, 16, 12, 12]) . - 펼치자 . a1(m1(c1(X))).reshape(12396,-1).shape . torch.Size([12396, 2304]) . - 2304의 디멘젼을 1로 만들자. . l1=torch.nn.Linear(in_features=2304,out_features=1) . l1(a1(m1(c1(X))).reshape(12396,-1)) . tensor([[-0.0023], [-0.0947], [ 0.0029], ..., [-0.1033], [-0.1207], [-0.1130]], grad_fn=&lt;AddmmBackward&gt;) . - 시그모이드를 걸자. . a2=torch.nn.Sigmoid() a2(l1(a1(m1(c1(X))).reshape(12396,-1))) . tensor([[0.4994], [0.4764], [0.5007], ..., [0.4742], [0.4699], [0.4718]], grad_fn=&lt;SigmoidBackward&gt;) . networks &#49444;&#44228; . net = nn.Sequential( c1, # 컨볼루션(선형) m1, # 맥스풀링(비선형) -- 효과? 이미지를 계층적으로 파악할 수 있게함 a1, # 렐루(비선형) a1(m1(c1(X))).reshape(12396,-1), ## 이걸 구현해야하는데?? l1) ## 마지막의 a2는 생략한다. torch.nn..BCEWithLogitsLoss()에 내장되어 있을것이므로 . TypeError Traceback (most recent call last) /tmp/ipykernel_2953/1160457815.py in &lt;module&gt; -&gt; 1 net = nn.Sequential( 2 c1, # 컨볼루션(선형) 3 m1, # 맥스풀링(비선형) -- 효과? 이미지를 계층적으로 파악할 수 있게함 4 a1, # 렐루(비선형) 5 ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/container.py in __init__(self, *args) 87 else: 88 for idx, module in enumerate(args): &gt; 89 self.add_module(str(idx), module) 90 91 def _get_item_by_idx(self, iterator, idx) -&gt; T: ~/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/modules/module.py in add_module(self, name, module) 370 &#34;&#34;&#34; 371 if not isinstance(module, Module) and module is not None: --&gt; 372 raise TypeError(&#34;{} is not a Module subclass&#34;.format( 373 torch.typename(module))) 374 elif not isinstance(name, torch._six.string_classes): TypeError: torch.FloatTensor is not a Module subclass . net = nn.Sequential( c1, # 컨볼루션(선형) m1, # 맥스풀링(비선형) -- 효과? 이미지를 계층적으로 파악할 수 있게함 a1, # 렐루(비선형) # a1(m1(c1(X))).reshape(12396,-1), ## 이걸 구현해야하는데?? l1) ## 마지막의 a2는 생략한다. torch.nn..BCEWithLogitsLoss()에 내장되어 있을것이므로 . - 결국 주석처리한 부분을 구현해야함. . - c1,m1,a1,l1의 공통점 . 무언가를 상속받는 클래스에서 생성된 인스턴스이다. | forward메소드가 있다. | . - custom layer를 만드는 방법 . torch.nn.Module을 상속받아서 클래스를 하나 만든다. | forward 메소드를 정의한다. (다음레이어로 리턴할 값) | . class Flatten(torch.nn.Module): def forward(self,x): return x.reshape(12396,-1) . flatten=Flatten() . flatten(a1(m1(c1(X)))).shape . torch.Size([12396, 2304]) . - 잘 구현이 된것 같다. . net = nn.Sequential( c1, # 컨볼루션(선형) m1, # 맥스풀링(비선형) -- 효과? 이미지를 계층적으로 파악할 수 있게함 a1, # 렐루(비선형) flatten,# a1(m1(c1(X))).reshape(12396,-1), ## 이걸 구현해야하는데?? l1) ## 마지막의 a2는 생략한다. torch.nn..BCEWithLogitsLoss()에 내장되어 있을것이므로 . - 손실함수와 옵티마이저 정의 . loss_fn=torch.nn.BCEWithLogitsLoss() optimizer= torch.optim.Adam(net.parameters()) . - step1~4 . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(y) plt.plot(a2(yhat.data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f6382540a90&gt;] . ypred=a2(yhat.data)&gt;0.5 . sum(ypred==y)/12396 . tensor([0.9927]) . - 좀 더 성능이 좋아졌다. (이미 좋았는데 약간 더 좋아짐) . &#49689;&#51228; . - torch.nn.MaxPool2d(2) 대신 torch.nn.MaxPool2d(3) 을 사용하여 모형을 학습해보고 결과비교 .",
            "url": "https://guebin.github.io/2021BDA/2021/10/21/(7%EC%A3%BC%EC%B0%A8)-10%EC%9B%9421%EC%9D%BC.html",
            "relUrl": "/2021/10/21/(7%EC%A3%BC%EC%B0%A8)-10%EC%9B%9421%EC%9D%BC.html",
            "date": " • Oct 21, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "(6주차) 10월19일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . import torch import matplotlib.pyplot as plt . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . plt.plot(X,y) . [&lt;matplotlib.lines.Line2D at 0x7f6f577c6820&gt;] . &#45348;&#53944;&#50892;&#53356; &#49444;&#51221;, &#50741;&#54000;&#47560;&#51060;&#51200;, &#47196;&#49828; . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . &#47784;&#54805;&#54617;&#49845; . for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y) plt.plot(X,yhat.data) . [&lt;matplotlib.lines.Line2D at 0x7f6f54ac3760&gt;] . train / validation . X1=X[:80] y1=y[:80] X2=X[80:] y2=y[80:] . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . for epoc in range(1000): ## 1 y1hat=net(X1) ## 2 loss=loss_fn(y1hat,y1) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f6f549d9790&gt;] . &#46300;&#46989;&#50500;&#50883; . X1=X[:80] y1=y[:80] X2=X[80:] y2=y[80:] . torch.manual_seed(1) net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . for epoc in range(1000): ## 1 y1hat=net(X1) ## 2 loss=loss_fn(y1hat,y1) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . net.eval() ## 네트워크를 평가모드로 전환 plt.plot(X,y) plt.plot(X1,net(X1).data,&#39;--r&#39;) plt.plot(X2,net(X2).data,&#39;--g&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f6f401b5a90&gt;] . &#54617;&#49845;&#44284;&#51221; &#48708;&#44368; (&#51452;&#51032;: &#53076;&#46300;&#48373;&#51105;&#54632;) . - 데이터 생성 . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1) . - tr/val 분리 . X_tr=X[:80] y_tr=y[:80] X_val=X[80:] y_val=y[80:] . - 네트워크, 옵티마이저, 손실함수 설정 . 드랍아웃을 이용한 네트워트 (net2)와 그렇지 않은 네트워크 (net1) | 대응하는 옵티마이저 1,2 설정 | 손실함수 | . torch.manual_seed(1) net1=torch.nn.Sequential( torch.nn.Linear(1,512), torch.nn.ReLU(), torch.nn.Linear(512,1)) optimizer_net1 = torch.optim.Adam(net1.parameters()) net2=torch.nn.Sequential( torch.nn.Linear(1,512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(512,1)) optimizer_net2 = torch.optim.Adam(net2.parameters()) loss_fn=torch.nn.MSELoss() . tr_loss_net1=[] val_loss_net1=[] tr_loss_net2=[] val_loss_net2=[] . for epoc in range(1000): ## 1 yhat_tr_net1 = net1(X_tr) ## 2 loss_tr = loss_fn(yhat_tr_net1, y_tr) ## 3 loss_tr.backward() ## 4 optimizer_net1.step() net1.zero_grad() ## 5 기록 ### tr tr_loss_net1.append(loss_tr.item()) ### val yhat_val_net1 = net1(X_val) loss_val = loss_fn(yhat_val_net1,y_val) val_loss_net1.append(loss_val.item()) . for epoc in range(1000): ## 1 yhat_tr_net2 = net2(X_tr) ## 2 loss_tr = loss_fn(yhat_tr_net2, y_tr) ## 3 loss_tr.backward() ## 4 optimizer_net2.step() net2.zero_grad() ## 5 기록 ### tr net2.eval() tr_loss_net2.append(loss_tr.item()) ### val yhat_val_net2 = net2(X_val) loss_val = loss_fn(yhat_val_net2,y_val) val_loss_net2.append(loss_val.item()) net2.train() . net2.eval() fig , ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2) ax1.plot(X,y,&#39;.&#39;);ax1.plot(X_tr,net1(X_tr).data); ax1.plot(X_val,net1(X_val).data); ax2.plot(X,y,&#39;.&#39;);ax2.plot(X_tr,net2(X_tr).data); ax2.plot(X_val,net2(X_val).data); ax3.plot(tr_loss_net1);ax3.plot(val_loss_net1); ax4.plot(tr_loss_net2);ax4.plot(val_loss_net2); . - 다 좋은데 코드를 짜는것이 너무 힘들다. . 생각해보니까 미니배치도 만들어야 함 + 미니배치를 나눈상태에서 GPU 메모리에 파라메터도 올려야함. | 조기종료와 같은 기능도 구현해야함 + 기타등등을 구현해야함. | 나중에는 학습률을 서로 다르게 돌려가며 결과도 기록해야함 $ to$ 그래야 좋은 학습률 선택가능 | for문안에 step1~step4를 넣는것도 너무 반복작업임. | 등등.. | . - 위와 같은 것들의 특징: 머리로 상상하기는 쉽지만 실제 구현하는 것은 까다롭다. . - 사실 우리가 하고싶은것 . 아키텍처를 설계: 데이터를 보고 맞춰서 설계해야할 때가 많음 (우리가 해야한다) | 손실함수: 통계학과 교수님들이 연구하심 | 옵티마이저: 산공교수님들이 연구하심 | . - 제 생각 . 기업의욕심: read-data를 분석하는 딥러닝 아키텍처 설계 $ to$ 아키텍처별로 결과를 관찰 (편하게) $ Longrightarrow$ fastai + read data | 학생의욕심: 그러면서도 모형이 돌아가는 원리는 아주 세밀하게 알고싶음 $ Longrightarrow$ pytorch + toy example (regression 등을 위주로) | 연구자의욕심: 기존의 모형을 조금 변경해서 쓰고싶음 $ Longrightarrow$ (pytorch +fastai) + any data | . - tensorflow + keras vs pytorch + fastai . pytorch + fastai . - 데이터셋을 만든다. . X_tr=X[:80] y_tr=y[:80] X_val=X[80:] y_val=y[80:] . ds1=torch.utils.data.TensorDataset(X_tr,y_tr) ds2=torch.utils.data.TensorDataset(X_val,y_val) . - 데이터로더를 만든다. . dl1 = torch.utils.data.DataLoader(ds1, batch_size=80) dl2 = torch.utils.data.DataLoader(ds2, batch_size=20) . - 데이터로더스를 만든다. . from fastai.vision.all import * . dls=DataLoaders(dl1,dl2) . &#46300;&#46989;&#50500;&#50883; &#51228;&#50808;&#48260;&#51204; . - 네트워크 설계 (드랍아웃 제외) . torch.manual_seed(1) net_fastai = torch.nn.Sequential( torch.nn.Linear(in_features=1, out_features=512), torch.nn.ReLU(), #torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512, out_features=1)) #optimizer loss_fn=torch.nn.MSELoss() . - 러너오브젝트 (for문 대신돌려주는 오브젝트) . lrnr= Learner(dls,net_fastai,opt_func=Adam,loss_func=loss_fn) . - 에폭만 설정하고 바로 학습 . lrnr.fit(1000) . epoch train_loss valid_loss time . 0 | 1.277156 | 0.491314 | 00:00 | . 1 | 1.277145 | 0.455286 | 00:00 | . 2 | 1.275104 | 0.444275 | 00:00 | . 3 | 1.274429 | 0.465787 | 00:00 | . 4 | 1.273436 | 0.507203 | 00:00 | . 5 | 1.272421 | 0.548102 | 00:00 | . 6 | 1.271840 | 0.561292 | 00:00 | . 7 | 1.271377 | 0.549409 | 00:00 | . 8 | 1.270855 | 0.530416 | 00:00 | . 9 | 1.270437 | 0.520700 | 00:00 | . 10 | 1.270176 | 0.526273 | 00:00 | . 11 | 1.269935 | 0.543579 | 00:00 | . 12 | 1.269655 | 0.562939 | 00:00 | . 13 | 1.269411 | 0.571586 | 00:00 | . 14 | 1.269217 | 0.563700 | 00:00 | . 15 | 1.269018 | 0.543646 | 00:00 | . 16 | 1.268787 | 0.521385 | 00:00 | . 17 | 1.268563 | 0.505799 | 00:00 | . 18 | 1.268362 | 0.500011 | 00:00 | . 19 | 1.268159 | 0.501830 | 00:00 | . 20 | 1.267941 | 0.506255 | 00:00 | . 21 | 1.267730 | 0.506739 | 00:00 | . 22 | 1.267540 | 0.499733 | 00:00 | . 23 | 1.267353 | 0.487385 | 00:00 | . 24 | 1.267163 | 0.474839 | 00:00 | . 25 | 1.266981 | 0.466926 | 00:00 | . 26 | 1.266814 | 0.465347 | 00:00 | . 27 | 1.266648 | 0.468656 | 00:00 | . 28 | 1.266480 | 0.473641 | 00:00 | . 29 | 1.266316 | 0.476266 | 00:00 | . 30 | 1.266156 | 0.474677 | 00:00 | . 31 | 1.265996 | 0.469958 | 00:00 | . 32 | 1.265833 | 0.465630 | 00:00 | . 33 | 1.265673 | 0.464544 | 00:00 | . 34 | 1.265514 | 0.467181 | 00:00 | . 35 | 1.265355 | 0.472571 | 00:00 | . 36 | 1.265194 | 0.477105 | 00:00 | . 37 | 1.265037 | 0.478357 | 00:00 | . 38 | 1.264880 | 0.475766 | 00:00 | . 39 | 1.264724 | 0.471696 | 00:00 | . 40 | 1.264569 | 0.469089 | 00:00 | . 41 | 1.264416 | 0.469158 | 00:00 | . 42 | 1.264262 | 0.471343 | 00:00 | . 43 | 1.264108 | 0.472992 | 00:00 | . 44 | 1.263955 | 0.471979 | 00:00 | . 45 | 1.263801 | 0.468276 | 00:00 | . 46 | 1.263646 | 0.463477 | 00:00 | . 47 | 1.263491 | 0.460086 | 00:00 | . 48 | 1.263336 | 0.458932 | 00:00 | . 49 | 1.263181 | 0.459443 | 00:00 | . 50 | 1.263025 | 0.459690 | 00:00 | . 51 | 1.262869 | 0.457996 | 00:00 | . 52 | 1.262714 | 0.454969 | 00:00 | . 53 | 1.262558 | 0.451982 | 00:00 | . 54 | 1.262402 | 0.450564 | 00:00 | . 55 | 1.262247 | 0.450934 | 00:00 | . 56 | 1.262090 | 0.451861 | 00:00 | . 57 | 1.261933 | 0.451914 | 00:00 | . 58 | 1.261776 | 0.450721 | 00:00 | . 59 | 1.261619 | 0.448978 | 00:00 | . 60 | 1.261461 | 0.447796 | 00:00 | . 61 | 1.261303 | 0.448038 | 00:00 | . 62 | 1.261144 | 0.448761 | 00:00 | . 63 | 1.260986 | 0.449142 | 00:00 | . 64 | 1.260826 | 0.448443 | 00:00 | . 65 | 1.260667 | 0.446837 | 00:00 | . 66 | 1.260507 | 0.445661 | 00:00 | . 67 | 1.260347 | 0.445344 | 00:00 | . 68 | 1.260187 | 0.445592 | 00:00 | . 69 | 1.260026 | 0.445488 | 00:00 | . 70 | 1.259866 | 0.444427 | 00:00 | . 71 | 1.259705 | 0.442824 | 00:00 | . 72 | 1.259543 | 0.441615 | 00:00 | . 73 | 1.259382 | 0.441126 | 00:00 | . 74 | 1.259220 | 0.441023 | 00:00 | . 75 | 1.259058 | 0.440497 | 00:00 | . 76 | 1.258896 | 0.439592 | 00:00 | . 77 | 1.258733 | 0.438460 | 00:00 | . 78 | 1.258569 | 0.437588 | 00:00 | . 79 | 1.258405 | 0.437321 | 00:00 | . 80 | 1.258241 | 0.437219 | 00:00 | . 81 | 1.258077 | 0.436916 | 00:00 | . 82 | 1.257912 | 0.435913 | 00:00 | . 83 | 1.257747 | 0.435003 | 00:00 | . 84 | 1.257582 | 0.434601 | 00:00 | . 85 | 1.257416 | 0.434494 | 00:00 | . 86 | 1.257249 | 0.434309 | 00:00 | . 87 | 1.257081 | 0.433745 | 00:00 | . 88 | 1.256913 | 0.432914 | 00:00 | . 89 | 1.256744 | 0.432331 | 00:00 | . 90 | 1.256575 | 0.432165 | 00:00 | . 91 | 1.256406 | 0.432003 | 00:00 | . 92 | 1.256236 | 0.431670 | 00:00 | . 93 | 1.256065 | 0.430937 | 00:00 | . 94 | 1.255894 | 0.430317 | 00:00 | . 95 | 1.255723 | 0.429924 | 00:00 | . 96 | 1.255550 | 0.429707 | 00:00 | . 97 | 1.255377 | 0.429296 | 00:00 | . 98 | 1.255203 | 0.428846 | 00:00 | . 99 | 1.255029 | 0.428160 | 00:00 | . 100 | 1.254854 | 0.427743 | 00:00 | . 101 | 1.254679 | 0.427369 | 00:00 | . 102 | 1.254504 | 0.426952 | 00:00 | . 103 | 1.254328 | 0.426511 | 00:00 | . 104 | 1.254151 | 0.426140 | 00:00 | . 105 | 1.253973 | 0.425836 | 00:00 | . 106 | 1.253796 | 0.425516 | 00:00 | . 107 | 1.253617 | 0.425156 | 00:00 | . 108 | 1.253438 | 0.424890 | 00:00 | . 109 | 1.253259 | 0.424599 | 00:00 | . 110 | 1.253079 | 0.424250 | 00:00 | . 111 | 1.252898 | 0.423973 | 00:00 | . 112 | 1.252717 | 0.423872 | 00:00 | . 113 | 1.252535 | 0.423620 | 00:00 | . 114 | 1.252353 | 0.423358 | 00:00 | . 115 | 1.252170 | 0.422883 | 00:00 | . 116 | 1.251987 | 0.422549 | 00:00 | . 117 | 1.251803 | 0.422482 | 00:00 | . 118 | 1.251619 | 0.422277 | 00:00 | . 119 | 1.251435 | 0.421926 | 00:00 | . 120 | 1.251249 | 0.421529 | 00:00 | . 121 | 1.251063 | 0.421358 | 00:00 | . 122 | 1.250877 | 0.421251 | 00:00 | . 123 | 1.250690 | 0.421048 | 00:00 | . 124 | 1.250502 | 0.420763 | 00:00 | . 125 | 1.250314 | 0.420404 | 00:00 | . 126 | 1.250125 | 0.420322 | 00:00 | . 127 | 1.249936 | 0.420242 | 00:00 | . 128 | 1.249746 | 0.420147 | 00:00 | . 129 | 1.249556 | 0.419852 | 00:00 | . 130 | 1.249366 | 0.419579 | 00:00 | . 131 | 1.249175 | 0.419527 | 00:00 | . 132 | 1.248984 | 0.419416 | 00:00 | . 133 | 1.248792 | 0.419148 | 00:00 | . 134 | 1.248599 | 0.418997 | 00:00 | . 135 | 1.248406 | 0.418859 | 00:00 | . 136 | 1.248212 | 0.418857 | 00:00 | . 137 | 1.248018 | 0.418830 | 00:00 | . 138 | 1.247823 | 0.418669 | 00:00 | . 139 | 1.247628 | 0.418535 | 00:00 | . 140 | 1.247432 | 0.418488 | 00:00 | . 141 | 1.247236 | 0.418400 | 00:00 | . 142 | 1.247040 | 0.418214 | 00:00 | . 143 | 1.246843 | 0.417942 | 00:00 | . 144 | 1.246645 | 0.417894 | 00:00 | . 145 | 1.246448 | 0.417886 | 00:00 | . 146 | 1.246250 | 0.417820 | 00:00 | . 147 | 1.246051 | 0.417744 | 00:00 | . 148 | 1.245852 | 0.417791 | 00:00 | . 149 | 1.245651 | 0.417857 | 00:00 | . 150 | 1.245451 | 0.417884 | 00:00 | . 151 | 1.245250 | 0.417780 | 00:00 | . 152 | 1.245049 | 0.417736 | 00:00 | . 153 | 1.244848 | 0.417721 | 00:00 | . 154 | 1.244646 | 0.417662 | 00:00 | . 155 | 1.244443 | 0.417639 | 00:00 | . 156 | 1.244240 | 0.417623 | 00:00 | . 157 | 1.244037 | 0.417599 | 00:00 | . 158 | 1.243833 | 0.417624 | 00:00 | . 159 | 1.243629 | 0.417713 | 00:00 | . 160 | 1.243424 | 0.417719 | 00:00 | . 161 | 1.243219 | 0.417705 | 00:00 | . 162 | 1.243013 | 0.417843 | 00:00 | . 163 | 1.242807 | 0.417914 | 00:00 | . 164 | 1.242601 | 0.417929 | 00:00 | . 165 | 1.242394 | 0.417990 | 00:00 | . 166 | 1.242187 | 0.418116 | 00:00 | . 167 | 1.241980 | 0.418189 | 00:00 | . 168 | 1.241772 | 0.418205 | 00:00 | . 169 | 1.241564 | 0.418334 | 00:00 | . 170 | 1.241355 | 0.418501 | 00:00 | . 171 | 1.241146 | 0.418554 | 00:00 | . 172 | 1.240937 | 0.418608 | 00:00 | . 173 | 1.240727 | 0.418772 | 00:00 | . 174 | 1.240517 | 0.418854 | 00:00 | . 175 | 1.240307 | 0.418996 | 00:00 | . 176 | 1.240097 | 0.419114 | 00:00 | . 177 | 1.239886 | 0.419256 | 00:00 | . 178 | 1.239675 | 0.419356 | 00:00 | . 179 | 1.239463 | 0.419527 | 00:00 | . 180 | 1.239251 | 0.419626 | 00:00 | . 181 | 1.239039 | 0.419796 | 00:00 | . 182 | 1.238827 | 0.419984 | 00:00 | . 183 | 1.238615 | 0.420269 | 00:00 | . 184 | 1.238402 | 0.420389 | 00:00 | . 185 | 1.238188 | 0.420558 | 00:00 | . 186 | 1.237974 | 0.420761 | 00:00 | . 187 | 1.237759 | 0.420946 | 00:00 | . 188 | 1.237545 | 0.421110 | 00:00 | . 189 | 1.237331 | 0.421286 | 00:00 | . 190 | 1.237115 | 0.421507 | 00:00 | . 191 | 1.236900 | 0.421727 | 00:00 | . 192 | 1.236684 | 0.421919 | 00:00 | . 193 | 1.236467 | 0.422220 | 00:00 | . 194 | 1.236251 | 0.422527 | 00:00 | . 195 | 1.236034 | 0.422738 | 00:00 | . 196 | 1.235817 | 0.422963 | 00:00 | . 197 | 1.235600 | 0.423270 | 00:00 | . 198 | 1.235382 | 0.423566 | 00:00 | . 199 | 1.235164 | 0.423725 | 00:00 | . 200 | 1.234946 | 0.423986 | 00:00 | . 201 | 1.234727 | 0.424333 | 00:00 | . 202 | 1.234508 | 0.424390 | 00:00 | . 203 | 1.234289 | 0.424699 | 00:00 | . 204 | 1.234070 | 0.425270 | 00:00 | . 205 | 1.233850 | 0.425272 | 00:00 | . 206 | 1.233630 | 0.425684 | 00:00 | . 207 | 1.233410 | 0.426120 | 00:00 | . 208 | 1.233190 | 0.426429 | 00:00 | . 209 | 1.232970 | 0.426418 | 00:00 | . 210 | 1.232749 | 0.427115 | 00:00 | . 211 | 1.232528 | 0.427216 | 00:00 | . 212 | 1.232306 | 0.427429 | 00:00 | . 213 | 1.232085 | 0.427920 | 00:00 | . 214 | 1.231864 | 0.428236 | 00:00 | . 215 | 1.231642 | 0.428453 | 00:00 | . 216 | 1.231421 | 0.428856 | 00:00 | . 217 | 1.231198 | 0.429515 | 00:00 | . 218 | 1.230976 | 0.429647 | 00:00 | . 219 | 1.230753 | 0.430105 | 00:00 | . 220 | 1.230530 | 0.430761 | 00:00 | . 221 | 1.230307 | 0.430849 | 00:00 | . 222 | 1.230084 | 0.431280 | 00:00 | . 223 | 1.229861 | 0.431862 | 00:00 | . 224 | 1.229637 | 0.432191 | 00:00 | . 225 | 1.229413 | 0.432374 | 00:00 | . 226 | 1.229189 | 0.433027 | 00:00 | . 227 | 1.228966 | 0.433583 | 00:00 | . 228 | 1.228741 | 0.433802 | 00:00 | . 229 | 1.228517 | 0.434520 | 00:00 | . 230 | 1.228293 | 0.435066 | 00:00 | . 231 | 1.228068 | 0.435148 | 00:00 | . 232 | 1.227843 | 0.435727 | 00:00 | . 233 | 1.227618 | 0.436116 | 00:00 | . 234 | 1.227393 | 0.435958 | 00:00 | . 235 | 1.227168 | 0.436991 | 00:00 | . 236 | 1.226943 | 0.437511 | 00:00 | . 237 | 1.226718 | 0.438006 | 00:00 | . 238 | 1.226493 | 0.438966 | 00:00 | . 239 | 1.226267 | 0.439815 | 00:00 | . 240 | 1.226041 | 0.439775 | 00:00 | . 241 | 1.225815 | 0.440939 | 00:00 | . 242 | 1.225590 | 0.440882 | 00:00 | . 243 | 1.225363 | 0.440836 | 00:00 | . 244 | 1.225137 | 0.441480 | 00:00 | . 245 | 1.224910 | 0.441281 | 00:00 | . 246 | 1.224684 | 0.442244 | 00:00 | . 247 | 1.224457 | 0.442685 | 00:00 | . 248 | 1.224231 | 0.443636 | 00:00 | . 249 | 1.224004 | 0.444059 | 00:00 | . 250 | 1.223777 | 0.444930 | 00:00 | . 251 | 1.223551 | 0.445588 | 00:00 | . 252 | 1.223324 | 0.446594 | 00:00 | . 253 | 1.223097 | 0.446623 | 00:00 | . 254 | 1.222870 | 0.447728 | 00:00 | . 255 | 1.222642 | 0.447877 | 00:00 | . 256 | 1.222415 | 0.448117 | 00:00 | . 257 | 1.222188 | 0.449032 | 00:00 | . 258 | 1.221961 | 0.449322 | 00:00 | . 259 | 1.221733 | 0.449238 | 00:00 | . 260 | 1.221505 | 0.451040 | 00:00 | . 261 | 1.221278 | 0.450359 | 00:00 | . 262 | 1.221051 | 0.452112 | 00:00 | . 263 | 1.220824 | 0.452225 | 00:00 | . 264 | 1.220597 | 0.453696 | 00:00 | . 265 | 1.220369 | 0.454094 | 00:00 | . 266 | 1.220141 | 0.454912 | 00:00 | . 267 | 1.219914 | 0.455107 | 00:00 | . 268 | 1.219687 | 0.455415 | 00:00 | . 269 | 1.219459 | 0.455917 | 00:00 | . 270 | 1.219232 | 0.456291 | 00:00 | . 271 | 1.219005 | 0.457516 | 00:00 | . 272 | 1.218778 | 0.458215 | 00:00 | . 273 | 1.218550 | 0.459798 | 00:00 | . 274 | 1.218323 | 0.460129 | 00:00 | . 275 | 1.218096 | 0.461005 | 00:00 | . 276 | 1.217869 | 0.460792 | 00:00 | . 277 | 1.217642 | 0.461096 | 00:00 | . 278 | 1.217414 | 0.460790 | 00:00 | . 279 | 1.217186 | 0.462483 | 00:00 | . 280 | 1.216958 | 0.462127 | 00:00 | . 281 | 1.216730 | 0.466005 | 00:00 | . 282 | 1.216504 | 0.464130 | 00:00 | . 283 | 1.216277 | 0.469921 | 00:00 | . 284 | 1.216050 | 0.463971 | 00:00 | . 285 | 1.215824 | 0.471405 | 00:00 | . 286 | 1.215599 | 0.463746 | 00:00 | . 287 | 1.215374 | 0.471046 | 00:00 | . 288 | 1.215147 | 0.466031 | 00:00 | . 289 | 1.214920 | 0.469181 | 00:00 | . 290 | 1.214693 | 0.471406 | 00:00 | . 291 | 1.214465 | 0.469302 | 00:00 | . 292 | 1.214239 | 0.475704 | 00:00 | . 293 | 1.214013 | 0.471744 | 00:00 | . 294 | 1.213787 | 0.475277 | 00:00 | . 295 | 1.213560 | 0.475393 | 00:00 | . 296 | 1.213333 | 0.472734 | 00:00 | . 297 | 1.213107 | 0.477125 | 00:00 | . 298 | 1.212881 | 0.474237 | 00:00 | . 299 | 1.212655 | 0.477554 | 00:00 | . 300 | 1.212428 | 0.477674 | 00:00 | . 301 | 1.212203 | 0.478424 | 00:00 | . 302 | 1.211978 | 0.481764 | 00:00 | . 303 | 1.211752 | 0.479665 | 00:00 | . 304 | 1.211527 | 0.483109 | 00:00 | . 305 | 1.211300 | 0.481590 | 00:00 | . 306 | 1.211075 | 0.482610 | 00:00 | . 307 | 1.210849 | 0.484192 | 00:00 | . 308 | 1.210624 | 0.482331 | 00:00 | . 309 | 1.210399 | 0.487119 | 00:00 | . 310 | 1.210175 | 0.483336 | 00:00 | . 311 | 1.209951 | 0.488953 | 00:00 | . 312 | 1.209726 | 0.486667 | 00:00 | . 313 | 1.209502 | 0.489473 | 00:00 | . 314 | 1.209278 | 0.490365 | 00:00 | . 315 | 1.209055 | 0.488710 | 00:00 | . 316 | 1.208831 | 0.493933 | 00:00 | . 317 | 1.208609 | 0.488504 | 00:00 | . 318 | 1.208385 | 0.495865 | 00:00 | . 319 | 1.208164 | 0.490756 | 00:00 | . 320 | 1.207942 | 0.495374 | 00:00 | . 321 | 1.207719 | 0.495526 | 00:00 | . 322 | 1.207496 | 0.493535 | 00:00 | . 323 | 1.207274 | 0.500878 | 00:00 | . 324 | 1.207053 | 0.492885 | 00:00 | . 325 | 1.206832 | 0.502528 | 00:00 | . 326 | 1.206610 | 0.497001 | 00:00 | . 327 | 1.206388 | 0.499801 | 00:00 | . 328 | 1.206167 | 0.504342 | 00:00 | . 329 | 1.205946 | 0.498521 | 00:00 | . 330 | 1.205726 | 0.507436 | 00:00 | . 331 | 1.205506 | 0.502286 | 00:00 | . 332 | 1.205286 | 0.504370 | 00:00 | . 333 | 1.205066 | 0.506807 | 00:00 | . 334 | 1.204845 | 0.502806 | 00:00 | . 335 | 1.204626 | 0.508645 | 00:00 | . 336 | 1.204406 | 0.505454 | 00:00 | . 337 | 1.204187 | 0.507372 | 00:00 | . 338 | 1.203967 | 0.511078 | 00:00 | . 339 | 1.203748 | 0.508682 | 00:00 | . 340 | 1.203528 | 0.515019 | 00:00 | . 341 | 1.203309 | 0.511679 | 00:00 | . 342 | 1.203090 | 0.515847 | 00:00 | . 343 | 1.202872 | 0.514210 | 00:00 | . 344 | 1.202654 | 0.513550 | 00:00 | . 345 | 1.202437 | 0.517338 | 00:00 | . 346 | 1.202219 | 0.513895 | 00:00 | . 347 | 1.202002 | 0.518733 | 00:00 | . 348 | 1.201786 | 0.517710 | 00:00 | . 349 | 1.201569 | 0.519124 | 00:00 | . 350 | 1.201353 | 0.523362 | 00:00 | . 351 | 1.201136 | 0.521150 | 00:00 | . 352 | 1.200921 | 0.526446 | 00:00 | . 353 | 1.200706 | 0.522027 | 00:00 | . 354 | 1.200491 | 0.526017 | 00:00 | . 355 | 1.200276 | 0.522019 | 00:00 | . 356 | 1.200061 | 0.525617 | 00:00 | . 357 | 1.199846 | 0.524996 | 00:00 | . 358 | 1.199632 | 0.526826 | 00:00 | . 359 | 1.199418 | 0.530623 | 00:00 | . 360 | 1.199204 | 0.529384 | 00:00 | . 361 | 1.198991 | 0.534076 | 00:00 | . 362 | 1.198778 | 0.529402 | 00:00 | . 363 | 1.198565 | 0.536225 | 00:00 | . 364 | 1.198352 | 0.531828 | 00:00 | . 365 | 1.198140 | 0.536218 | 00:00 | . 366 | 1.197927 | 0.537211 | 00:00 | . 367 | 1.197714 | 0.535652 | 00:00 | . 368 | 1.197502 | 0.542194 | 00:00 | . 369 | 1.197290 | 0.534898 | 00:00 | . 370 | 1.197079 | 0.546001 | 00:00 | . 371 | 1.196868 | 0.534406 | 00:00 | . 372 | 1.196657 | 0.546841 | 00:00 | . 373 | 1.196448 | 0.538652 | 00:00 | . 374 | 1.196237 | 0.543475 | 00:00 | . 375 | 1.196027 | 0.545992 | 00:00 | . 376 | 1.195816 | 0.540382 | 00:00 | . 377 | 1.195607 | 0.551180 | 00:00 | . 378 | 1.195398 | 0.543836 | 00:00 | . 379 | 1.195188 | 0.549081 | 00:00 | . 380 | 1.194979 | 0.552940 | 00:00 | . 381 | 1.194771 | 0.546051 | 00:00 | . 382 | 1.194563 | 0.556583 | 00:00 | . 383 | 1.194356 | 0.547764 | 00:00 | . 384 | 1.194149 | 0.554733 | 00:00 | . 385 | 1.193941 | 0.554931 | 00:00 | . 386 | 1.193734 | 0.551797 | 00:00 | . 387 | 1.193528 | 0.559369 | 00:00 | . 388 | 1.193321 | 0.554356 | 00:00 | . 389 | 1.193116 | 0.557717 | 00:00 | . 390 | 1.192910 | 0.559447 | 00:00 | . 391 | 1.192706 | 0.555653 | 00:00 | . 392 | 1.192500 | 0.563805 | 00:00 | . 393 | 1.192295 | 0.556521 | 00:00 | . 394 | 1.192092 | 0.564368 | 00:00 | . 395 | 1.191887 | 0.561668 | 00:00 | . 396 | 1.191683 | 0.560722 | 00:00 | . 397 | 1.191478 | 0.567270 | 00:00 | . 398 | 1.191274 | 0.559664 | 00:00 | . 399 | 1.191071 | 0.569009 | 00:00 | . 400 | 1.190868 | 0.563938 | 00:00 | . 401 | 1.190666 | 0.567402 | 00:00 | . 402 | 1.190463 | 0.573133 | 00:00 | . 403 | 1.190261 | 0.566651 | 00:00 | . 404 | 1.190060 | 0.576093 | 00:00 | . 405 | 1.189860 | 0.567995 | 00:00 | . 406 | 1.189659 | 0.571855 | 00:00 | . 407 | 1.189458 | 0.574249 | 00:00 | . 408 | 1.189258 | 0.569290 | 00:00 | . 409 | 1.189058 | 0.579131 | 00:00 | . 410 | 1.188859 | 0.572503 | 00:00 | . 411 | 1.188658 | 0.578302 | 00:00 | . 412 | 1.188460 | 0.576851 | 00:00 | . 413 | 1.188261 | 0.573910 | 00:00 | . 414 | 1.188061 | 0.581201 | 00:00 | . 415 | 1.187863 | 0.573142 | 00:00 | . 416 | 1.187665 | 0.583227 | 00:00 | . 417 | 1.187467 | 0.580256 | 00:00 | . 418 | 1.187269 | 0.582546 | 00:00 | . 419 | 1.187071 | 0.586348 | 00:00 | . 420 | 1.186874 | 0.579812 | 00:00 | . 421 | 1.186677 | 0.589364 | 00:00 | . 422 | 1.186480 | 0.580780 | 00:00 | . 423 | 1.186284 | 0.589222 | 00:00 | . 424 | 1.186088 | 0.587141 | 00:00 | . 425 | 1.185891 | 0.589203 | 00:00 | . 426 | 1.185696 | 0.591853 | 00:00 | . 427 | 1.185501 | 0.588281 | 00:00 | . 428 | 1.185305 | 0.593388 | 00:00 | . 429 | 1.185110 | 0.589403 | 00:00 | . 430 | 1.184916 | 0.595557 | 00:00 | . 431 | 1.184721 | 0.592521 | 00:00 | . 432 | 1.184529 | 0.601984 | 00:00 | . 433 | 1.184336 | 0.594591 | 00:00 | . 434 | 1.184142 | 0.605421 | 00:00 | . 435 | 1.183950 | 0.596454 | 00:00 | . 436 | 1.183757 | 0.604492 | 00:00 | . 437 | 1.183564 | 0.599748 | 00:00 | . 438 | 1.183372 | 0.601403 | 00:00 | . 439 | 1.183180 | 0.605842 | 00:00 | . 440 | 1.182988 | 0.603062 | 00:00 | . 441 | 1.182797 | 0.611140 | 00:00 | . 442 | 1.182606 | 0.604043 | 00:00 | . 443 | 1.182416 | 0.611879 | 00:00 | . 444 | 1.182226 | 0.604252 | 00:00 | . 445 | 1.182036 | 0.611922 | 00:00 | . 446 | 1.181846 | 0.607113 | 00:00 | . 447 | 1.181655 | 0.612432 | 00:00 | . 448 | 1.181467 | 0.613410 | 00:00 | . 449 | 1.181278 | 0.613533 | 00:00 | . 450 | 1.181088 | 0.619271 | 00:00 | . 451 | 1.180901 | 0.614651 | 00:00 | . 452 | 1.180712 | 0.623310 | 00:00 | . 453 | 1.180524 | 0.613269 | 00:00 | . 454 | 1.180337 | 0.624101 | 00:00 | . 455 | 1.180150 | 0.612796 | 00:00 | . 456 | 1.179964 | 0.625325 | 00:00 | . 457 | 1.179777 | 0.617129 | 00:00 | . 458 | 1.179590 | 0.625354 | 00:00 | . 459 | 1.179404 | 0.621866 | 00:00 | . 460 | 1.179218 | 0.624785 | 00:00 | . 461 | 1.179033 | 0.626110 | 00:00 | . 462 | 1.178848 | 0.625076 | 00:00 | . 463 | 1.178664 | 0.628602 | 00:00 | . 464 | 1.178480 | 0.628231 | 00:00 | . 465 | 1.178295 | 0.629565 | 00:00 | . 466 | 1.178110 | 0.634869 | 00:00 | . 467 | 1.177927 | 0.628839 | 00:00 | . 468 | 1.177743 | 0.639646 | 00:00 | . 469 | 1.177561 | 0.628106 | 00:00 | . 470 | 1.177379 | 0.642643 | 00:00 | . 471 | 1.177198 | 0.626533 | 00:00 | . 472 | 1.177018 | 0.645775 | 00:00 | . 473 | 1.176837 | 0.630790 | 00:00 | . 474 | 1.176656 | 0.645833 | 00:00 | . 475 | 1.176476 | 0.639144 | 00:00 | . 476 | 1.176294 | 0.642675 | 00:00 | . 477 | 1.176114 | 0.644251 | 00:00 | . 478 | 1.175933 | 0.639224 | 00:00 | . 479 | 1.175753 | 0.646021 | 00:00 | . 480 | 1.175574 | 0.638502 | 00:00 | . 481 | 1.175395 | 0.648855 | 00:00 | . 482 | 1.175216 | 0.641207 | 00:00 | . 483 | 1.175039 | 0.651341 | 00:00 | . 484 | 1.174861 | 0.647684 | 00:00 | . 485 | 1.174683 | 0.652326 | 00:00 | . 486 | 1.174505 | 0.653870 | 00:00 | . 487 | 1.174329 | 0.649032 | 00:00 | . 488 | 1.174153 | 0.655512 | 00:00 | . 489 | 1.173977 | 0.649586 | 00:00 | . 490 | 1.173801 | 0.654173 | 00:00 | . 491 | 1.173624 | 0.652167 | 00:00 | . 492 | 1.173448 | 0.655364 | 00:00 | . 493 | 1.173273 | 0.656568 | 00:00 | . 494 | 1.173098 | 0.658468 | 00:00 | . 495 | 1.172923 | 0.660450 | 00:00 | . 496 | 1.172747 | 0.658418 | 00:00 | . 497 | 1.172574 | 0.664447 | 00:00 | . 498 | 1.172400 | 0.655454 | 00:00 | . 499 | 1.172228 | 0.666339 | 00:00 | . 500 | 1.172055 | 0.654350 | 00:00 | . 501 | 1.171883 | 0.667965 | 00:00 | . 502 | 1.171710 | 0.660691 | 00:00 | . 503 | 1.171538 | 0.666292 | 00:00 | . 504 | 1.171365 | 0.669763 | 00:00 | . 505 | 1.171193 | 0.661788 | 00:00 | . 506 | 1.171022 | 0.676019 | 00:00 | . 507 | 1.170852 | 0.656674 | 00:00 | . 508 | 1.170684 | 0.678302 | 00:00 | . 509 | 1.170515 | 0.661348 | 00:00 | . 510 | 1.170346 | 0.669641 | 00:00 | . 511 | 1.170176 | 0.674612 | 00:00 | . 512 | 1.170005 | 0.663549 | 00:00 | . 513 | 1.169838 | 0.679929 | 00:00 | . 514 | 1.169668 | 0.670535 | 00:00 | . 515 | 1.169498 | 0.671963 | 00:00 | . 516 | 1.169330 | 0.680020 | 00:00 | . 517 | 1.169162 | 0.665240 | 00:00 | . 518 | 1.168995 | 0.679197 | 00:00 | . 519 | 1.168829 | 0.670004 | 00:00 | . 520 | 1.168662 | 0.674645 | 00:00 | . 521 | 1.168495 | 0.675399 | 00:00 | . 522 | 1.168327 | 0.675977 | 00:00 | . 523 | 1.168159 | 0.680865 | 00:00 | . 524 | 1.167992 | 0.681353 | 00:00 | . 525 | 1.167827 | 0.680745 | 00:00 | . 526 | 1.167661 | 0.685391 | 00:00 | . 527 | 1.167495 | 0.679892 | 00:00 | . 528 | 1.167331 | 0.687801 | 00:00 | . 529 | 1.167167 | 0.679404 | 00:00 | . 530 | 1.167005 | 0.689592 | 00:00 | . 531 | 1.166840 | 0.682430 | 00:00 | . 532 | 1.166679 | 0.689441 | 00:00 | . 533 | 1.166516 | 0.686591 | 00:00 | . 534 | 1.166354 | 0.690570 | 00:00 | . 535 | 1.166191 | 0.690022 | 00:00 | . 536 | 1.166030 | 0.688995 | 00:00 | . 537 | 1.165868 | 0.695726 | 00:00 | . 538 | 1.165707 | 0.692526 | 00:00 | . 539 | 1.165547 | 0.702132 | 00:00 | . 540 | 1.165386 | 0.699653 | 00:00 | . 541 | 1.165227 | 0.706975 | 00:00 | . 542 | 1.165067 | 0.704185 | 00:00 | . 543 | 1.164908 | 0.705872 | 00:00 | . 544 | 1.164748 | 0.704565 | 00:00 | . 545 | 1.164590 | 0.697399 | 00:00 | . 546 | 1.164433 | 0.709462 | 00:00 | . 547 | 1.164274 | 0.692216 | 00:00 | . 548 | 1.164118 | 0.717534 | 00:00 | . 549 | 1.163962 | 0.695129 | 00:00 | . 550 | 1.163807 | 0.712931 | 00:00 | . 551 | 1.163651 | 0.705726 | 00:00 | . 552 | 1.163493 | 0.702003 | 00:00 | . 553 | 1.163336 | 0.710318 | 00:00 | . 554 | 1.163179 | 0.698091 | 00:00 | . 555 | 1.163025 | 0.711031 | 00:00 | . 556 | 1.162869 | 0.706409 | 00:00 | . 557 | 1.162713 | 0.713703 | 00:00 | . 558 | 1.162558 | 0.719242 | 00:00 | . 559 | 1.162403 | 0.711609 | 00:00 | . 560 | 1.162249 | 0.724058 | 00:00 | . 561 | 1.162096 | 0.712843 | 00:00 | . 562 | 1.161943 | 0.721852 | 00:00 | . 563 | 1.161789 | 0.718676 | 00:00 | . 564 | 1.161636 | 0.721415 | 00:00 | . 565 | 1.161484 | 0.720994 | 00:00 | . 566 | 1.161332 | 0.720838 | 00:00 | . 567 | 1.161180 | 0.723023 | 00:00 | . 568 | 1.161029 | 0.721188 | 00:00 | . 569 | 1.160877 | 0.723265 | 00:00 | . 570 | 1.160725 | 0.724430 | 00:00 | . 571 | 1.160574 | 0.727131 | 00:00 | . 572 | 1.160422 | 0.727649 | 00:00 | . 573 | 1.160273 | 0.728649 | 00:00 | . 574 | 1.160122 | 0.727610 | 00:00 | . 575 | 1.159973 | 0.728769 | 00:00 | . 576 | 1.159824 | 0.731067 | 00:00 | . 577 | 1.159676 | 0.734913 | 00:00 | . 578 | 1.159526 | 0.735820 | 00:00 | . 579 | 1.159379 | 0.737612 | 00:00 | . 580 | 1.159229 | 0.735839 | 00:00 | . 581 | 1.159080 | 0.735314 | 00:00 | . 582 | 1.158932 | 0.733589 | 00:00 | . 583 | 1.158784 | 0.735477 | 00:00 | . 584 | 1.158638 | 0.732117 | 00:00 | . 585 | 1.158491 | 0.737441 | 00:00 | . 586 | 1.158345 | 0.734808 | 00:00 | . 587 | 1.158200 | 0.743212 | 00:00 | . 588 | 1.158056 | 0.740217 | 00:00 | . 589 | 1.157910 | 0.746061 | 00:00 | . 590 | 1.157764 | 0.745176 | 00:00 | . 591 | 1.157619 | 0.745762 | 00:00 | . 592 | 1.157475 | 0.744707 | 00:00 | . 593 | 1.157332 | 0.742506 | 00:00 | . 594 | 1.157188 | 0.748639 | 00:00 | . 595 | 1.157044 | 0.740185 | 00:00 | . 596 | 1.156901 | 0.757565 | 00:00 | . 597 | 1.156759 | 0.736161 | 00:00 | . 598 | 1.156619 | 0.771549 | 00:00 | . 599 | 1.156482 | 0.735059 | 00:00 | . 600 | 1.156345 | 0.769085 | 00:00 | . 601 | 1.156207 | 0.750762 | 00:00 | . 602 | 1.156066 | 0.744747 | 00:00 | . 603 | 1.155928 | 0.774248 | 00:00 | . 604 | 1.155791 | 0.742776 | 00:00 | . 605 | 1.155653 | 0.764062 | 00:00 | . 606 | 1.155515 | 0.768612 | 00:00 | . 607 | 1.155378 | 0.745709 | 00:00 | . 608 | 1.155242 | 0.772326 | 00:00 | . 609 | 1.155105 | 0.762058 | 00:00 | . 610 | 1.154967 | 0.749233 | 00:00 | . 611 | 1.154831 | 0.768939 | 00:00 | . 612 | 1.154693 | 0.753700 | 00:00 | . 613 | 1.154555 | 0.754060 | 00:00 | . 614 | 1.154418 | 0.769759 | 00:00 | . 615 | 1.154282 | 0.756170 | 00:00 | . 616 | 1.154146 | 0.766465 | 00:00 | . 617 | 1.154011 | 0.772657 | 00:00 | . 618 | 1.153876 | 0.769429 | 00:00 | . 619 | 1.153741 | 0.771619 | 00:00 | . 620 | 1.153608 | 0.770932 | 00:00 | . 621 | 1.153476 | 0.768476 | 00:00 | . 622 | 1.153342 | 0.770343 | 00:00 | . 623 | 1.153209 | 0.768920 | 00:00 | . 624 | 1.153077 | 0.772649 | 00:00 | . 625 | 1.152945 | 0.773119 | 00:00 | . 626 | 1.152812 | 0.771454 | 00:00 | . 627 | 1.152680 | 0.778261 | 00:00 | . 628 | 1.152548 | 0.776608 | 00:00 | . 629 | 1.152418 | 0.773290 | 00:00 | . 630 | 1.152286 | 0.780656 | 00:00 | . 631 | 1.152156 | 0.775731 | 00:00 | . 632 | 1.152025 | 0.779517 | 00:00 | . 633 | 1.151896 | 0.778083 | 00:00 | . 634 | 1.151767 | 0.775307 | 00:00 | . 635 | 1.151638 | 0.782973 | 00:00 | . 636 | 1.151511 | 0.772681 | 00:00 | . 637 | 1.151383 | 0.786697 | 00:00 | . 638 | 1.151256 | 0.781338 | 00:00 | . 639 | 1.151129 | 0.779945 | 00:00 | . 640 | 1.151001 | 0.796323 | 00:00 | . 641 | 1.150876 | 0.779720 | 00:00 | . 642 | 1.150750 | 0.793527 | 00:00 | . 643 | 1.150625 | 0.792330 | 00:00 | . 644 | 1.150499 | 0.773774 | 00:00 | . 645 | 1.150375 | 0.800118 | 00:00 | . 646 | 1.150253 | 0.781727 | 00:00 | . 647 | 1.150127 | 0.789161 | 00:00 | . 648 | 1.150002 | 0.807146 | 00:00 | . 649 | 1.149879 | 0.785683 | 00:00 | . 650 | 1.149758 | 0.801186 | 00:00 | . 651 | 1.149637 | 0.799043 | 00:00 | . 652 | 1.149514 | 0.784458 | 00:00 | . 653 | 1.149392 | 0.804605 | 00:00 | . 654 | 1.149269 | 0.793532 | 00:00 | . 655 | 1.149148 | 0.788827 | 00:00 | . 656 | 1.149027 | 0.810499 | 00:00 | . 657 | 1.148907 | 0.784906 | 00:00 | . 658 | 1.148789 | 0.797004 | 00:00 | . 659 | 1.148669 | 0.808318 | 00:00 | . 660 | 1.148550 | 0.790818 | 00:00 | . 661 | 1.148430 | 0.807112 | 00:00 | . 662 | 1.148311 | 0.807896 | 00:00 | . 663 | 1.148192 | 0.793612 | 00:00 | . 664 | 1.148074 | 0.814762 | 00:00 | . 665 | 1.147955 | 0.803445 | 00:00 | . 666 | 1.147837 | 0.797560 | 00:00 | . 667 | 1.147718 | 0.811713 | 00:00 | . 668 | 1.147601 | 0.799508 | 00:00 | . 669 | 1.147484 | 0.799374 | 00:00 | . 670 | 1.147368 | 0.814020 | 00:00 | . 671 | 1.147251 | 0.808156 | 00:00 | . 672 | 1.147136 | 0.812753 | 00:00 | . 673 | 1.147021 | 0.819477 | 00:00 | . 674 | 1.146906 | 0.804505 | 00:00 | . 675 | 1.146790 | 0.817322 | 00:00 | . 676 | 1.146675 | 0.806367 | 00:00 | . 677 | 1.146561 | 0.804483 | 00:00 | . 678 | 1.146447 | 0.817632 | 00:00 | . 679 | 1.146334 | 0.804266 | 00:00 | . 680 | 1.146220 | 0.818144 | 00:00 | . 681 | 1.146108 | 0.816939 | 00:00 | . 682 | 1.145994 | 0.809324 | 00:00 | . 683 | 1.145882 | 0.824353 | 00:00 | . 684 | 1.145771 | 0.814552 | 00:00 | . 685 | 1.145658 | 0.812293 | 00:00 | . 686 | 1.145546 | 0.823278 | 00:00 | . 687 | 1.145435 | 0.812532 | 00:00 | . 688 | 1.145323 | 0.817525 | 00:00 | . 689 | 1.145211 | 0.820862 | 00:00 | . 690 | 1.145103 | 0.814268 | 00:00 | . 691 | 1.144992 | 0.826711 | 00:00 | . 692 | 1.144881 | 0.825731 | 00:00 | . 693 | 1.144772 | 0.825377 | 00:00 | . 694 | 1.144664 | 0.831147 | 00:00 | . 695 | 1.144554 | 0.825554 | 00:00 | . 696 | 1.144444 | 0.822579 | 00:00 | . 697 | 1.144335 | 0.827216 | 00:00 | . 698 | 1.144226 | 0.818370 | 00:00 | . 699 | 1.144118 | 0.824985 | 00:00 | . 700 | 1.144011 | 0.827606 | 00:00 | . 701 | 1.143903 | 0.825293 | 00:00 | . 702 | 1.143797 | 0.824510 | 00:00 | . 703 | 1.143687 | 0.831842 | 00:00 | . 704 | 1.143579 | 0.819512 | 00:00 | . 705 | 1.143472 | 0.831834 | 00:00 | . 706 | 1.143366 | 0.830589 | 00:00 | . 707 | 1.143260 | 0.823436 | 00:00 | . 708 | 1.143155 | 0.841350 | 00:00 | . 709 | 1.143051 | 0.821013 | 00:00 | . 710 | 1.142948 | 0.841491 | 00:00 | . 711 | 1.142844 | 0.833130 | 00:00 | . 712 | 1.142738 | 0.830558 | 00:00 | . 713 | 1.142634 | 0.839617 | 00:00 | . 714 | 1.142530 | 0.829297 | 00:00 | . 715 | 1.142426 | 0.832571 | 00:00 | . 716 | 1.142321 | 0.838219 | 00:00 | . 717 | 1.142219 | 0.824259 | 00:00 | . 718 | 1.142117 | 0.849616 | 00:00 | . 719 | 1.142013 | 0.829544 | 00:00 | . 720 | 1.141912 | 0.837785 | 00:00 | . 721 | 1.141808 | 0.839404 | 00:00 | . 722 | 1.141706 | 0.824520 | 00:00 | . 723 | 1.141604 | 0.850135 | 00:00 | . 724 | 1.141504 | 0.831047 | 00:00 | . 725 | 1.141403 | 0.846186 | 00:00 | . 726 | 1.141299 | 0.846048 | 00:00 | . 727 | 1.141198 | 0.837446 | 00:00 | . 728 | 1.141097 | 0.848955 | 00:00 | . 729 | 1.140998 | 0.837740 | 00:00 | . 730 | 1.140897 | 0.840581 | 00:00 | . 731 | 1.140797 | 0.843217 | 00:00 | . 732 | 1.140696 | 0.836666 | 00:00 | . 733 | 1.140596 | 0.849162 | 00:00 | . 734 | 1.140497 | 0.838051 | 00:00 | . 735 | 1.140396 | 0.845237 | 00:00 | . 736 | 1.140297 | 0.843339 | 00:00 | . 737 | 1.140200 | 0.846230 | 00:00 | . 738 | 1.140100 | 0.844921 | 00:00 | . 739 | 1.140002 | 0.848239 | 00:00 | . 740 | 1.139903 | 0.843349 | 00:00 | . 741 | 1.139805 | 0.852105 | 00:00 | . 742 | 1.139708 | 0.842042 | 00:00 | . 743 | 1.139609 | 0.856012 | 00:00 | . 744 | 1.139512 | 0.842623 | 00:00 | . 745 | 1.139416 | 0.848598 | 00:00 | . 746 | 1.139319 | 0.849762 | 00:00 | . 747 | 1.139220 | 0.843101 | 00:00 | . 748 | 1.139124 | 0.856936 | 00:00 | . 749 | 1.139028 | 0.850956 | 00:00 | . 750 | 1.138933 | 0.857461 | 00:00 | . 751 | 1.138837 | 0.850585 | 00:00 | . 752 | 1.138741 | 0.859319 | 00:00 | . 753 | 1.138645 | 0.847639 | 00:00 | . 754 | 1.138551 | 0.863867 | 00:00 | . 755 | 1.138455 | 0.844789 | 00:00 | . 756 | 1.138359 | 0.864412 | 00:00 | . 757 | 1.138262 | 0.849402 | 00:00 | . 758 | 1.138168 | 0.854914 | 00:00 | . 759 | 1.138074 | 0.858879 | 00:00 | . 760 | 1.137979 | 0.853253 | 00:00 | . 761 | 1.137886 | 0.868428 | 00:00 | . 762 | 1.137792 | 0.849593 | 00:00 | . 763 | 1.137699 | 0.869901 | 00:00 | . 764 | 1.137607 | 0.850486 | 00:00 | . 765 | 1.137514 | 0.863039 | 00:00 | . 766 | 1.137419 | 0.854652 | 00:00 | . 767 | 1.137325 | 0.856614 | 00:00 | . 768 | 1.137231 | 0.861490 | 00:00 | . 769 | 1.137138 | 0.855539 | 00:00 | . 770 | 1.137045 | 0.865625 | 00:00 | . 771 | 1.136952 | 0.858192 | 00:00 | . 772 | 1.136857 | 0.865162 | 00:00 | . 773 | 1.136762 | 0.862942 | 00:00 | . 774 | 1.136669 | 0.863200 | 00:00 | . 775 | 1.136577 | 0.866388 | 00:00 | . 776 | 1.136485 | 0.860678 | 00:00 | . 777 | 1.136392 | 0.864243 | 00:00 | . 778 | 1.136300 | 0.855608 | 00:00 | . 779 | 1.136209 | 0.864398 | 00:00 | . 780 | 1.136120 | 0.860409 | 00:00 | . 781 | 1.136030 | 0.872384 | 00:00 | . 782 | 1.135939 | 0.862076 | 00:00 | . 783 | 1.135848 | 0.874728 | 00:00 | . 784 | 1.135755 | 0.861477 | 00:00 | . 785 | 1.135663 | 0.874420 | 00:00 | . 786 | 1.135571 | 0.861232 | 00:00 | . 787 | 1.135479 | 0.870913 | 00:00 | . 788 | 1.135388 | 0.868093 | 00:00 | . 789 | 1.135296 | 0.875822 | 00:00 | . 790 | 1.135203 | 0.873967 | 00:00 | . 791 | 1.135110 | 0.871325 | 00:00 | . 792 | 1.135015 | 0.876681 | 00:00 | . 793 | 1.134925 | 0.859543 | 00:00 | . 794 | 1.134835 | 0.879577 | 00:00 | . 795 | 1.134743 | 0.864079 | 00:00 | . 796 | 1.134653 | 0.892693 | 00:00 | . 797 | 1.134563 | 0.860914 | 00:00 | . 798 | 1.134474 | 0.892006 | 00:00 | . 799 | 1.134385 | 0.854810 | 00:00 | . 800 | 1.134295 | 0.877297 | 00:00 | . 801 | 1.134205 | 0.867235 | 00:00 | . 802 | 1.134112 | 0.863984 | 00:00 | . 803 | 1.134019 | 0.879986 | 00:00 | . 804 | 1.133928 | 0.861217 | 00:00 | . 805 | 1.133838 | 0.876992 | 00:00 | . 806 | 1.133745 | 0.869224 | 00:00 | . 807 | 1.133653 | 0.869163 | 00:00 | . 808 | 1.133562 | 0.874003 | 00:00 | . 809 | 1.133471 | 0.865148 | 00:00 | . 810 | 1.133381 | 0.873356 | 00:00 | . 811 | 1.133292 | 0.863060 | 00:00 | . 812 | 1.133201 | 0.868399 | 00:00 | . 813 | 1.133111 | 0.864882 | 00:00 | . 814 | 1.133021 | 0.867160 | 00:00 | . 815 | 1.132932 | 0.865521 | 00:00 | . 816 | 1.132843 | 0.873941 | 00:00 | . 817 | 1.132755 | 0.860558 | 00:00 | . 818 | 1.132668 | 0.879268 | 00:00 | . 819 | 1.132582 | 0.852869 | 00:00 | . 820 | 1.132495 | 0.872444 | 00:00 | . 821 | 1.132408 | 0.855299 | 00:00 | . 822 | 1.132323 | 0.862534 | 00:00 | . 823 | 1.132236 | 0.872384 | 00:00 | . 824 | 1.132149 | 0.853979 | 00:00 | . 825 | 1.132063 | 0.873976 | 00:00 | . 826 | 1.131977 | 0.853876 | 00:00 | . 827 | 1.131892 | 0.866198 | 00:00 | . 828 | 1.131807 | 0.870824 | 00:00 | . 829 | 1.131720 | 0.854757 | 00:00 | . 830 | 1.131633 | 0.873532 | 00:00 | . 831 | 1.131549 | 0.853658 | 00:00 | . 832 | 1.131464 | 0.869641 | 00:00 | . 833 | 1.131376 | 0.863078 | 00:00 | . 834 | 1.131289 | 0.861533 | 00:00 | . 835 | 1.131203 | 0.870369 | 00:00 | . 836 | 1.131117 | 0.859380 | 00:00 | . 837 | 1.131034 | 0.858565 | 00:00 | . 838 | 1.130949 | 0.868208 | 00:00 | . 839 | 1.130866 | 0.854680 | 00:00 | . 840 | 1.130783 | 0.874185 | 00:00 | . 841 | 1.130700 | 0.859076 | 00:00 | . 842 | 1.130616 | 0.863381 | 00:00 | . 843 | 1.130533 | 0.857904 | 00:00 | . 844 | 1.130450 | 0.857336 | 00:00 | . 845 | 1.130367 | 0.860589 | 00:00 | . 846 | 1.130286 | 0.871394 | 00:00 | . 847 | 1.130202 | 0.852711 | 00:00 | . 848 | 1.130119 | 0.870563 | 00:00 | . 849 | 1.130038 | 0.847779 | 00:00 | . 850 | 1.129956 | 0.860374 | 00:00 | . 851 | 1.129876 | 0.857340 | 00:00 | . 852 | 1.129794 | 0.850603 | 00:00 | . 853 | 1.129714 | 0.864460 | 00:00 | . 854 | 1.129635 | 0.854669 | 00:00 | . 855 | 1.129553 | 0.858434 | 00:00 | . 856 | 1.129472 | 0.864192 | 00:00 | . 857 | 1.129392 | 0.849015 | 00:00 | . 858 | 1.129312 | 0.867516 | 00:00 | . 859 | 1.129233 | 0.842211 | 00:00 | . 860 | 1.129155 | 0.862945 | 00:00 | . 861 | 1.129078 | 0.853798 | 00:00 | . 862 | 1.128999 | 0.858534 | 00:00 | . 863 | 1.128921 | 0.859560 | 00:00 | . 864 | 1.128842 | 0.857198 | 00:00 | . 865 | 1.128763 | 0.857788 | 00:00 | . 866 | 1.128684 | 0.856849 | 00:00 | . 867 | 1.128606 | 0.854766 | 00:00 | . 868 | 1.128528 | 0.859631 | 00:00 | . 869 | 1.128451 | 0.860161 | 00:00 | . 870 | 1.128372 | 0.853523 | 00:00 | . 871 | 1.128294 | 0.858132 | 00:00 | . 872 | 1.128217 | 0.848801 | 00:00 | . 873 | 1.128139 | 0.858670 | 00:00 | . 874 | 1.128064 | 0.853983 | 00:00 | . 875 | 1.127988 | 0.858677 | 00:00 | . 876 | 1.127911 | 0.857006 | 00:00 | . 877 | 1.127835 | 0.849795 | 00:00 | . 878 | 1.127759 | 0.854704 | 00:00 | . 879 | 1.127682 | 0.848892 | 00:00 | . 880 | 1.127607 | 0.855406 | 00:00 | . 881 | 1.127532 | 0.850048 | 00:00 | . 882 | 1.127455 | 0.848791 | 00:00 | . 883 | 1.127379 | 0.853477 | 00:00 | . 884 | 1.127305 | 0.840029 | 00:00 | . 885 | 1.127230 | 0.868410 | 00:00 | . 886 | 1.127156 | 0.831517 | 00:00 | . 887 | 1.127086 | 0.881237 | 00:00 | . 888 | 1.127015 | 0.835450 | 00:00 | . 889 | 1.126945 | 0.856970 | 00:00 | . 890 | 1.126871 | 0.860736 | 00:00 | . 891 | 1.126796 | 0.829224 | 00:00 | . 892 | 1.126725 | 0.863013 | 00:00 | . 893 | 1.126655 | 0.850852 | 00:00 | . 894 | 1.126581 | 0.837041 | 00:00 | . 895 | 1.126509 | 0.864583 | 00:00 | . 896 | 1.126438 | 0.842915 | 00:00 | . 897 | 1.126366 | 0.838824 | 00:00 | . 898 | 1.126293 | 0.864659 | 00:00 | . 899 | 1.126220 | 0.842312 | 00:00 | . 900 | 1.126149 | 0.850549 | 00:00 | . 901 | 1.126076 | 0.860747 | 00:00 | . 902 | 1.126004 | 0.835979 | 00:00 | . 903 | 1.125931 | 0.847959 | 00:00 | . 904 | 1.125860 | 0.851000 | 00:00 | . 905 | 1.125789 | 0.836149 | 00:00 | . 906 | 1.125719 | 0.850032 | 00:00 | . 907 | 1.125648 | 0.847336 | 00:00 | . 908 | 1.125576 | 0.837743 | 00:00 | . 909 | 1.125505 | 0.852608 | 00:00 | . 910 | 1.125436 | 0.846568 | 00:00 | . 911 | 1.125366 | 0.840082 | 00:00 | . 912 | 1.125297 | 0.852738 | 00:00 | . 913 | 1.125231 | 0.839086 | 00:00 | . 914 | 1.125161 | 0.844776 | 00:00 | . 915 | 1.125091 | 0.853013 | 00:00 | . 916 | 1.125024 | 0.843607 | 00:00 | . 917 | 1.124954 | 0.843370 | 00:00 | . 918 | 1.124885 | 0.851984 | 00:00 | . 919 | 1.124818 | 0.830260 | 00:00 | . 920 | 1.124753 | 0.848803 | 00:00 | . 921 | 1.124686 | 0.848179 | 00:00 | . 922 | 1.124618 | 0.836883 | 00:00 | . 923 | 1.124550 | 0.853833 | 00:00 | . 924 | 1.124483 | 0.839324 | 00:00 | . 925 | 1.124419 | 0.841535 | 00:00 | . 926 | 1.124353 | 0.842931 | 00:00 | . 927 | 1.124289 | 0.841324 | 00:00 | . 928 | 1.124222 | 0.846839 | 00:00 | . 929 | 1.124154 | 0.843471 | 00:00 | . 930 | 1.124089 | 0.840840 | 00:00 | . 931 | 1.124022 | 0.841792 | 00:00 | . 932 | 1.123956 | 0.837194 | 00:00 | . 933 | 1.123891 | 0.837308 | 00:00 | . 934 | 1.123827 | 0.844652 | 00:00 | . 935 | 1.123760 | 0.842046 | 00:00 | . 936 | 1.123690 | 0.852645 | 00:00 | . 937 | 1.123624 | 0.837106 | 00:00 | . 938 | 1.123557 | 0.836191 | 00:00 | . 939 | 1.123491 | 0.839347 | 00:00 | . 940 | 1.123427 | 0.828554 | 00:00 | . 941 | 1.123361 | 0.846440 | 00:00 | . 942 | 1.123297 | 0.841281 | 00:00 | . 943 | 1.123232 | 0.845126 | 00:00 | . 944 | 1.123168 | 0.840104 | 00:00 | . 945 | 1.123103 | 0.833093 | 00:00 | . 946 | 1.123038 | 0.829462 | 00:00 | . 947 | 1.122975 | 0.839022 | 00:00 | . 948 | 1.122910 | 0.827858 | 00:00 | . 949 | 1.122847 | 0.844970 | 00:00 | . 950 | 1.122783 | 0.829865 | 00:00 | . 951 | 1.122721 | 0.845319 | 00:00 | . 952 | 1.122656 | 0.830022 | 00:00 | . 953 | 1.122593 | 0.832491 | 00:00 | . 954 | 1.122528 | 0.837621 | 00:00 | . 955 | 1.122462 | 0.820146 | 00:00 | . 956 | 1.122397 | 0.844777 | 00:00 | . 957 | 1.122334 | 0.825796 | 00:00 | . 958 | 1.122272 | 0.832674 | 00:00 | . 959 | 1.122209 | 0.835724 | 00:00 | . 960 | 1.122144 | 0.825456 | 00:00 | . 961 | 1.122078 | 0.841224 | 00:00 | . 962 | 1.122014 | 0.825974 | 00:00 | . 963 | 1.121951 | 0.840021 | 00:00 | . 964 | 1.121888 | 0.831758 | 00:00 | . 965 | 1.121823 | 0.819274 | 00:00 | . 966 | 1.121762 | 0.837447 | 00:00 | . 967 | 1.121698 | 0.819602 | 00:00 | . 968 | 1.121633 | 0.839202 | 00:00 | . 969 | 1.121570 | 0.825377 | 00:00 | . 970 | 1.121507 | 0.825362 | 00:00 | . 971 | 1.121445 | 0.832869 | 00:00 | . 972 | 1.121384 | 0.808087 | 00:00 | . 973 | 1.121323 | 0.836934 | 00:00 | . 974 | 1.121263 | 0.815343 | 00:00 | . 975 | 1.121203 | 0.829066 | 00:00 | . 976 | 1.121140 | 0.831464 | 00:00 | . 977 | 1.121077 | 0.821343 | 00:00 | . 978 | 1.121016 | 0.826258 | 00:00 | . 979 | 1.120953 | 0.824040 | 00:00 | . 980 | 1.120890 | 0.817167 | 00:00 | . 981 | 1.120829 | 0.835620 | 00:00 | . 982 | 1.120770 | 0.811536 | 00:00 | . 983 | 1.120709 | 0.825223 | 00:00 | . 984 | 1.120647 | 0.814452 | 00:00 | . 985 | 1.120587 | 0.812379 | 00:00 | . 986 | 1.120528 | 0.823268 | 00:00 | . 987 | 1.120465 | 0.808463 | 00:00 | . 988 | 1.120403 | 0.828295 | 00:00 | . 989 | 1.120343 | 0.814847 | 00:00 | . 990 | 1.120281 | 0.814448 | 00:00 | . 991 | 1.120219 | 0.820980 | 00:00 | . 992 | 1.120160 | 0.804554 | 00:00 | . 993 | 1.120100 | 0.824731 | 00:00 | . 994 | 1.120041 | 0.805505 | 00:00 | . 995 | 1.119982 | 0.818074 | 00:00 | . 996 | 1.119921 | 0.816324 | 00:00 | . 997 | 1.119859 | 0.806917 | 00:00 | . 998 | 1.119800 | 0.816600 | 00:00 | . 999 | 1.119738 | 0.805386 | 00:00 | . . - loss들도 에폭별로 기록되어 있음 . lrnr.recorder.plot_loss() . - net_fastai에도 파라메터가 업데이트 되어있음 . # list(net_fastai.parameters()) . 리스트를 확인해보면 net_fastai 의 파라메터가 알아서 GPU로 옮겨져서 학습됨. | . - 플랏 . net_fastai.to(&quot;cpu&quot;) plt.plot(X,y,&#39;.&#39;) plt.plot(X_tr,net_fastai(X_tr).data) plt.plot(X_val,net_fastai(X_val).data) . [&lt;matplotlib.lines.Line2D at 0x7f6e97599790&gt;] . &#46300;&#46989;&#50500;&#50883; &#52628;&#44032;&#48260;&#51204; . - 네트워크 설계 (드랍아웃 추가) . torch.manual_seed(1) net_fastai = torch.nn.Sequential( torch.nn.Linear(in_features=1, out_features=512), torch.nn.ReLU(), torch.nn.Dropout(0.8), torch.nn.Linear(in_features=512, out_features=1)) #optimizer loss_fn=torch.nn.MSELoss() . - 러너오브젝트 (for문 대신돌려주는 오브젝트) . lrnr= Learner(dls,net_fastai,opt_func=Adam,loss_func=loss_fn) . - 에폭만 설정하고 바로 학습 . lrnr.fit(1000) . epoch train_loss valid_loss time . 0 | 1.585653 | 0.428918 | 00:00 | . 1 | 1.552326 | 0.434847 | 00:00 | . 2 | 1.568810 | 0.442775 | 00:00 | . 3 | 1.543528 | 0.449585 | 00:00 | . 4 | 1.562597 | 0.456666 | 00:00 | . 5 | 1.523623 | 0.459943 | 00:00 | . 6 | 1.506816 | 0.458130 | 00:00 | . 7 | 1.510407 | 0.455353 | 00:00 | . 8 | 1.532602 | 0.449054 | 00:00 | . 9 | 1.528153 | 0.445443 | 00:00 | . 10 | 1.518390 | 0.442207 | 00:00 | . 11 | 1.508012 | 0.442086 | 00:00 | . 12 | 1.498026 | 0.443293 | 00:00 | . 13 | 1.502874 | 0.444508 | 00:00 | . 14 | 1.502828 | 0.445713 | 00:00 | . 15 | 1.496831 | 0.446047 | 00:00 | . 16 | 1.483070 | 0.447462 | 00:00 | . 17 | 1.496551 | 0.449803 | 00:00 | . 18 | 1.482904 | 0.450663 | 00:00 | . 19 | 1.471269 | 0.453689 | 00:00 | . 20 | 1.467480 | 0.456816 | 00:00 | . 21 | 1.457825 | 0.460537 | 00:00 | . 22 | 1.450724 | 0.463197 | 00:00 | . 23 | 1.445010 | 0.466199 | 00:00 | . 24 | 1.441184 | 0.471516 | 00:00 | . 25 | 1.436977 | 0.474600 | 00:00 | . 26 | 1.431098 | 0.476256 | 00:00 | . 27 | 1.423327 | 0.478671 | 00:00 | . 28 | 1.416092 | 0.479825 | 00:00 | . 29 | 1.414993 | 0.478338 | 00:00 | . 30 | 1.421260 | 0.477377 | 00:00 | . 31 | 1.413346 | 0.474661 | 00:00 | . 32 | 1.417670 | 0.470384 | 00:00 | . 33 | 1.412011 | 0.468277 | 00:00 | . 34 | 1.414570 | 0.465151 | 00:00 | . 35 | 1.416442 | 0.461778 | 00:00 | . 36 | 1.410454 | 0.457763 | 00:00 | . 37 | 1.405844 | 0.453920 | 00:00 | . 38 | 1.405701 | 0.451884 | 00:00 | . 39 | 1.405358 | 0.450063 | 00:00 | . 40 | 1.402212 | 0.449002 | 00:00 | . 41 | 1.403139 | 0.450335 | 00:00 | . 42 | 1.403911 | 0.450523 | 00:00 | . 43 | 1.397601 | 0.453860 | 00:00 | . 44 | 1.399249 | 0.456292 | 00:00 | . 45 | 1.395007 | 0.460008 | 00:00 | . 46 | 1.391067 | 0.464115 | 00:00 | . 47 | 1.387260 | 0.471899 | 00:00 | . 48 | 1.390660 | 0.477962 | 00:00 | . 49 | 1.391881 | 0.484811 | 00:00 | . 50 | 1.390658 | 0.491120 | 00:00 | . 51 | 1.390670 | 0.495985 | 00:00 | . 52 | 1.391075 | 0.500300 | 00:00 | . 53 | 1.392950 | 0.502631 | 00:00 | . 54 | 1.394412 | 0.507397 | 00:00 | . 55 | 1.393165 | 0.511569 | 00:00 | . 56 | 1.392622 | 0.511544 | 00:00 | . 57 | 1.388416 | 0.510609 | 00:00 | . 58 | 1.389699 | 0.505464 | 00:00 | . 59 | 1.388712 | 0.501359 | 00:00 | . 60 | 1.390845 | 0.493002 | 00:00 | . 61 | 1.389795 | 0.485509 | 00:00 | . 62 | 1.388309 | 0.479296 | 00:00 | . 63 | 1.385704 | 0.473247 | 00:00 | . 64 | 1.381633 | 0.470756 | 00:00 | . 65 | 1.379894 | 0.468657 | 00:00 | . 66 | 1.377811 | 0.466901 | 00:00 | . 67 | 1.373864 | 0.466839 | 00:00 | . 68 | 1.373379 | 0.467094 | 00:00 | . 69 | 1.373237 | 0.469634 | 00:00 | . 70 | 1.371915 | 0.471138 | 00:00 | . 71 | 1.374786 | 0.473315 | 00:00 | . 72 | 1.375253 | 0.477511 | 00:00 | . 73 | 1.373597 | 0.482231 | 00:00 | . 74 | 1.370517 | 0.486836 | 00:00 | . 75 | 1.368542 | 0.490195 | 00:00 | . 76 | 1.366800 | 0.491340 | 00:00 | . 77 | 1.365475 | 0.493011 | 00:00 | . 78 | 1.364186 | 0.492646 | 00:00 | . 79 | 1.362411 | 0.491744 | 00:00 | . 80 | 1.363654 | 0.490551 | 00:00 | . 81 | 1.364646 | 0.486897 | 00:00 | . 82 | 1.363839 | 0.484334 | 00:00 | . 83 | 1.360841 | 0.483685 | 00:00 | . 84 | 1.357780 | 0.482620 | 00:00 | . 85 | 1.354387 | 0.482355 | 00:00 | . 86 | 1.354743 | 0.480981 | 00:00 | . 87 | 1.352487 | 0.480221 | 00:00 | . 88 | 1.350849 | 0.480390 | 00:00 | . 89 | 1.347193 | 0.481674 | 00:00 | . 90 | 1.348291 | 0.482961 | 00:00 | . 91 | 1.348093 | 0.484509 | 00:00 | . 92 | 1.349149 | 0.485349 | 00:00 | . 93 | 1.347975 | 0.486714 | 00:00 | . 94 | 1.348029 | 0.487455 | 00:00 | . 95 | 1.347019 | 0.487787 | 00:00 | . 96 | 1.347150 | 0.488614 | 00:00 | . 97 | 1.346721 | 0.488363 | 00:00 | . 98 | 1.346410 | 0.488697 | 00:00 | . 99 | 1.344512 | 0.487400 | 00:00 | . 100 | 1.342906 | 0.484375 | 00:00 | . 101 | 1.342780 | 0.481898 | 00:00 | . 102 | 1.341344 | 0.479472 | 00:00 | . 103 | 1.341765 | 0.476342 | 00:00 | . 104 | 1.342349 | 0.473114 | 00:00 | . 105 | 1.340648 | 0.469774 | 00:00 | . 106 | 1.338787 | 0.466538 | 00:00 | . 107 | 1.337694 | 0.463039 | 00:00 | . 108 | 1.336146 | 0.461036 | 00:00 | . 109 | 1.335181 | 0.460885 | 00:00 | . 110 | 1.335002 | 0.460633 | 00:00 | . 111 | 1.333601 | 0.460474 | 00:00 | . 112 | 1.332647 | 0.459493 | 00:00 | . 113 | 1.332113 | 0.458576 | 00:00 | . 114 | 1.331091 | 0.458245 | 00:00 | . 115 | 1.331055 | 0.457598 | 00:00 | . 116 | 1.329440 | 0.457297 | 00:00 | . 117 | 1.329174 | 0.458239 | 00:00 | . 118 | 1.328747 | 0.459092 | 00:00 | . 119 | 1.328131 | 0.459786 | 00:00 | . 120 | 1.327026 | 0.460401 | 00:00 | . 121 | 1.324988 | 0.461529 | 00:00 | . 122 | 1.325732 | 0.463060 | 00:00 | . 123 | 1.324014 | 0.464970 | 00:00 | . 124 | 1.324666 | 0.467042 | 00:00 | . 125 | 1.323317 | 0.467260 | 00:00 | . 126 | 1.321263 | 0.467520 | 00:00 | . 127 | 1.321853 | 0.467667 | 00:00 | . 128 | 1.319355 | 0.468604 | 00:00 | . 129 | 1.318295 | 0.468806 | 00:00 | . 130 | 1.319103 | 0.469363 | 00:00 | . 131 | 1.318806 | 0.469256 | 00:00 | . 132 | 1.319240 | 0.468360 | 00:00 | . 133 | 1.319684 | 0.467827 | 00:00 | . 134 | 1.319690 | 0.467868 | 00:00 | . 135 | 1.318426 | 0.467066 | 00:00 | . 136 | 1.318111 | 0.466023 | 00:00 | . 137 | 1.319230 | 0.463543 | 00:00 | . 138 | 1.319114 | 0.460140 | 00:00 | . 139 | 1.317928 | 0.457014 | 00:00 | . 140 | 1.317386 | 0.454275 | 00:00 | . 141 | 1.317327 | 0.451683 | 00:00 | . 142 | 1.314812 | 0.450069 | 00:00 | . 143 | 1.314484 | 0.448842 | 00:00 | . 144 | 1.314361 | 0.448207 | 00:00 | . 145 | 1.312965 | 0.447664 | 00:00 | . 146 | 1.312361 | 0.447536 | 00:00 | . 147 | 1.310588 | 0.447214 | 00:00 | . 148 | 1.311692 | 0.446319 | 00:00 | . 149 | 1.309162 | 0.445097 | 00:00 | . 150 | 1.308690 | 0.443991 | 00:00 | . 151 | 1.309653 | 0.444124 | 00:00 | . 152 | 1.308728 | 0.444485 | 00:00 | . 153 | 1.309734 | 0.446062 | 00:00 | . 154 | 1.309190 | 0.447515 | 00:00 | . 155 | 1.310401 | 0.448601 | 00:00 | . 156 | 1.310624 | 0.449225 | 00:00 | . 157 | 1.311330 | 0.450946 | 00:00 | . 158 | 1.311746 | 0.452627 | 00:00 | . 159 | 1.311103 | 0.454660 | 00:00 | . 160 | 1.310514 | 0.455949 | 00:00 | . 161 | 1.311919 | 0.455852 | 00:00 | . 162 | 1.312855 | 0.454658 | 00:00 | . 163 | 1.313069 | 0.454663 | 00:00 | . 164 | 1.311808 | 0.454568 | 00:00 | . 165 | 1.310780 | 0.455139 | 00:00 | . 166 | 1.310751 | 0.455698 | 00:00 | . 167 | 1.310131 | 0.456399 | 00:00 | . 168 | 1.310501 | 0.457548 | 00:00 | . 169 | 1.308650 | 0.458662 | 00:00 | . 170 | 1.307447 | 0.458368 | 00:00 | . 171 | 1.306210 | 0.458754 | 00:00 | . 172 | 1.306657 | 0.459125 | 00:00 | . 173 | 1.305704 | 0.459026 | 00:00 | . 174 | 1.305946 | 0.458391 | 00:00 | . 175 | 1.305129 | 0.457954 | 00:00 | . 176 | 1.305813 | 0.457656 | 00:00 | . 177 | 1.304454 | 0.456099 | 00:00 | . 178 | 1.304170 | 0.454567 | 00:00 | . 179 | 1.303862 | 0.452808 | 00:00 | . 180 | 1.303645 | 0.450852 | 00:00 | . 181 | 1.304117 | 0.449986 | 00:00 | . 182 | 1.306056 | 0.450320 | 00:00 | . 183 | 1.306082 | 0.451507 | 00:00 | . 184 | 1.306572 | 0.453438 | 00:00 | . 185 | 1.307314 | 0.454431 | 00:00 | . 186 | 1.307979 | 0.455223 | 00:00 | . 187 | 1.308226 | 0.455543 | 00:00 | . 188 | 1.307733 | 0.454571 | 00:00 | . 189 | 1.306858 | 0.452855 | 00:00 | . 190 | 1.306951 | 0.451105 | 00:00 | . 191 | 1.307192 | 0.448794 | 00:00 | . 192 | 1.306901 | 0.447157 | 00:00 | . 193 | 1.306474 | 0.445820 | 00:00 | . 194 | 1.306584 | 0.444357 | 00:00 | . 195 | 1.305671 | 0.443530 | 00:00 | . 196 | 1.305142 | 0.442438 | 00:00 | . 197 | 1.305862 | 0.442103 | 00:00 | . 198 | 1.305954 | 0.442020 | 00:00 | . 199 | 1.306188 | 0.443073 | 00:00 | . 200 | 1.305721 | 0.444795 | 00:00 | . 201 | 1.304766 | 0.447127 | 00:00 | . 202 | 1.304900 | 0.449381 | 00:00 | . 203 | 1.304818 | 0.451541 | 00:00 | . 204 | 1.303382 | 0.454321 | 00:00 | . 205 | 1.303250 | 0.456620 | 00:00 | . 206 | 1.301603 | 0.458452 | 00:00 | . 207 | 1.300827 | 0.460165 | 00:00 | . 208 | 1.300216 | 0.461326 | 00:00 | . 209 | 1.299984 | 0.461125 | 00:00 | . 210 | 1.299863 | 0.460487 | 00:00 | . 211 | 1.299613 | 0.460132 | 00:00 | . 212 | 1.298147 | 0.458775 | 00:00 | . 213 | 1.297861 | 0.457812 | 00:00 | . 214 | 1.297246 | 0.457525 | 00:00 | . 215 | 1.297409 | 0.457489 | 00:00 | . 216 | 1.296456 | 0.457481 | 00:00 | . 217 | 1.295172 | 0.457752 | 00:00 | . 218 | 1.294975 | 0.457882 | 00:00 | . 219 | 1.295359 | 0.458115 | 00:00 | . 220 | 1.295161 | 0.458298 | 00:00 | . 221 | 1.295173 | 0.458718 | 00:00 | . 222 | 1.294700 | 0.458995 | 00:00 | . 223 | 1.294092 | 0.459594 | 00:00 | . 224 | 1.294339 | 0.459755 | 00:00 | . 225 | 1.294004 | 0.460028 | 00:00 | . 226 | 1.293507 | 0.460291 | 00:00 | . 227 | 1.293260 | 0.459926 | 00:00 | . 228 | 1.293112 | 0.460015 | 00:00 | . 229 | 1.293474 | 0.462001 | 00:00 | . 230 | 1.293882 | 0.463123 | 00:00 | . 231 | 1.293100 | 0.463192 | 00:00 | . 232 | 1.294397 | 0.460964 | 00:00 | . 233 | 1.293472 | 0.458559 | 00:00 | . 234 | 1.292968 | 0.456203 | 00:00 | . 235 | 1.291682 | 0.453646 | 00:00 | . 236 | 1.290647 | 0.450848 | 00:00 | . 237 | 1.290732 | 0.448872 | 00:00 | . 238 | 1.291056 | 0.448222 | 00:00 | . 239 | 1.291046 | 0.448295 | 00:00 | . 240 | 1.290196 | 0.448293 | 00:00 | . 241 | 1.290132 | 0.447221 | 00:00 | . 242 | 1.290471 | 0.447136 | 00:00 | . 243 | 1.290599 | 0.447810 | 00:00 | . 244 | 1.291708 | 0.449028 | 00:00 | . 245 | 1.291515 | 0.449940 | 00:00 | . 246 | 1.292217 | 0.451628 | 00:00 | . 247 | 1.292809 | 0.453637 | 00:00 | . 248 | 1.291820 | 0.456249 | 00:00 | . 249 | 1.290426 | 0.458512 | 00:00 | . 250 | 1.289343 | 0.460048 | 00:00 | . 251 | 1.289096 | 0.461299 | 00:00 | . 252 | 1.288898 | 0.462527 | 00:00 | . 253 | 1.288980 | 0.464177 | 00:00 | . 254 | 1.289070 | 0.463416 | 00:00 | . 255 | 1.290112 | 0.461251 | 00:00 | . 256 | 1.288822 | 0.460299 | 00:00 | . 257 | 1.288775 | 0.458695 | 00:00 | . 258 | 1.288434 | 0.457089 | 00:00 | . 259 | 1.287203 | 0.455199 | 00:00 | . 260 | 1.287099 | 0.452804 | 00:00 | . 261 | 1.287053 | 0.449477 | 00:00 | . 262 | 1.286709 | 0.447072 | 00:00 | . 263 | 1.286041 | 0.445487 | 00:00 | . 264 | 1.285576 | 0.444238 | 00:00 | . 265 | 1.284309 | 0.443065 | 00:00 | . 266 | 1.283903 | 0.442231 | 00:00 | . 267 | 1.283920 | 0.441861 | 00:00 | . 268 | 1.283106 | 0.441960 | 00:00 | . 269 | 1.283582 | 0.443035 | 00:00 | . 270 | 1.282750 | 0.445642 | 00:00 | . 271 | 1.283448 | 0.448107 | 00:00 | . 272 | 1.282522 | 0.449803 | 00:00 | . 273 | 1.281676 | 0.452021 | 00:00 | . 274 | 1.281590 | 0.453510 | 00:00 | . 275 | 1.282207 | 0.454524 | 00:00 | . 276 | 1.281351 | 0.455472 | 00:00 | . 277 | 1.281237 | 0.457178 | 00:00 | . 278 | 1.282604 | 0.459779 | 00:00 | . 279 | 1.281335 | 0.462591 | 00:00 | . 280 | 1.280466 | 0.463542 | 00:00 | . 281 | 1.281321 | 0.464619 | 00:00 | . 282 | 1.280022 | 0.465860 | 00:00 | . 283 | 1.279205 | 0.466361 | 00:00 | . 284 | 1.278493 | 0.465831 | 00:00 | . 285 | 1.278625 | 0.464630 | 00:00 | . 286 | 1.277769 | 0.462467 | 00:00 | . 287 | 1.278440 | 0.458461 | 00:00 | . 288 | 1.277338 | 0.453783 | 00:00 | . 289 | 1.276033 | 0.449824 | 00:00 | . 290 | 1.276147 | 0.447182 | 00:00 | . 291 | 1.277112 | 0.444997 | 00:00 | . 292 | 1.277598 | 0.442409 | 00:00 | . 293 | 1.278379 | 0.440894 | 00:00 | . 294 | 1.278243 | 0.440328 | 00:00 | . 295 | 1.277778 | 0.440284 | 00:00 | . 296 | 1.279097 | 0.441207 | 00:00 | . 297 | 1.279043 | 0.442428 | 00:00 | . 298 | 1.279270 | 0.444249 | 00:00 | . 299 | 1.278434 | 0.445340 | 00:00 | . 300 | 1.278132 | 0.446224 | 00:00 | . 301 | 1.277234 | 0.447085 | 00:00 | . 302 | 1.275964 | 0.447684 | 00:00 | . 303 | 1.274671 | 0.448605 | 00:00 | . 304 | 1.275020 | 0.449076 | 00:00 | . 305 | 1.273722 | 0.450046 | 00:00 | . 306 | 1.274755 | 0.451013 | 00:00 | . 307 | 1.275642 | 0.451433 | 00:00 | . 308 | 1.275408 | 0.450719 | 00:00 | . 309 | 1.273247 | 0.449887 | 00:00 | . 310 | 1.272665 | 0.447994 | 00:00 | . 311 | 1.273003 | 0.446439 | 00:00 | . 312 | 1.273043 | 0.445430 | 00:00 | . 313 | 1.273437 | 0.444877 | 00:00 | . 314 | 1.273943 | 0.444771 | 00:00 | . 315 | 1.274404 | 0.444911 | 00:00 | . 316 | 1.275467 | 0.446417 | 00:00 | . 317 | 1.276742 | 0.447893 | 00:00 | . 318 | 1.276362 | 0.449337 | 00:00 | . 319 | 1.275604 | 0.448122 | 00:00 | . 320 | 1.276364 | 0.448442 | 00:00 | . 321 | 1.276813 | 0.449577 | 00:00 | . 322 | 1.276665 | 0.450526 | 00:00 | . 323 | 1.277380 | 0.451509 | 00:00 | . 324 | 1.276901 | 0.451206 | 00:00 | . 325 | 1.276423 | 0.449930 | 00:00 | . 326 | 1.275547 | 0.450028 | 00:00 | . 327 | 1.275081 | 0.450576 | 00:00 | . 328 | 1.274731 | 0.451294 | 00:00 | . 329 | 1.273817 | 0.451883 | 00:00 | . 330 | 1.273240 | 0.453445 | 00:00 | . 331 | 1.274742 | 0.453539 | 00:00 | . 332 | 1.274715 | 0.454304 | 00:00 | . 333 | 1.275226 | 0.454264 | 00:00 | . 334 | 1.274455 | 0.453197 | 00:00 | . 335 | 1.275521 | 0.451644 | 00:00 | . 336 | 1.275896 | 0.450473 | 00:00 | . 337 | 1.275860 | 0.448176 | 00:00 | . 338 | 1.276271 | 0.445593 | 00:00 | . 339 | 1.276003 | 0.442808 | 00:00 | . 340 | 1.275415 | 0.440202 | 00:00 | . 341 | 1.276127 | 0.439005 | 00:00 | . 342 | 1.275972 | 0.439031 | 00:00 | . 343 | 1.276569 | 0.440024 | 00:00 | . 344 | 1.276008 | 0.441973 | 00:00 | . 345 | 1.275777 | 0.443781 | 00:00 | . 346 | 1.276256 | 0.444582 | 00:00 | . 347 | 1.277215 | 0.446039 | 00:00 | . 348 | 1.276950 | 0.448173 | 00:00 | . 349 | 1.277574 | 0.449313 | 00:00 | . 350 | 1.278115 | 0.451054 | 00:00 | . 351 | 1.277225 | 0.451989 | 00:00 | . 352 | 1.276574 | 0.453221 | 00:00 | . 353 | 1.275585 | 0.455469 | 00:00 | . 354 | 1.274473 | 0.456068 | 00:00 | . 355 | 1.274140 | 0.454947 | 00:00 | . 356 | 1.274698 | 0.453422 | 00:00 | . 357 | 1.275622 | 0.452169 | 00:00 | . 358 | 1.274646 | 0.450625 | 00:00 | . 359 | 1.274767 | 0.448509 | 00:00 | . 360 | 1.273904 | 0.446711 | 00:00 | . 361 | 1.273652 | 0.446107 | 00:00 | . 362 | 1.274045 | 0.444572 | 00:00 | . 363 | 1.273152 | 0.444826 | 00:00 | . 364 | 1.273077 | 0.444667 | 00:00 | . 365 | 1.273546 | 0.444369 | 00:00 | . 366 | 1.273254 | 0.444090 | 00:00 | . 367 | 1.272072 | 0.444427 | 00:00 | . 368 | 1.272523 | 0.443745 | 00:00 | . 369 | 1.272367 | 0.442162 | 00:00 | . 370 | 1.271725 | 0.441402 | 00:00 | . 371 | 1.272233 | 0.440817 | 00:00 | . 372 | 1.272786 | 0.439909 | 00:00 | . 373 | 1.271984 | 0.440383 | 00:00 | . 374 | 1.271438 | 0.440969 | 00:00 | . 375 | 1.272087 | 0.442260 | 00:00 | . 376 | 1.272138 | 0.443744 | 00:00 | . 377 | 1.272306 | 0.444796 | 00:00 | . 378 | 1.272574 | 0.445221 | 00:00 | . 379 | 1.271547 | 0.446293 | 00:00 | . 380 | 1.272340 | 0.447935 | 00:00 | . 381 | 1.273058 | 0.450134 | 00:00 | . 382 | 1.271911 | 0.451785 | 00:00 | . 383 | 1.272952 | 0.451823 | 00:00 | . 384 | 1.273204 | 0.451018 | 00:00 | . 385 | 1.273335 | 0.449144 | 00:00 | . 386 | 1.273633 | 0.447319 | 00:00 | . 387 | 1.272399 | 0.445352 | 00:00 | . 388 | 1.273201 | 0.442943 | 00:00 | . 389 | 1.273329 | 0.441387 | 00:00 | . 390 | 1.272785 | 0.439546 | 00:00 | . 391 | 1.272634 | 0.438152 | 00:00 | . 392 | 1.273203 | 0.437551 | 00:00 | . 393 | 1.272129 | 0.437695 | 00:00 | . 394 | 1.272987 | 0.437749 | 00:00 | . 395 | 1.273840 | 0.438374 | 00:00 | . 396 | 1.274974 | 0.439188 | 00:00 | . 397 | 1.274619 | 0.439760 | 00:00 | . 398 | 1.274106 | 0.440300 | 00:00 | . 399 | 1.275277 | 0.439990 | 00:00 | . 400 | 1.274680 | 0.440262 | 00:00 | . 401 | 1.273695 | 0.440013 | 00:00 | . 402 | 1.273230 | 0.438968 | 00:00 | . 403 | 1.274377 | 0.438218 | 00:00 | . 404 | 1.273531 | 0.437817 | 00:00 | . 405 | 1.273620 | 0.437358 | 00:00 | . 406 | 1.274253 | 0.436352 | 00:00 | . 407 | 1.273771 | 0.435403 | 00:00 | . 408 | 1.274173 | 0.434409 | 00:00 | . 409 | 1.273501 | 0.433826 | 00:00 | . 410 | 1.272775 | 0.433587 | 00:00 | . 411 | 1.272508 | 0.433174 | 00:00 | . 412 | 1.272207 | 0.433707 | 00:00 | . 413 | 1.272272 | 0.432533 | 00:00 | . 414 | 1.270983 | 0.430747 | 00:00 | . 415 | 1.272038 | 0.430000 | 00:00 | . 416 | 1.272086 | 0.429125 | 00:00 | . 417 | 1.272821 | 0.428850 | 00:00 | . 418 | 1.275159 | 0.429375 | 00:00 | . 419 | 1.275083 | 0.430764 | 00:00 | . 420 | 1.275092 | 0.432337 | 00:00 | . 421 | 1.275982 | 0.434301 | 00:00 | . 422 | 1.277127 | 0.436355 | 00:00 | . 423 | 1.276631 | 0.437124 | 00:00 | . 424 | 1.277536 | 0.438619 | 00:00 | . 425 | 1.278441 | 0.439234 | 00:00 | . 426 | 1.278212 | 0.440093 | 00:00 | . 427 | 1.277422 | 0.440520 | 00:00 | . 428 | 1.277893 | 0.440671 | 00:00 | . 429 | 1.277012 | 0.441104 | 00:00 | . 430 | 1.277210 | 0.440731 | 00:00 | . 431 | 1.277056 | 0.440195 | 00:00 | . 432 | 1.277160 | 0.439098 | 00:00 | . 433 | 1.275968 | 0.438190 | 00:00 | . 434 | 1.276130 | 0.438161 | 00:00 | . 435 | 1.276159 | 0.438568 | 00:00 | . 436 | 1.276241 | 0.439068 | 00:00 | . 437 | 1.276820 | 0.439943 | 00:00 | . 438 | 1.277444 | 0.440092 | 00:00 | . 439 | 1.278074 | 0.439790 | 00:00 | . 440 | 1.277538 | 0.438365 | 00:00 | . 441 | 1.277257 | 0.437584 | 00:00 | . 442 | 1.277888 | 0.436489 | 00:00 | . 443 | 1.278054 | 0.434792 | 00:00 | . 444 | 1.278555 | 0.433272 | 00:00 | . 445 | 1.279170 | 0.432295 | 00:00 | . 446 | 1.278721 | 0.431552 | 00:00 | . 447 | 1.278934 | 0.431901 | 00:00 | . 448 | 1.277781 | 0.431983 | 00:00 | . 449 | 1.277620 | 0.431903 | 00:00 | . 450 | 1.276831 | 0.431084 | 00:00 | . 451 | 1.278341 | 0.430876 | 00:00 | . 452 | 1.278537 | 0.430516 | 00:00 | . 453 | 1.278312 | 0.430885 | 00:00 | . 454 | 1.277749 | 0.431847 | 00:00 | . 455 | 1.277967 | 0.433086 | 00:00 | . 456 | 1.279019 | 0.434217 | 00:00 | . 457 | 1.278405 | 0.435246 | 00:00 | . 458 | 1.276616 | 0.435867 | 00:00 | . 459 | 1.276845 | 0.436367 | 00:00 | . 460 | 1.276245 | 0.437179 | 00:00 | . 461 | 1.276377 | 0.437631 | 00:00 | . 462 | 1.275729 | 0.437897 | 00:00 | . 463 | 1.275049 | 0.437593 | 00:00 | . 464 | 1.274093 | 0.437167 | 00:00 | . 465 | 1.274474 | 0.436689 | 00:00 | . 466 | 1.273303 | 0.435666 | 00:00 | . 467 | 1.273551 | 0.434357 | 00:00 | . 468 | 1.273654 | 0.433674 | 00:00 | . 469 | 1.272847 | 0.433034 | 00:00 | . 470 | 1.272470 | 0.432354 | 00:00 | . 471 | 1.273049 | 0.430940 | 00:00 | . 472 | 1.273412 | 0.429604 | 00:00 | . 473 | 1.274610 | 0.428852 | 00:00 | . 474 | 1.276161 | 0.429082 | 00:00 | . 475 | 1.275439 | 0.428738 | 00:00 | . 476 | 1.274739 | 0.428162 | 00:00 | . 477 | 1.274575 | 0.427499 | 00:00 | . 478 | 1.275174 | 0.427339 | 00:00 | . 479 | 1.275595 | 0.426646 | 00:00 | . 480 | 1.276064 | 0.426061 | 00:00 | . 481 | 1.276235 | 0.424929 | 00:00 | . 482 | 1.275934 | 0.424200 | 00:00 | . 483 | 1.276362 | 0.423808 | 00:00 | . 484 | 1.276524 | 0.424820 | 00:00 | . 485 | 1.276920 | 0.425996 | 00:00 | . 486 | 1.276008 | 0.427552 | 00:00 | . 487 | 1.274912 | 0.428545 | 00:00 | . 488 | 1.274581 | 0.429348 | 00:00 | . 489 | 1.274183 | 0.431096 | 00:00 | . 490 | 1.273627 | 0.432854 | 00:00 | . 491 | 1.273392 | 0.434724 | 00:00 | . 492 | 1.273660 | 0.435406 | 00:00 | . 493 | 1.273633 | 0.435743 | 00:00 | . 494 | 1.273769 | 0.435733 | 00:00 | . 495 | 1.273898 | 0.436706 | 00:00 | . 496 | 1.274712 | 0.436547 | 00:00 | . 497 | 1.274073 | 0.436535 | 00:00 | . 498 | 1.274464 | 0.434684 | 00:00 | . 499 | 1.275774 | 0.433847 | 00:00 | . 500 | 1.275434 | 0.432312 | 00:00 | . 501 | 1.276005 | 0.430961 | 00:00 | . 502 | 1.276263 | 0.429916 | 00:00 | . 503 | 1.276386 | 0.428123 | 00:00 | . 504 | 1.276625 | 0.426779 | 00:00 | . 505 | 1.276000 | 0.426228 | 00:00 | . 506 | 1.276098 | 0.426629 | 00:00 | . 507 | 1.275080 | 0.427692 | 00:00 | . 508 | 1.276389 | 0.429098 | 00:00 | . 509 | 1.276054 | 0.430441 | 00:00 | . 510 | 1.276090 | 0.431519 | 00:00 | . 511 | 1.277127 | 0.431709 | 00:00 | . 512 | 1.275999 | 0.430938 | 00:00 | . 513 | 1.275098 | 0.429506 | 00:00 | . 514 | 1.274982 | 0.428591 | 00:00 | . 515 | 1.275020 | 0.427200 | 00:00 | . 516 | 1.275092 | 0.425872 | 00:00 | . 517 | 1.275181 | 0.425218 | 00:00 | . 518 | 1.274409 | 0.425431 | 00:00 | . 519 | 1.273774 | 0.426154 | 00:00 | . 520 | 1.273251 | 0.427530 | 00:00 | . 521 | 1.273064 | 0.428511 | 00:00 | . 522 | 1.272297 | 0.428650 | 00:00 | . 523 | 1.273507 | 0.428638 | 00:00 | . 524 | 1.274507 | 0.428892 | 00:00 | . 525 | 1.273970 | 0.428889 | 00:00 | . 526 | 1.273723 | 0.428849 | 00:00 | . 527 | 1.272689 | 0.428296 | 00:00 | . 528 | 1.272379 | 0.427938 | 00:00 | . 529 | 1.272426 | 0.427906 | 00:00 | . 530 | 1.273074 | 0.427478 | 00:00 | . 531 | 1.274464 | 0.426175 | 00:00 | . 532 | 1.273956 | 0.425247 | 00:00 | . 533 | 1.273496 | 0.424632 | 00:00 | . 534 | 1.275143 | 0.424236 | 00:00 | . 535 | 1.274747 | 0.423956 | 00:00 | . 536 | 1.274909 | 0.423830 | 00:00 | . 537 | 1.275073 | 0.424100 | 00:00 | . 538 | 1.274790 | 0.424781 | 00:00 | . 539 | 1.275067 | 0.425287 | 00:00 | . 540 | 1.275010 | 0.426386 | 00:00 | . 541 | 1.274618 | 0.427106 | 00:00 | . 542 | 1.275144 | 0.427581 | 00:00 | . 543 | 1.274356 | 0.428153 | 00:00 | . 544 | 1.273233 | 0.428155 | 00:00 | . 545 | 1.273547 | 0.428011 | 00:00 | . 546 | 1.274343 | 0.428156 | 00:00 | . 547 | 1.274296 | 0.428199 | 00:00 | . 548 | 1.274896 | 0.427674 | 00:00 | . 549 | 1.274976 | 0.427745 | 00:00 | . 550 | 1.275443 | 0.427095 | 00:00 | . 551 | 1.274795 | 0.427033 | 00:00 | . 552 | 1.274088 | 0.427260 | 00:00 | . 553 | 1.273752 | 0.427573 | 00:00 | . 554 | 1.274754 | 0.427670 | 00:00 | . 555 | 1.275949 | 0.426888 | 00:00 | . 556 | 1.274297 | 0.426433 | 00:00 | . 557 | 1.275470 | 0.426053 | 00:00 | . 558 | 1.274680 | 0.425830 | 00:00 | . 559 | 1.274346 | 0.425301 | 00:00 | . 560 | 1.273932 | 0.424736 | 00:00 | . 561 | 1.274718 | 0.424207 | 00:00 | . 562 | 1.275055 | 0.423615 | 00:00 | . 563 | 1.275564 | 0.422614 | 00:00 | . 564 | 1.274421 | 0.421938 | 00:00 | . 565 | 1.274623 | 0.420876 | 00:00 | . 566 | 1.275101 | 0.420440 | 00:00 | . 567 | 1.274939 | 0.419782 | 00:00 | . 568 | 1.277139 | 0.419721 | 00:00 | . 569 | 1.276942 | 0.419491 | 00:00 | . 570 | 1.277254 | 0.419328 | 00:00 | . 571 | 1.277496 | 0.419572 | 00:00 | . 572 | 1.277800 | 0.419524 | 00:00 | . 573 | 1.278063 | 0.419531 | 00:00 | . 574 | 1.278172 | 0.419504 | 00:00 | . 575 | 1.277929 | 0.419522 | 00:00 | . 576 | 1.278976 | 0.420401 | 00:00 | . 577 | 1.278951 | 0.421076 | 00:00 | . 578 | 1.278936 | 0.421937 | 00:00 | . 579 | 1.278026 | 0.423059 | 00:00 | . 580 | 1.277990 | 0.424050 | 00:00 | . 581 | 1.276585 | 0.425667 | 00:00 | . 582 | 1.277262 | 0.427236 | 00:00 | . 583 | 1.277856 | 0.429521 | 00:00 | . 584 | 1.277002 | 0.431666 | 00:00 | . 585 | 1.276585 | 0.433043 | 00:00 | . 586 | 1.275947 | 0.434727 | 00:00 | . 587 | 1.276059 | 0.434814 | 00:00 | . 588 | 1.275011 | 0.434035 | 00:00 | . 589 | 1.275316 | 0.433805 | 00:00 | . 590 | 1.273905 | 0.433547 | 00:00 | . 591 | 1.274180 | 0.433468 | 00:00 | . 592 | 1.273776 | 0.434108 | 00:00 | . 593 | 1.273625 | 0.433555 | 00:00 | . 594 | 1.273317 | 0.432648 | 00:00 | . 595 | 1.273115 | 0.431505 | 00:00 | . 596 | 1.273500 | 0.430341 | 00:00 | . 597 | 1.272781 | 0.429411 | 00:00 | . 598 | 1.272768 | 0.428744 | 00:00 | . 599 | 1.273141 | 0.428526 | 00:00 | . 600 | 1.273931 | 0.427831 | 00:00 | . 601 | 1.275229 | 0.426826 | 00:00 | . 602 | 1.274655 | 0.426430 | 00:00 | . 603 | 1.272770 | 0.426874 | 00:00 | . 604 | 1.272791 | 0.427310 | 00:00 | . 605 | 1.271165 | 0.428531 | 00:00 | . 606 | 1.271338 | 0.429936 | 00:00 | . 607 | 1.271789 | 0.431701 | 00:00 | . 608 | 1.271045 | 0.433470 | 00:00 | . 609 | 1.270696 | 0.436342 | 00:00 | . 610 | 1.270494 | 0.440009 | 00:00 | . 611 | 1.270100 | 0.443815 | 00:00 | . 612 | 1.271096 | 0.448296 | 00:00 | . 613 | 1.271580 | 0.451146 | 00:00 | . 614 | 1.271622 | 0.452940 | 00:00 | . 615 | 1.270776 | 0.454178 | 00:00 | . 616 | 1.271864 | 0.454249 | 00:00 | . 617 | 1.272289 | 0.453211 | 00:00 | . 618 | 1.271519 | 0.450951 | 00:00 | . 619 | 1.271598 | 0.448288 | 00:00 | . 620 | 1.271333 | 0.446460 | 00:00 | . 621 | 1.272216 | 0.444449 | 00:00 | . 622 | 1.272854 | 0.442452 | 00:00 | . 623 | 1.272062 | 0.440141 | 00:00 | . 624 | 1.271588 | 0.437873 | 00:00 | . 625 | 1.272496 | 0.434874 | 00:00 | . 626 | 1.271760 | 0.432556 | 00:00 | . 627 | 1.270994 | 0.429463 | 00:00 | . 628 | 1.271371 | 0.426620 | 00:00 | . 629 | 1.270853 | 0.423774 | 00:00 | . 630 | 1.271135 | 0.421211 | 00:00 | . 631 | 1.271780 | 0.418900 | 00:00 | . 632 | 1.273019 | 0.417591 | 00:00 | . 633 | 1.273753 | 0.416858 | 00:00 | . 634 | 1.273858 | 0.416354 | 00:00 | . 635 | 1.274122 | 0.416114 | 00:00 | . 636 | 1.273795 | 0.415861 | 00:00 | . 637 | 1.273036 | 0.415816 | 00:00 | . 638 | 1.272659 | 0.415706 | 00:00 | . 639 | 1.272024 | 0.416092 | 00:00 | . 640 | 1.271669 | 0.416561 | 00:00 | . 641 | 1.272170 | 0.417270 | 00:00 | . 642 | 1.271865 | 0.418099 | 00:00 | . 643 | 1.271565 | 0.418794 | 00:00 | . 644 | 1.271142 | 0.419647 | 00:00 | . 645 | 1.270977 | 0.420059 | 00:00 | . 646 | 1.271979 | 0.420416 | 00:00 | . 647 | 1.271217 | 0.420808 | 00:00 | . 648 | 1.271259 | 0.420767 | 00:00 | . 649 | 1.272616 | 0.421066 | 00:00 | . 650 | 1.272668 | 0.421125 | 00:00 | . 651 | 1.271993 | 0.421768 | 00:00 | . 652 | 1.272138 | 0.422897 | 00:00 | . 653 | 1.271592 | 0.424054 | 00:00 | . 654 | 1.272083 | 0.424093 | 00:00 | . 655 | 1.272030 | 0.423063 | 00:00 | . 656 | 1.272285 | 0.422795 | 00:00 | . 657 | 1.271673 | 0.422893 | 00:00 | . 658 | 1.273349 | 0.423128 | 00:00 | . 659 | 1.272597 | 0.423218 | 00:00 | . 660 | 1.273699 | 0.422960 | 00:00 | . 661 | 1.273885 | 0.422069 | 00:00 | . 662 | 1.273517 | 0.421062 | 00:00 | . 663 | 1.273089 | 0.420342 | 00:00 | . 664 | 1.272442 | 0.419972 | 00:00 | . 665 | 1.271361 | 0.419623 | 00:00 | . 666 | 1.271217 | 0.419438 | 00:00 | . 667 | 1.269993 | 0.418890 | 00:00 | . 668 | 1.269655 | 0.418299 | 00:00 | . 669 | 1.269194 | 0.417920 | 00:00 | . 670 | 1.268759 | 0.417905 | 00:00 | . 671 | 1.268955 | 0.418348 | 00:00 | . 672 | 1.268707 | 0.418749 | 00:00 | . 673 | 1.268654 | 0.419811 | 00:00 | . 674 | 1.268233 | 0.421045 | 00:00 | . 675 | 1.267636 | 0.422275 | 00:00 | . 676 | 1.266986 | 0.423477 | 00:00 | . 677 | 1.267742 | 0.424165 | 00:00 | . 678 | 1.268641 | 0.425028 | 00:00 | . 679 | 1.269050 | 0.425611 | 00:00 | . 680 | 1.269403 | 0.426467 | 00:00 | . 681 | 1.269091 | 0.427412 | 00:00 | . 682 | 1.267687 | 0.427905 | 00:00 | . 683 | 1.267508 | 0.428243 | 00:00 | . 684 | 1.267759 | 0.428193 | 00:00 | . 685 | 1.268438 | 0.427318 | 00:00 | . 686 | 1.268508 | 0.426198 | 00:00 | . 687 | 1.268796 | 0.424193 | 00:00 | . 688 | 1.270079 | 0.422683 | 00:00 | . 689 | 1.269907 | 0.421311 | 00:00 | . 690 | 1.270103 | 0.420022 | 00:00 | . 691 | 1.270363 | 0.418645 | 00:00 | . 692 | 1.270039 | 0.417788 | 00:00 | . 693 | 1.268653 | 0.417462 | 00:00 | . 694 | 1.269908 | 0.417718 | 00:00 | . 695 | 1.270578 | 0.418532 | 00:00 | . 696 | 1.272404 | 0.419070 | 00:00 | . 697 | 1.272347 | 0.419774 | 00:00 | . 698 | 1.272877 | 0.420794 | 00:00 | . 699 | 1.272881 | 0.422011 | 00:00 | . 700 | 1.273312 | 0.422206 | 00:00 | . 701 | 1.273033 | 0.422188 | 00:00 | . 702 | 1.273083 | 0.421894 | 00:00 | . 703 | 1.273080 | 0.421378 | 00:00 | . 704 | 1.272576 | 0.420984 | 00:00 | . 705 | 1.272601 | 0.421140 | 00:00 | . 706 | 1.273961 | 0.420833 | 00:00 | . 707 | 1.273515 | 0.420523 | 00:00 | . 708 | 1.274026 | 0.420280 | 00:00 | . 709 | 1.274152 | 0.420168 | 00:00 | . 710 | 1.274102 | 0.420034 | 00:00 | . 711 | 1.274381 | 0.419448 | 00:00 | . 712 | 1.273970 | 0.419314 | 00:00 | . 713 | 1.273785 | 0.419432 | 00:00 | . 714 | 1.273130 | 0.420306 | 00:00 | . 715 | 1.272942 | 0.421600 | 00:00 | . 716 | 1.271915 | 0.422970 | 00:00 | . 717 | 1.272500 | 0.424144 | 00:00 | . 718 | 1.273117 | 0.424586 | 00:00 | . 719 | 1.272259 | 0.424356 | 00:00 | . 720 | 1.272185 | 0.424843 | 00:00 | . 721 | 1.271772 | 0.425027 | 00:00 | . 722 | 1.272063 | 0.424572 | 00:00 | . 723 | 1.272277 | 0.424014 | 00:00 | . 724 | 1.272755 | 0.423677 | 00:00 | . 725 | 1.273820 | 0.423826 | 00:00 | . 726 | 1.272688 | 0.423843 | 00:00 | . 727 | 1.272453 | 0.423943 | 00:00 | . 728 | 1.272389 | 0.423767 | 00:00 | . 729 | 1.273391 | 0.422945 | 00:00 | . 730 | 1.274099 | 0.421896 | 00:00 | . 731 | 1.273512 | 0.421346 | 00:00 | . 732 | 1.273110 | 0.420953 | 00:00 | . 733 | 1.272611 | 0.420504 | 00:00 | . 734 | 1.272441 | 0.420577 | 00:00 | . 735 | 1.271951 | 0.420622 | 00:00 | . 736 | 1.272573 | 0.420336 | 00:00 | . 737 | 1.273750 | 0.420125 | 00:00 | . 738 | 1.273916 | 0.420273 | 00:00 | . 739 | 1.273587 | 0.420420 | 00:00 | . 740 | 1.272597 | 0.420556 | 00:00 | . 741 | 1.271311 | 0.420938 | 00:00 | . 742 | 1.271327 | 0.421636 | 00:00 | . 743 | 1.271217 | 0.422211 | 00:00 | . 744 | 1.270743 | 0.422670 | 00:00 | . 745 | 1.269524 | 0.423153 | 00:00 | . 746 | 1.269111 | 0.424199 | 00:00 | . 747 | 1.268074 | 0.425692 | 00:00 | . 748 | 1.267374 | 0.427448 | 00:00 | . 749 | 1.267113 | 0.429417 | 00:00 | . 750 | 1.267896 | 0.430464 | 00:00 | . 751 | 1.268472 | 0.431497 | 00:00 | . 752 | 1.268011 | 0.432597 | 00:00 | . 753 | 1.269007 | 0.433072 | 00:00 | . 754 | 1.269112 | 0.433196 | 00:00 | . 755 | 1.269770 | 0.432484 | 00:00 | . 756 | 1.268727 | 0.431110 | 00:00 | . 757 | 1.268470 | 0.429870 | 00:00 | . 758 | 1.269278 | 0.428116 | 00:00 | . 759 | 1.271361 | 0.426084 | 00:00 | . 760 | 1.271295 | 0.423985 | 00:00 | . 761 | 1.271353 | 0.422090 | 00:00 | . 762 | 1.271455 | 0.420591 | 00:00 | . 763 | 1.271541 | 0.419276 | 00:00 | . 764 | 1.270986 | 0.418833 | 00:00 | . 765 | 1.270825 | 0.418824 | 00:00 | . 766 | 1.271915 | 0.419237 | 00:00 | . 767 | 1.272922 | 0.419968 | 00:00 | . 768 | 1.272471 | 0.420297 | 00:00 | . 769 | 1.271821 | 0.420603 | 00:00 | . 770 | 1.271629 | 0.420768 | 00:00 | . 771 | 1.271662 | 0.421128 | 00:00 | . 772 | 1.271456 | 0.420679 | 00:00 | . 773 | 1.272750 | 0.420017 | 00:00 | . 774 | 1.272356 | 0.419447 | 00:00 | . 775 | 1.271295 | 0.418807 | 00:00 | . 776 | 1.270830 | 0.418076 | 00:00 | . 777 | 1.270880 | 0.417508 | 00:00 | . 778 | 1.271060 | 0.416982 | 00:00 | . 779 | 1.271104 | 0.416521 | 00:00 | . 780 | 1.271205 | 0.416082 | 00:00 | . 781 | 1.271225 | 0.415755 | 00:00 | . 782 | 1.271719 | 0.415308 | 00:00 | . 783 | 1.271368 | 0.415058 | 00:00 | . 784 | 1.271442 | 0.415001 | 00:00 | . 785 | 1.271936 | 0.415055 | 00:00 | . 786 | 1.271050 | 0.415188 | 00:00 | . 787 | 1.270609 | 0.415531 | 00:00 | . 788 | 1.270226 | 0.416277 | 00:00 | . 789 | 1.270020 | 0.417182 | 00:00 | . 790 | 1.269789 | 0.418029 | 00:00 | . 791 | 1.270137 | 0.419041 | 00:00 | . 792 | 1.270787 | 0.419907 | 00:00 | . 793 | 1.270613 | 0.420784 | 00:00 | . 794 | 1.270307 | 0.421787 | 00:00 | . 795 | 1.269954 | 0.422248 | 00:00 | . 796 | 1.269829 | 0.422456 | 00:00 | . 797 | 1.270144 | 0.422551 | 00:00 | . 798 | 1.270793 | 0.423193 | 00:00 | . 799 | 1.271784 | 0.423621 | 00:00 | . 800 | 1.271582 | 0.424397 | 00:00 | . 801 | 1.271562 | 0.424124 | 00:00 | . 802 | 1.270906 | 0.423450 | 00:00 | . 803 | 1.272054 | 0.422543 | 00:00 | . 804 | 1.271724 | 0.421719 | 00:00 | . 805 | 1.271206 | 0.421711 | 00:00 | . 806 | 1.270157 | 0.421347 | 00:00 | . 807 | 1.268690 | 0.421744 | 00:00 | . 808 | 1.269302 | 0.422830 | 00:00 | . 809 | 1.269266 | 0.424024 | 00:00 | . 810 | 1.268813 | 0.424388 | 00:00 | . 811 | 1.269285 | 0.424709 | 00:00 | . 812 | 1.269601 | 0.425397 | 00:00 | . 813 | 1.269884 | 0.425630 | 00:00 | . 814 | 1.269699 | 0.426112 | 00:00 | . 815 | 1.269061 | 0.426136 | 00:00 | . 816 | 1.268275 | 0.425682 | 00:00 | . 817 | 1.268811 | 0.425237 | 00:00 | . 818 | 1.267624 | 0.424938 | 00:00 | . 819 | 1.267843 | 0.424417 | 00:00 | . 820 | 1.267787 | 0.423493 | 00:00 | . 821 | 1.268056 | 0.422811 | 00:00 | . 822 | 1.268793 | 0.422338 | 00:00 | . 823 | 1.269565 | 0.421562 | 00:00 | . 824 | 1.269217 | 0.421185 | 00:00 | . 825 | 1.268815 | 0.421531 | 00:00 | . 826 | 1.268254 | 0.421782 | 00:00 | . 827 | 1.267708 | 0.422387 | 00:00 | . 828 | 1.267267 | 0.422830 | 00:00 | . 829 | 1.267699 | 0.423381 | 00:00 | . 830 | 1.268082 | 0.424316 | 00:00 | . 831 | 1.269389 | 0.424898 | 00:00 | . 832 | 1.270597 | 0.425325 | 00:00 | . 833 | 1.270172 | 0.425304 | 00:00 | . 834 | 1.271292 | 0.424949 | 00:00 | . 835 | 1.272409 | 0.425245 | 00:00 | . 836 | 1.272232 | 0.425594 | 00:00 | . 837 | 1.272535 | 0.426534 | 00:00 | . 838 | 1.272956 | 0.427575 | 00:00 | . 839 | 1.271954 | 0.429451 | 00:00 | . 840 | 1.272637 | 0.431023 | 00:00 | . 841 | 1.273308 | 0.432484 | 00:00 | . 842 | 1.274460 | 0.433855 | 00:00 | . 843 | 1.274670 | 0.434733 | 00:00 | . 844 | 1.274493 | 0.434891 | 00:00 | . 845 | 1.273065 | 0.434844 | 00:00 | . 846 | 1.273523 | 0.433179 | 00:00 | . 847 | 1.273709 | 0.431525 | 00:00 | . 848 | 1.272139 | 0.430383 | 00:00 | . 849 | 1.271468 | 0.429011 | 00:00 | . 850 | 1.272084 | 0.427564 | 00:00 | . 851 | 1.271823 | 0.426059 | 00:00 | . 852 | 1.272588 | 0.424455 | 00:00 | . 853 | 1.272524 | 0.423188 | 00:00 | . 854 | 1.273265 | 0.422436 | 00:00 | . 855 | 1.272757 | 0.421354 | 00:00 | . 856 | 1.271702 | 0.420359 | 00:00 | . 857 | 1.272387 | 0.419776 | 00:00 | . 858 | 1.273033 | 0.419270 | 00:00 | . 859 | 1.273170 | 0.419229 | 00:00 | . 860 | 1.272661 | 0.419336 | 00:00 | . 861 | 1.271850 | 0.419764 | 00:00 | . 862 | 1.271725 | 0.420668 | 00:00 | . 863 | 1.272077 | 0.421613 | 00:00 | . 864 | 1.271688 | 0.422072 | 00:00 | . 865 | 1.272325 | 0.422564 | 00:00 | . 866 | 1.272381 | 0.422797 | 00:00 | . 867 | 1.273450 | 0.423622 | 00:00 | . 868 | 1.273376 | 0.424079 | 00:00 | . 869 | 1.273843 | 0.424435 | 00:00 | . 870 | 1.273430 | 0.424200 | 00:00 | . 871 | 1.273257 | 0.424379 | 00:00 | . 872 | 1.272924 | 0.423945 | 00:00 | . 873 | 1.272440 | 0.423741 | 00:00 | . 874 | 1.271832 | 0.424008 | 00:00 | . 875 | 1.271346 | 0.424027 | 00:00 | . 876 | 1.270279 | 0.424191 | 00:00 | . 877 | 1.271330 | 0.424767 | 00:00 | . 878 | 1.272347 | 0.424582 | 00:00 | . 879 | 1.271782 | 0.424495 | 00:00 | . 880 | 1.270341 | 0.423923 | 00:00 | . 881 | 1.270595 | 0.423531 | 00:00 | . 882 | 1.270957 | 0.423210 | 00:00 | . 883 | 1.270394 | 0.422807 | 00:00 | . 884 | 1.270517 | 0.422459 | 00:00 | . 885 | 1.271277 | 0.422543 | 00:00 | . 886 | 1.272307 | 0.422034 | 00:00 | . 887 | 1.272899 | 0.420725 | 00:00 | . 888 | 1.271770 | 0.419568 | 00:00 | . 889 | 1.271013 | 0.419065 | 00:00 | . 890 | 1.271375 | 0.418563 | 00:00 | . 891 | 1.271399 | 0.417978 | 00:00 | . 892 | 1.269894 | 0.417693 | 00:00 | . 893 | 1.269401 | 0.417770 | 00:00 | . 894 | 1.270070 | 0.418011 | 00:00 | . 895 | 1.271703 | 0.418210 | 00:00 | . 896 | 1.270701 | 0.417986 | 00:00 | . 897 | 1.270333 | 0.418393 | 00:00 | . 898 | 1.270212 | 0.418955 | 00:00 | . 899 | 1.269930 | 0.419543 | 00:00 | . 900 | 1.269447 | 0.420885 | 00:00 | . 901 | 1.269472 | 0.422435 | 00:00 | . 902 | 1.270327 | 0.424117 | 00:00 | . 903 | 1.269371 | 0.425407 | 00:00 | . 904 | 1.269742 | 0.427297 | 00:00 | . 905 | 1.269597 | 0.428427 | 00:00 | . 906 | 1.269879 | 0.428448 | 00:00 | . 907 | 1.268686 | 0.427980 | 00:00 | . 908 | 1.268300 | 0.427277 | 00:00 | . 909 | 1.268517 | 0.426578 | 00:00 | . 910 | 1.270159 | 0.425584 | 00:00 | . 911 | 1.269550 | 0.424793 | 00:00 | . 912 | 1.269740 | 0.423792 | 00:00 | . 913 | 1.269372 | 0.422663 | 00:00 | . 914 | 1.270665 | 0.421337 | 00:00 | . 915 | 1.271434 | 0.419985 | 00:00 | . 916 | 1.271421 | 0.418839 | 00:00 | . 917 | 1.270465 | 0.418105 | 00:00 | . 918 | 1.269036 | 0.417136 | 00:00 | . 919 | 1.267547 | 0.416472 | 00:00 | . 920 | 1.266729 | 0.416265 | 00:00 | . 921 | 1.267774 | 0.416307 | 00:00 | . 922 | 1.267467 | 0.416354 | 00:00 | . 923 | 1.266899 | 0.416285 | 00:00 | . 924 | 1.266400 | 0.416102 | 00:00 | . 925 | 1.266270 | 0.416197 | 00:00 | . 926 | 1.267540 | 0.416471 | 00:00 | . 927 | 1.267549 | 0.416598 | 00:00 | . 928 | 1.267579 | 0.417045 | 00:00 | . 929 | 1.267099 | 0.417216 | 00:00 | . 930 | 1.267423 | 0.417131 | 00:00 | . 931 | 1.266348 | 0.417050 | 00:00 | . 932 | 1.266774 | 0.416642 | 00:00 | . 933 | 1.267326 | 0.416432 | 00:00 | . 934 | 1.268196 | 0.416297 | 00:00 | . 935 | 1.268687 | 0.416261 | 00:00 | . 936 | 1.268104 | 0.416380 | 00:00 | . 937 | 1.267747 | 0.416236 | 00:00 | . 938 | 1.267965 | 0.416246 | 00:00 | . 939 | 1.267852 | 0.416088 | 00:00 | . 940 | 1.267749 | 0.416140 | 00:00 | . 941 | 1.267872 | 0.415994 | 00:00 | . 942 | 1.268932 | 0.415794 | 00:00 | . 943 | 1.268650 | 0.415612 | 00:00 | . 944 | 1.268238 | 0.415426 | 00:00 | . 945 | 1.268917 | 0.415220 | 00:00 | . 946 | 1.269694 | 0.415236 | 00:00 | . 947 | 1.268451 | 0.415348 | 00:00 | . 948 | 1.269323 | 0.415505 | 00:00 | . 949 | 1.269393 | 0.415677 | 00:00 | . 950 | 1.269968 | 0.415693 | 00:00 | . 951 | 1.270348 | 0.415737 | 00:00 | . 952 | 1.269088 | 0.415885 | 00:00 | . 953 | 1.269453 | 0.416111 | 00:00 | . 954 | 1.268461 | 0.416437 | 00:00 | . 955 | 1.268772 | 0.416775 | 00:00 | . 956 | 1.267800 | 0.416897 | 00:00 | . 957 | 1.267764 | 0.416881 | 00:00 | . 958 | 1.267281 | 0.416874 | 00:00 | . 959 | 1.267384 | 0.416768 | 00:00 | . 960 | 1.265235 | 0.416502 | 00:00 | . 961 | 1.264060 | 0.415914 | 00:00 | . 962 | 1.264244 | 0.415704 | 00:00 | . 963 | 1.264464 | 0.415380 | 00:00 | . 964 | 1.264683 | 0.414916 | 00:00 | . 965 | 1.263351 | 0.414535 | 00:00 | . 966 | 1.262699 | 0.414396 | 00:00 | . 967 | 1.263175 | 0.414138 | 00:00 | . 968 | 1.264636 | 0.414032 | 00:00 | . 969 | 1.265427 | 0.414129 | 00:00 | . 970 | 1.263703 | 0.414361 | 00:00 | . 971 | 1.264736 | 0.414615 | 00:00 | . 972 | 1.265115 | 0.414957 | 00:00 | . 973 | 1.265979 | 0.415205 | 00:00 | . 974 | 1.265494 | 0.415441 | 00:00 | . 975 | 1.264690 | 0.415604 | 00:00 | . 976 | 1.263579 | 0.415683 | 00:00 | . 977 | 1.263605 | 0.415899 | 00:00 | . 978 | 1.264619 | 0.415924 | 00:00 | . 979 | 1.264595 | 0.416032 | 00:00 | . 980 | 1.263995 | 0.416174 | 00:00 | . 981 | 1.265043 | 0.416207 | 00:00 | . 982 | 1.264780 | 0.416322 | 00:00 | . 983 | 1.264264 | 0.416483 | 00:00 | . 984 | 1.264869 | 0.416668 | 00:00 | . 985 | 1.265409 | 0.417176 | 00:00 | . 986 | 1.265599 | 0.417357 | 00:00 | . 987 | 1.265436 | 0.417462 | 00:00 | . 988 | 1.266293 | 0.417758 | 00:00 | . 989 | 1.264438 | 0.417809 | 00:00 | . 990 | 1.264117 | 0.418057 | 00:00 | . 991 | 1.263802 | 0.417804 | 00:00 | . 992 | 1.264000 | 0.417707 | 00:00 | . 993 | 1.264081 | 0.417715 | 00:00 | . 994 | 1.264172 | 0.417634 | 00:00 | . 995 | 1.265378 | 0.417668 | 00:00 | . 996 | 1.265913 | 0.417539 | 00:00 | . 997 | 1.266175 | 0.417496 | 00:00 | . 998 | 1.265986 | 0.417067 | 00:00 | . 999 | 1.266017 | 0.416893 | 00:00 | . . - loss들도 에폭별로 기록되어 있음 . lrnr.recorder.plot_loss() . - net_fastai에도 파라메터가 업데이트 되어있음 . . 리스트를 확인해보면 net_fastai 의 파라메터가 알아서 GPU로 옮겨져서 학습됨. | . - 플랏 . net_fastai.to(&quot;cpu&quot;) plt.plot(X,y,&#39;.&#39;) plt.plot(X_tr,net_fastai(X_tr).data) plt.plot(X_val,net_fastai(X_val).data) . [&lt;matplotlib.lines.Line2D at 0x7f6e94e31640&gt;] . CPU vs GPU &#49884;&#44036;&#48708;&#44368; . import time . CPU (512) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 0.6667273044586182 . GPU (512) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) net.to(&quot;cuda:0&quot;) X=X.to(&quot;cuda:0&quot;) y=y.to(&quot;cuda:0&quot;) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 2.074880838394165 . - ?? CPU가 더 빠르다!! . CPU (20480) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=20480), torch.nn.ReLU(), torch.nn.Linear(in_features=20480,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 3.695246696472168 . GPU (20480) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=20480), torch.nn.ReLU(), torch.nn.Linear(in_features=20480,out_features=1)) net.to(&quot;cuda:0&quot;) X=X.to(&quot;cuda:0&quot;) y=y.to(&quot;cuda:0&quot;) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 2.2188520431518555 . CPU (204800) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=204800), torch.nn.ReLU(), torch.nn.Linear(in_features=204800,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 62.97744035720825 . GPU (204800) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=204800), torch.nn.ReLU(), torch.nn.Linear(in_features=204800,out_features=1)) net.to(&quot;cuda:0&quot;) X=X.to(&quot;cuda:0&quot;) y=y.to(&quot;cuda:0&quot;) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 2.404008626937866 . &#49689;&#51228; . - 현재 작업하고 있는 컴퓨터에서 아래코드를 실행후 시간을 출력하여 스샷제출 . CPU (512) . torch.manual_seed(5) X=torch.linspace(0,1,100).reshape(100,1) y=torch.randn(100).reshape(100,1)*0.01 . torch.manual_seed(1) # 초기가중치를 똑같이 net=torch.nn.Sequential( torch.nn.Linear(in_features=1,out_features=512), torch.nn.ReLU(), torch.nn.Linear(in_features=512,out_features=1)) optimizer= torch.optim.Adam(net.parameters()) loss_fn= torch.nn.MSELoss() . t1=time.time() for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() t2=time.time() . t2-t1 . 0.6667273044586182 .",
            "url": "https://guebin.github.io/2021BDA/2021/10/19/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9419%EC%9D%BC.html",
            "relUrl": "/2021/10/19/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9419%EC%9D%BC.html",
            "date": " • Oct 19, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "(6주차) 10월14일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/4) 시험일정 공지 . - (2/4) 미니배치 . - (3/4) 딥러닝용 컴퓨터를 고르는 요령 . - (4/4) 과제설명 . import . import torch from fastai.vision.all import * . Dataset . X=torch.tensor([3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]) y=torch.tensor([1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]) . X,y . (tensor([3., 4., 5., 6., 7., 8., 9.]), tensor([1., 0., 1., 0., 1., 1., 0.])) . ds=torch.utils.data.TensorDataset(X,y) . ds ## 그냥 텐서들의 pair . &lt;torch.utils.data.dataset.TensorDataset at 0x7f7e8ae947f0&gt; . ds.tensors . (tensor([3., 4., 5., 6., 7., 8., 9.]), tensor([1., 0., 1., 0., 1., 1., 0.])) . DataLoader . - 배치사이즈=2, 셔플= True, . dl=torch.utils.data.DataLoader(ds,batch_size=2,shuffle=True) . dl . &lt;torch.utils.data.dataloader.DataLoader at 0x7f7e8a1f4f40&gt; . dir(dl) . [&#39;_DataLoader__initialized&#39;, &#39;_DataLoader__multiprocessing_context&#39;, &#39;_IterableDataset_len_called&#39;, &#39;__annotations__&#39;, &#39;__class__&#39;, &#39;__class_getitem__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__orig_bases__&#39;, &#39;__parameters__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__slots__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_auto_collation&#39;, &#39;_dataset_kind&#39;, &#39;_get_iterator&#39;, &#39;_index_sampler&#39;, &#39;_is_protocol&#39;, &#39;_iterator&#39;, &#39;batch_sampler&#39;, &#39;batch_size&#39;, &#39;check_worker_number_rationality&#39;, &#39;collate_fn&#39;, &#39;dataset&#39;, &#39;drop_last&#39;, &#39;generator&#39;, &#39;multiprocessing_context&#39;, &#39;num_workers&#39;, &#39;persistent_workers&#39;, &#39;pin_memory&#39;, &#39;prefetch_factor&#39;, &#39;sampler&#39;, &#39;timeout&#39;, &#39;worker_init_fn&#39;] . dl은 배치를 만드는 기능이 있어보임 | . for xx,yy in dl: print(xx,yy) . tensor([7., 3.]) tensor([1., 1.]) tensor([6., 9.]) tensor([0., 0.]) tensor([4., 5.]) tensor([0., 1.]) tensor([8.]) tensor([1.]) . - 배치사이즈=2, 셔플= False . dl=torch.utils.data.DataLoader(ds,batch_size=2,shuffle=False) . for xx,yy in dl: print(xx,yy) . tensor([3., 4.]) tensor([1., 0.]) tensor([5., 6.]) tensor([1., 0.]) tensor([7., 8.]) tensor([1., 1.]) tensor([9.]) tensor([0.]) . - 배치사이즈=3, 셔플= True . dl=torch.utils.data.DataLoader(ds,batch_size=3,shuffle=True) . for xx,yy in dl: print(xx,yy) . tensor([3., 6., 5.]) tensor([1., 0., 1.]) tensor([8., 7., 9.]) tensor([1., 1., 0.]) tensor([4.]) tensor([0.]) . MNIST 3/7 &#50696;&#51228; . - 우선 텐서로 이루어진 X,y를 만들자. . path = untar_data(URLs.MNIST_SAMPLE) . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) y=torch.tensor([0.0]*6265 + [1.0]*6131).reshape(12396,1) . - dataset=(X,y) 를 만들자. . ds=torch.utils.data.TensorDataset(X,y) . - dataloader를 만들자. . dl=torch.utils.data.DataLoader(ds,batch_size=2048,shuffle=True) . - 네트워크(아키텍처), 손실함수, 옵티마이저 . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . - 저번시간 복습 . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= loss_fn(yhat,y) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(yhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7e8bd37b50&gt;] . f=torch.nn.Sigmoid() plt.plot(f(yhat.data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7e73ba0910&gt;] . - 미니배치활용 . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=784,out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30,out_features=1) #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . 네트워크 파라메터 다시 초기화 | . 12396 / 2048 . 6.052734375 . 총 7개의 미니배치가 만들어질것임 $ to$ 따라서 파라메터를 업데이트하는 횟수는 7 $ times$ epoc 임 (실제적으로는 6 $ times$ epoc) | . 200/6 . 33.333333333333336 . for epoc in range(33): for xx,yy in dl: ### 총 7번돌면 끝나는 for ## 1 yyhat=net(xx) ## 2 loss= loss_fn(yyhat,yy) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(yyhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7e746f2d30&gt;] . 이게 왜이러지?? | . - 배치사이즈를 다시 확인해보자. . for xx,yy in dl: print(xx.shape,yy.shape) . torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([2048, 784]) torch.Size([2048, 1]) torch.Size([108, 784]) torch.Size([108, 1]) . - 마지막이 108개이므로 108개의 y만 그려짐 . plt.plot(net(X).data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7e7474f1c0&gt;] . - 2048개 정도만 대충학습해도 동일 반복횟수에 대하여 거의 대등한 효율이 나옴 . - GPU에 있는 메모리로 12396개의 데이터를 모두 보내지 않아도 괜찮겠다 $ to$ 그래픽카드의 메모리를 얼마나 큰 것으로 살지는 자료의 크기와는 상관없다. . - net.parameters()에 저장된 값들은 그대로 GPU로 가야만한다. $ to$ 그래픽카드의 메모리를 얼마나 큰것으로 살지는 모형의 복잡도와 관련이 있다. . 컴퓨터사는방법 . 메모리: $n$이 큰 자료를 다룰수록 메모리가 커야한다. | GPU의 메모리: 모형의 복잡도가 커질수록 GPU의 메모리가 커야한다. | . &#49689;&#51228; . - batchsize=1024로 바꾼후 학습해보고 결과를 관찰할것 .",
            "url": "https://guebin.github.io/2021BDA/2021/10/14/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9414%EC%9D%BC.html",
            "relUrl": "/2021/10/14/(6%EC%A3%BC%EC%B0%A8)-10%EC%9B%9414%EC%9D%BC.html",
            "date": " • Oct 14, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "(5주차) 10월12일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/8) 손실함수 (1) . - (2/8) 손실함수 (2) . - (3/8) 손실함수차이를 애니메이션으로 . - (4/8) Adam . - (5/8) Adam animation . - (6/8) 신경망은 왜 깊어졌는가?,universal approximation theorem . - (7/8) MNIST with MLP 풀이1 . - (8/8) MNIST with MLP 풀이2,3 . MSEloss &#50752; BCEloss &#48708;&#44368; . &#49552;&#49892;&#54632;&#49688;&#51032; &#47784;&#50577;&#48708;&#44368; . import torch import numpy as np import matplotlib.pyplot as plt . torch.manual_seed(1) X=torch.linspace(-1,1,2000).reshape(2000,1) w0=-1.0 w1=5.0 u=w0+X*w1 v=torch.exp(u)/(1+torch.exp(u)) y=torch.bernoulli(v) . plt.scatter(X,y,alpha=0.01) plt.plot(X,v) . [&lt;matplotlib.lines.Line2D at 0x7fce37be2220&gt;] . _w0= np.arange(-10,3,0.05) _w1= np.arange(-1,10,0.05) . _w0, _w1 =np.meshgrid(_w0,_w1,indexing=&#39;ij&#39;) . _w0=_w0.reshape(-1) _w1=_w1.reshape(-1) . def lossfn_crossenp(w0,w1): yhat=torch.exp( w0+w1*X) / (1+torch.exp( w0+w1*X)) loss= - torch.mean (y*torch.log(yhat)+(1-y)*torch.log(1-yhat)) return loss.tolist() . def lossfn_mse(w0,w1): yhat=torch.exp( w0+w1*X) / (1+torch.exp( w0+w1*X)) loss= torch.mean((y-yhat)**2) return loss.tolist() . _l1=list(map(lossfn_crossenp,_w0,_w1)) _l2=list(map(lossfn_mse,_w0,_w1)) . fig = plt.figure() ax1=fig.add_subplot(1,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(1,2,2,projection=&#39;3d&#39;) ax1.elev=15 ax2.elev=15 ax1.azim=75 ax2.azim=75 fig.set_figheight(15) fig.set_figwidth(15) . ax1.scatter(_w0,_w1,_l1,s=0.01) ax2.scatter(_w0,_w1,_l2,s=0.01) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fce3767d370&gt; . _w0[np.argmin(_l1)],_w1[np.argmin(_l1)] . (-0.9999999999998721, 5.150000000000006) . _w0[np.argmin(_l2)],_w1[np.argmin(_l2)] . (-0.9999999999998721, 5.100000000000005) . ax1.scatter(_w0[np.argmin(_l1)],_w1[np.argmin(_l1)],np.min(_l1),s=200,marker=&#39;*&#39;) ax2.scatter(_w0[np.argmin(_l2)],_w1[np.argmin(_l2)],np.min(_l2),s=200,marker=&#39;*&#39;) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fce376ac2e0&gt; . fig . &#50500;&#53412;&#53581;&#52376;, &#50741;&#54000;&#47560;&#51060;&#51200; . l1=torch.nn.Linear(in_features=1,out_features=1,bias=True) a1=torch.nn.Sigmoid() net=torch.nn.Sequential(l1,a1) optimizer=torch.optim.SGD(net.parameters(),lr=0.05) . &#52488;&#44592;&#44050; $(w_0,w_1)=(-3,-1)$&#51012; &#45824;&#51077;&#54616;&#44256; &#49688;&#47156;&#44284;&#51221;&#51012; animation&#51004;&#47196; &#44288;&#52272;&#54616;&#51088;. . - 파라메터 초기값 $(w_0,w_1)=(-3,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([0.0331]), tensor([[-0.1853]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - BCEloss를 이용하여 학습+기록 . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-0.6726]), tensor([[3.3696]])) . - 파라메터 초기값 $(w_0,w_1)=(-3,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-0.6726]), tensor([[3.3696]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - MSEloss를 이용하여 학습+기록 . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-0.9688]), tensor([[0.7116]])) . - plot . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-3,-1,lossfn_crossenp(-3,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-3,-1,lossfn_mse(-3,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#52488;&#44592;&#44050; $(w_0,w_1)=(-10,-1)$&#51012; &#45824;&#51077;&#54616;&#44256; &#49688;&#47156;&#44284;&#51221;&#51012; animation&#51004;&#47196; &#44288;&#52272;&#54616;&#51088;. . - 파라메터 초기값 $(w_0,w_1)=(-10,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-0.9688]), tensor([[0.7116]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - BCEloss를 이용하여 학습+기록 . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-0.8302]), tensor([[4.0264]])) . - 파라메터 초기값 $(w_0,w_1)=(-10,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-0.8302]), tensor([[4.0264]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - MSEloss를 이용하여 학습+기록 . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . l1.bias.data,l1.weight.data . (tensor([-9.9990]), tensor([[-0.9995]])) . - plot . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-10,-1,lossfn_crossenp(-10,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-10,-1,lossfn_mse(-10,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect Adam &#50741;&#54000;&#47560;&#51060;&#51200;, $(w_0,w_1)=(-3,-1)$ . - 옵티마이저 재설정 . optimizer=torch.optim.Adam(net.parameters(),lr=0.05) . - 파라메터 초기값 $(w_0,w_1)=(-3,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-9.9990]), tensor([[-0.9995]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - BCEloss를 사용하여 학습 + 기록 . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . - 파라메터 초기값 $(w_0,w_1)=(-3,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-1.0201]), tensor([[5.1584]])) . l1.bias.data=torch.tensor([-3.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-3.]), tensor([[-1.]])) . - MSEloss를 사용하여 학습+기록 . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-3,-1,lossfn_crossenp(-3,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-3,-1,lossfn_mse(-3,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect Adam &#50741;&#54000;&#47560;&#51060;&#51200;, $(w_0,w_1)=(-10,-1)$ . - 옵티마이저 재설정 . optimizer=torch.optim.Adam(net.parameters(),lr=0.05) . - 파라메터 초기값 $(w_0,w_1)=(-10,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-0.9995]), tensor([[5.0790]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - BCEloss를 사용하여 학습 + 기록 . w0_bce=[] w1_bce=[] loss_bce=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= - torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_bce.append(l1.bias.data.item()) w1_bce.append(l1.weight.data.item()) loss_bce.append(loss.item()) . - 파라메터 초기값 $(w_0,w_1)=(-10,-1)$로 설정 . l1.bias.data, l1.weight.data . (tensor([-1.0243]), tensor([[5.1769]])) . l1.bias.data=torch.tensor([-10.0]) l1.weight.data=torch.tensor([[-1.0]]) . l1.bias.data, l1.weight.data . (tensor([-10.]), tensor([[-1.]])) . - MSEloss를 사용하여 학습+기록 . w0_mse=[] w1_mse=[] loss_mse=[] for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss= torch.mean((y-yhat)**2) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() ## 5 if epoc%20 == 0: w0_mse.append(l1.bias.data.item()) w1_mse.append(l1.weight.data.item()) loss_mse.append(loss.item()) . fig = plt.figure() ax1= fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2= fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax3= fig.add_subplot(2,2,3) ax4= fig.add_subplot(2,2,4) ax1.elev = 15 ax2.elev = 15 ax1.azim = 75 ax2.azim = 75 fig.set_figheight(15) fig.set_figwidth(15) ### init plot ax1.scatter(_w0,_w1,_l1,s=0.05) ax2.scatter(_w0,_w1,_l2,s=0.05) ax1.scatter(-10,-1,lossfn_crossenp(-10,-1),color=&#39;gray&#39;) ## bceloss ax1.scatter(-1,5.1,lossfn_crossenp(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## bceloss ax2.scatter(-10,-1,lossfn_mse(-10,-1),color=&#39;gray&#39;) ## mseloss ax2.scatter(-1,5.1,lossfn_mse(-1,5.1),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## mseloss ax3.scatter(X,y,alpha=0.01) ax3.plot(X,v,&#39;--&#39;) line3, = ax3.plot(X,1/(1+torch.exp(-w0_bce[0]-w1_bce[0]*X)),&#39;--&#39;) ax4.scatter(X,y,alpha=0.01) ax4.plot(X,v,&#39;--&#39;) line4, = ax4.plot(X,1/(1+torch.exp(-w0_mse[0]-w1_mse[0]*X)),&#39;--&#39;) ### animation def animate(i): ax1.scatter(w0_bce[i],w1_bce[i],lossfn_crossenp(w0_bce[i],w1_bce[i]),color=&#39;gray&#39;) ax2.scatter(w0_mse[i],w1_mse[i],lossfn_mse(w0_mse[i],w1_mse[i]),color=&#39;gray&#39;) line3.set_ydata(1/(1+torch.exp(-w0_bce[i]-w1_bce[i]*X))) line4.set_ydata(1/(1+torch.exp(-w0_mse[i]-w1_mse[i]*X))) return line3,line4 ani = animation.FuncAnimation(fig, animate, frames=50) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#47784;&#54805;&#51032; &#54364;&#54788;&#47141;: &#50780; &#49888;&#44221;&#47581;&#51008; &#44618;&#50612;&#51276;&#45716;&#44032;? . &#45331;&#51008; &#49888;&#44221;&#47581; (&#54616;&#45208;&#51032; &#51008;&#45769;&#52789; + &#52649;&#48516;&#55176; &#53360; &#45432;&#46300;&#47484; &#44032;&#51652; &#49888;&#44221;&#47581;) . - (universal approximation theorem) 하나의 은닉층과 충분히 큰 노드를 가진 신경망은 거의 모든 함수를 근사할 수 있다. . - 핵심아이디어: (node1=선형+비선형) + (node2=선형+비선형) $ to$ locally compact basis $ to$ 구불구불하게 다 맞출수가 있다. . 선형변환을 무한번 선형변환해도 결과는 그냥 선형변환이다 $ to$ 모든 range에 값이 있는 basis $ to$ 표현력이 약하다. (한쪽을 맞추면 다른쪽을 맞추기 힘듬) | 하지만 아주 단순한 비선형변환을 섞기만 해도 표현력이 비약적으로 상승한다. | . - 트릭은 비선형변환 . &#44536;&#47111;&#45796;&#47732; &#50780; &#45331;&#51008; &#49888;&#44221;&#47581;&#51012; &#50416;&#51648; &#50506;&#45716;&#44032;? . - 안전한 대답 (그리고 쓸모없는 대답): 실험적으로 깊은 신경망이 더 효과적임이 입증되었다. . - 좀 더 고민을 해본 대답 . 넓은신경망보다 깊은신경망이 파라메터수 대비 복잡도를 더 쉽게 올릴수 있다. | 넓은신경망보다 깊은신경망이 오퍼피팅 이슈를 피하기 쉽다. | . - 내 생각 . 깊은 신경망은 계층적인 모형이다. | 즉 깊은 신경망은 여러스케일로 자료를 관찰한다. | . Pytoch MLP (MNIST 3,7) . import torch from fastai.vision.all import * . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . data . - download data . path = untar_data(URLs.MNIST_SAMPLE) . path.ls() . (#3) [Path(&#39;/home/cgb4/.fastai/data/mnist_sample/labels.csv&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_sample/train&#39;),Path(&#39;/home/cgb4/.fastai/data/mnist_sample/valid&#39;)] . - list . threes=(path/&#39;train&#39;/&#39;3&#39;).ls() sevens=(path/&#39;train&#39;/&#39;7&#39;).ls() . - list $ to$ image . Image.open(threes[4]) . - image $ to$ tensor . tensor(Image.open(threes[4])) . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 72, 156, 241, 254, 255, 188, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 168, 250, 232, 147, 79, 143, 254, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 109, 231, 164, 39, 0, 0, 0, 86, 251, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 40, 0, 0, 0, 0, 4, 200, 157, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 92, 249, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 221, 128, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 147, 185, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 137, 224, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 137, 239, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 239, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 245, 104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 179, 254, 224, 217, 147, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 44, 117, 117, 196, 237, 104, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 117, 246, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 85, 241, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 225, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 170, 131, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 104, 0, 0, 0, 0, 17, 234, 87, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 198, 179, 29, 0, 42, 199, 235, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 154, 236, 250, 252, 163, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8) . 여기에서 tensor는 파이토치가 아니라 fastai에서 구현한 함수임 | . - 여러개의 리스트를 모두 텐서로 바꿔보자. . seven_tensor = torch.stack([tensor(Image.open(i)) for i in sevens]).float()/255 three_tensor = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255 . - $X$와 $y$를 만들자. . seven_tensor.shape, three_tensor.shape . (torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28])) . y=torch.tensor([0.0]*6265+ [1.0]*6131).reshape(12396,1) . X=torch.vstack([seven_tensor,three_tensor]).reshape(12396,-1) . X.shape, y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . &#47784;&#54805; . ${ bf X} to { bf WX+b} to f({ bf WX+b}) to dots to { bf y}$ . ${ bf X}=12396 times 784$ matrix | ${ bf y}=12396 times 1$ (col) vector | . - 모델을 어떻게 구성할것인가? . 아키텍처: 적당히 깊게... + 적당히 넓게... + 표현력이 충분하면서도 + 과적합은 일어나지 않도록.. (저도 잘 몰라요) | 손실함수: BCEloss | 옵티마이저: Adam | . - 교재의 모형 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node30&quot; -&gt; &quot;y&quot; label = &quot;Layer 2: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2: Sigmoid x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y y node1&#45;&gt;y node2&#45;&gt;y ...&#45;&gt;y node30&#45;&gt;y &#54400;&#51060;1 . - 그럼 이제 풀어보자. (아키텍처만 만들어주면 금방구현한다.) . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28, out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30, out_features=1), torch.nn.Sigmoid() ) optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= -torch.mean(y*torch.log(yhat)+(1-y)*torch.log(1-yhat)) ## BCEloss ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], requires_grad=True), Parameter containing: tensor([-0.0098, 0.0315, 0.0363, -0.0093, 0.1433, 0.0175, 0.0139, -0.0237, 0.0323, 0.0351, -0.0125, 0.0443, 0.0176, 0.0745, 0.0098, 0.0042, 0.0361, 0.0394, 0.0534, 0.0175, 0.0567, 0.0148, 0.0459, 0.0648, 0.0009, -0.0279, 0.0972, 0.0478, 0.0612, 0.0504], requires_grad=True), Parameter containing: tensor([[ 0.2154, 0.1926, 0.2019, 0.1671, -0.1840, -0.0726, -0.1608, 0.1046, -0.2522, -0.2444, 0.1257, -0.1815, 0.1002, -0.0963, -0.3047, 0.1256, 0.1862, 0.2499, -0.1381, 0.2051, -0.2633, 0.1915, -0.1853, -0.1719, 0.1156, 0.1573, -0.1129, 0.1308, -0.1625, -0.1472]], requires_grad=True), Parameter containing: tensor([-0.1153], requires_grad=True)] . plt.plot(y) plt.plot(yhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fce0c58d070&gt;] . ypred=yhat&gt;0.5 . sum(ypred==y)/12396 . tensor([0.9893]) . &#54400;&#51060;2: torch&#50640; &#45236;&#51109;&#46108; &#49552;&#49892;&#54632;&#49688; &#51060;&#50857; . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28, out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30, out_features=1), #torch.nn.Sigmoid() ) loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat=net(X) ## 2 loss= loss_fn(yhat,y) ## BCEloss ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], requires_grad=True), Parameter containing: tensor([-0.0098, 0.0315, 0.0363, -0.0093, 0.1433, 0.0175, 0.0139, -0.0237, 0.0323, 0.0351, -0.0125, 0.0443, 0.0176, 0.0745, 0.0098, 0.0042, 0.0361, 0.0394, 0.0534, 0.0175, 0.0567, 0.0148, 0.0459, 0.0648, 0.0009, -0.0279, 0.0972, 0.0478, 0.0612, 0.0504], requires_grad=True), Parameter containing: tensor([[ 0.2154, 0.1926, 0.2019, 0.1671, -0.1840, -0.0726, -0.1608, 0.1046, -0.2522, -0.2444, 0.1257, -0.1815, 0.1002, -0.0963, -0.3047, 0.1256, 0.1862, 0.2499, -0.1381, 0.2051, -0.2633, 0.1915, -0.1853, -0.1719, 0.1156, 0.1573, -0.1129, 0.1308, -0.1625, -0.1472]], requires_grad=True), Parameter containing: tensor([-0.1153], requires_grad=True)] . plt.plot(y) plt.plot(yhat.data,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fce0c364220&gt;] . f=torch.nn.Sigmoid() plt.plot(y) plt.plot(f(yhat.data),&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fce0c304af0&gt;] . &#54400;&#51060;3: torch&#50640; &#45236;&#51109;&#46108; &#49552;&#49892;&#54632;&#49688; &#51060;&#50857; + GPU . torch.manual_seed(1) net = torch.nn.Sequential( torch.nn.Linear(in_features=28*28, out_features=30), torch.nn.ReLU(), torch.nn.Linear(in_features=30, out_features=1), #torch.nn.Sigmoid() ) . net.to(&quot;cuda:0&quot;) . Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) . X_gpu=X.to(&quot;cuda:0&quot;) y_gpu=y.to(&quot;cuda:0&quot;) . loss_fn=torch.nn.BCEWithLogitsLoss() optimizer=torch.optim.Adam(net.parameters()) . for epoc in range(200): ## 1 yhat_gpu=net(X_gpu) ## 2 loss= loss_fn(yhat_gpu,y_gpu) ## BCEloss ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 0.0184, -0.0158, -0.0069, ..., 0.0068, -0.0041, 0.0025], [-0.0274, -0.0224, -0.0309, ..., -0.0029, 0.0013, -0.0167], [ 0.0282, -0.0095, -0.0340, ..., -0.0141, 0.0056, -0.0335], ..., [ 0.0267, 0.0186, -0.0326, ..., 0.0047, -0.0072, -0.0301], [-0.0190, 0.0291, 0.0221, ..., 0.0067, 0.0206, 0.0151], [ 0.0226, 0.0331, 0.0182, ..., 0.0150, 0.0278, -0.0073]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([-0.0098, 0.0315, 0.0363, -0.0093, 0.1433, 0.0175, 0.0139, -0.0238, 0.0323, 0.0351, -0.0125, 0.0443, 0.0176, 0.0745, 0.0098, 0.0042, 0.0361, 0.0394, 0.0534, 0.0175, 0.0567, 0.0148, 0.0459, 0.0648, 0.0009, -0.0279, 0.0972, 0.0478, 0.0612, 0.0504], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([[ 0.2154, 0.1926, 0.2019, 0.1671, -0.1840, -0.0726, -0.1608, 0.1046, -0.2522, -0.2444, 0.1257, -0.1815, 0.1002, -0.0963, -0.3047, 0.1256, 0.1862, 0.2499, -0.1381, 0.2051, -0.2633, 0.1915, -0.1853, -0.1719, 0.1156, 0.1573, -0.1129, 0.1308, -0.1625, -0.1472]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([-0.1153], device=&#39;cuda:0&#39;, requires_grad=True)] .",
            "url": "https://guebin.github.io/2021BDA/2021/10/12/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%9412%EC%9D%BC.html",
            "relUrl": "/2021/10/12/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%9412%EC%9D%BC.html",
            "date": " • Oct 12, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "(5주차) 10월7일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/2) 기말고사 안내 + 알렉스넷 . - (2/2) 로지스틱 회귀분석 . Logistic regression . import torch import matplotlib.pyplot as plt . Example . - 현실에서 이런 경우가 많음 . $x$가 커질수록 (혹은 작아질수록) 성공확률이 증가함. | . - 이러한 모형은 아래와 같이 설계할 수 있음 &lt; 외우세요!!! . $y_i sim Ber( pi_i), quad $ where $ pi_i = frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$ . | $ hat{y}_i= frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= frac{1}{1+ exp(- hat{w}_0- hat{w}_1x_i)}$ . | $loss= - sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ &lt; 외우세요!! . | . - 예제시작 . X=torch.linspace(-1,1,2000).reshape(2000,1) w0= - 1 w1= 5 u = w0+X*w1 v = torch.exp(u)/(1+torch.exp(u)) # v=πi y = torch.bernoulli(v) . plt.scatter(X,y,alpha=0.05) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f74306145b0&gt;] . - 다이어그램으로 표현하면 . import graphviz . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39; + s + &#39;; }&#39;) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;X@W&quot;[label=&quot;@W&quot;] &quot;X@W&quot; -&gt; &quot;Sigmoid(X@W)=yhat&quot;[label=&quot;Sigmoid&quot;] label = &quot;Layer 1&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 X X X@W X@W X&#45;&gt;X@W @W Sigmoid(X@W)=yhat Sigmoid(X@W)=yhat X@W&#45;&gt;Sigmoid(X@W)=yhat Sigmoid gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; X label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; X -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Sigmoid X X node1=yhat node1=yhat X&#45;&gt;node1=yhat - 아키텍처, 손실함수, 옵티마이저 . torch.manual_seed(43052) l1=torch.nn.Linear(in_features=1,out_features=1,bias=True) a1=torch.nn.Sigmoid() net=torch.nn.Sequential(l1,a1) #loss = torch.mean((y-yhat)**2) &lt; 이러면 안됩니다!!! optimizer=torch.optim.SGD(net.parameters(),lr=0.05) . plt.scatter(X,y,alpha=0.01) plt.plot(X,net(X).data,&#39;--&#39;) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f743046b5e0&gt;] . - step1~4 . for epoc in range(10000): ## 1 yhat=net(X) ## 2 loss=-torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[4.7395]], requires_grad=True), Parameter containing: tensor([-0.8655], requires_grad=True)] . plt.scatter(X,y,alpha=0.01) plt.plot(X,net(X).data,&#39;--&#39;) plt.plot(X,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f74284afb50&gt;] . &#49689;&#51228; . loss를 mse로 바꿔서 돌려볼것 . torch.manual_seed(43052) l1=torch.nn.Linear(in_features=1,out_features=1,bias=True) a1=torch.nn.Sigmoid() net=torch.nn.Sequential(l1,a1) #loss = torch.mean((y-yhat)**2) &lt; 이러면 안됩니다!!! optimizer=torch.optim.SGD(net.parameters(),lr=0.05) . for epoc in range(10000): ## 1 yhat=net(X) ## 2 ##### loss=-torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) &lt;-- 여기만수정해서!! ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . plt.scatter(X,y,alpha=0.01) plt.plot(X,net(X).data,&#39;--&#39;) plt.plot(X,v,&#39;--r&#39;) .",
            "url": "https://guebin.github.io/2021BDA/2021/10/07/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%947%EC%9D%BC.html",
            "relUrl": "/2021/10/07/(5%EC%A3%BC%EC%B0%A8)-10%EC%9B%947%EC%9D%BC.html",
            "date": " • Oct 7, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "(4주차) 10월5일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/5) : 신경망 다어어그램 소개, polynomial regression (1) . - (2/5) : polynomial regression (2), peicewise linear model . - (3/5) : 표현력의 증가, 신경망 다이어그램을 다시 소개 . - (4/5) : 노드수가 증가하면 국소최소점에 빠질 확률이 높음 (파라메터 학습어려움) . - (5/5) : 노드수가 증가하면 오버피팅 이슈가 있음 . Import . import torch import numpy as np import matplotlib.pyplot as plt . graphviz setting . import graphviz . 설치가 되어있지 않다면 아래를 실행할것 | . !conda install -c conda-forge python-graphviz . ref: https://anaconda.org/conda-forge/python-graphviz | . - 다이어그램을 그리기 위한 준비 . def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . &#50696;&#51228;1: &#49440;&#54805;&#47784;&#54805; . - $y_i= w_0+w_1 x_i + epsilon_i Longrightarrow hat{y}_i = hat{w}_0+ hat{w}_1 x_i$ . $ epsilon_i sim N(0,1)$ | . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;w0 + x*w1&quot;[label=&quot;* w0&quot;] &quot;x&quot; -&gt; &quot;w0 + x*w1&quot; [label=&quot;* w1&quot;] &quot;w0 + x*w1&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 w0 + x*w1 w0 + x*w1 1&#45;&gt;w0 + x*w1 * w0 yhat yhat w0 + x*w1&#45;&gt;yhat indentity x x x&#45;&gt;w0 + x*w1 * w1 gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@W, bias=False&quot;[label=&quot;@W&quot;] ; &quot;X@W, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@W, bias=False X@W, bias=False X&#45;&gt;X@W, bias=False @W yhat yhat X@W, bias=False&#45;&gt;yhat indentity gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] ; &quot;x*w, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*w, bias=True x*w, bias=True x&#45;&gt;x*w, bias=True *w yhat yhat x*w, bias=True&#45;&gt;yhat indentity &#50696;&#51228;2: polynomial regression . $y_i=w_0+w_1x_i + w_2 x_i^2 + w_3 x_i^3 + epsilon_i$ . gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@W, bias=True&quot;[label=&quot;@W&quot;] &quot;X@W, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@W, bias=True X@W, bias=True X&#45;&gt;X@W, bias=True @W yhat yhat X@W, bias=True&#45;&gt;yhat indentity ${ bf X} = begin{bmatrix} x_1 &amp; x_1^2 &amp; x_1^3 x_2 &amp; x_2^2 &amp; x_2^3 dots &amp; dots &amp; dots x_n &amp; x_n^2 &amp; x_n^3 end{bmatrix}, quad { bf W} = begin{bmatrix} w_1 w_2 w_3 end{bmatrix}$. | . &#49884;&#48044;&#47112;&#51060;&#49496; &#50672;&#49845; . - 모형 . torch.manual_seed(43052) x,_ = torch.randn(100).sort() X=torch.vstack([x,x**2,x**3]).T W=torch.tensor([[4.0],[3.0],[-2.0]]) bias=1.0 ϵ=torch.randn(100,1) y=X@W+bias + ϵ . plt.plot(X[:,0],y,&#39;.&#39;) #plt.plot(X[:,0],X@W+bias,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac46c1c430&gt;] . - 아키텍처 . net = torch.nn.Linear(in_features=3,out_features=1,bias=True) . - 손실함수 . loss_fn=torch.nn.MSELoss() . - 옵티마이저 . optimizer= torch.optim.SGD(net.parameters(),lr=0.01) . - step1~4 . for epoc in range(1000): ## 1 yhat=net(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . list(net.parameters()) . [Parameter containing: tensor([[ 3.7411, 2.8648, -1.9074]], requires_grad=True), Parameter containing: tensor([1.0239], requires_grad=True)] . plt.plot(X[:,0],y,&#39;.&#39;) plt.plot(X[:,0],yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3e3e5700&gt;] . &#50696;&#51228;3: piece-wise linear regression . - 모델 . _x = np.linspace(-1,1,100).tolist() _f = lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5 +np.random.normal()*0.3 _y = list(map(_f,_x)) . plt.plot(_x,_y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3e350340&gt;] . X=torch.tensor(_x).reshape(100,1) y=torch.tensor(_y).reshape(100,1) . &#54400;&#51060;1 . - 아키텍처 + 손실함수(MSE) + 옵티마이저(SGD) . net=torch.nn.Linear(in_features=1,out_features=1,bias=True) loss_fn = torch.nn.MSELoss() optimizer = torch.optim.SGD(net.parameters(),lr=0.1) . - step1~4 . for epoc in range(10000): ## 1 yhat=net(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer.step() net.zero_grad() . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3e2c8550&gt;] . - 실패: 그리고 epoc을 10억번 반복해도 이건 실패할 모형임 . 왜? 모델자체가 틀렸음. | 모델의 표현력이 너무 부족하다. $ to$ underfitting | . &#54400;&#51060;2 (&#48708;&#49440;&#54805; &#54876;&#49457;&#54868;&#54632;&#49688;&#47484; &#46020;&#51077;) . - 비선형활성화함수를 도입하자. (네트워크수정) . torch.manual_seed(1) layer1 = torch.nn.Linear(in_features=1,out_features=1,bias=False) activation1 = torch.nn.ReLU() layer2 = torch.nn.Linear(in_features=1,out_features=1,bias=False) net2 = torch.nn.Sequential(layer1,activation1,layer2) . _x=np.linspace(-1,1,100) plt.plot(_x,_x) plt.plot(_x,activation1(torch.tensor(_x))) . [&lt;matplotlib.lines.Line2D at 0x7fac3e2be5e0&gt;] . - 표현력 확인 . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net2(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3e22cf70&gt;] . - 옵티마이저2 . optimizer2 = torch.optim.SGD(net2.parameters(),lr=0.1) . - step1~4 . for epoc in range(1000): ## 1 yhat=net2(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer2.step() net2.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3e1a2b80&gt;] . - discussion . 이것 역시 수백억번 epoc을 반복해도 이 이상 적합하기 힘들다. $ to$ 모형의 표현력이 낮다. | 해결책: 주황색점선이 2개 있다면 어떨까? | . &#54400;&#51060;3 (&#45432;&#46300;&#49688; &#52628;&#44032;) . - 아키텍처 + 옵티마이저 . torch.manual_seed(1) ## 초기가중치를 동일하게 layer1 = torch.nn.Linear(in_features=1,out_features=2,bias=False) activation1 = torch.nn.ReLU() layer2 = torch.nn.Linear(in_features=2,out_features=1,bias=False) net3 = torch.nn.Sequential(layer1,activation1,layer2) optimizer3= torch.optim.SGD(net3.parameters(),lr=0.1) . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net3(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3e120640&gt;] . - Step 1~4 . for epoc in range(1000): ## 1 yhat=net3(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer3.step() net3.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3e0940d0&gt;] . - discussion . list(net3.parameters()) . [Parameter containing: tensor([[ 1.9052], [-1.0738]], requires_grad=True), Parameter containing: tensor([[ 1.8533, -1.0374]], requires_grad=True)] . 파라메터확인 | . W1=(layer1.weight.data).T W2=(layer2.weight.data).T W1,W2 . (tensor([[ 1.9052, -1.0738]]), tensor([[ 1.8533], [-1.0374]])) . 파라메터 저장 | . - 어떻게 적합이 이렇게 우수하게 되었는지 따져보자. . u1=X@W1 plt.plot(u1) #plt.plot(X@W1) . [&lt;matplotlib.lines.Line2D at 0x7fac3e07d4c0&gt;, &lt;matplotlib.lines.Line2D at 0x7fac3e07d4f0&gt;] . v1=activation1(u1) plt.plot(v1) #plt.plot(activation1(X@W1)) . [&lt;matplotlib.lines.Line2D at 0x7fac3dfe5e50&gt;, &lt;matplotlib.lines.Line2D at 0x7fac3dfe5e80&gt;] . _yhat=v1@W2 plt.plot(X,y,&#39;.&#39;) plt.plot(X,_yhat,&#39;--&#39;) #plt.plot(X,activation1(X@W1)@W2,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3df58df0&gt;] . &#51104;&#44624;&#50836;&#50557; (&#49888;&#44221;&#47581;) . - 계산과정 . (1) $X to X@W^{(1)} to ReLU(X@W^{(1)}) to ReLU(X@W^{(1)})@W^{(2)}=yhat$ . $X: n times 1$ | $W^{(0)}: 1 times 2$ | $W^{(1)}: 2 times 1$ | . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;X@W1&quot;[label=&quot;@W1&quot;] &quot;X@W1&quot; -&gt; &quot;ReLU(X@W1)&quot;[label=&quot;ReLU&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;ReLU(X@W1)&quot; -&gt; &quot;ReLU(X@W1)@W2:=yhat&quot;[label=&quot;@W2&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 cluster_3 Layer 2 X X X@W1 X@W1 X&#45;&gt;X@W1 @W1 ReLU(X@W1) ReLU(X@W1) X@W1&#45;&gt;ReLU(X@W1) ReLU ReLU(X@W1)@W2:=yhat ReLU(X@W1)@W2:=yhat ReLU(X@W1)&#45;&gt;ReLU(X@W1)@W2:=yhat @W2 (2) 아래와 같이 표현할 수도 있다. . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;u1[:,0]&quot;[label=&quot;*W1[0,0]&quot;] &quot;X&quot; -&gt; &quot;u1[:,1]&quot;[label=&quot;*W1[0,1]&quot;] &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;Relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;Relu&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;v1[:,0]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[0,0]&quot;] &quot;v1[:,1]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[1,0]&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 cluster_3 Layer 2 X X u1[:,0] u1[:,0] X&#45;&gt;u1[:,0] *W1[0,0] u1[:,1] u1[:,1] X&#45;&gt;u1[:,1] *W1[0,1] v1[:,0] v1[:,0] u1[:,0]&#45;&gt;v1[:,0] Relu v1[:,1] v1[:,1] u1[:,1]&#45;&gt;v1[:,1] Relu yhat yhat v1[:,0]&#45;&gt;yhat *W2[0,0] v1[:,1]&#45;&gt;yhat *W2[1,0] gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; label = &quot;Layer 1: ReLU&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: ReLU cluster_3 Layer 2 X X node1 node1 X&#45;&gt;node1 node2 node2 X&#45;&gt;node2 yhat yhat node1&#45;&gt;yhat node2&#45;&gt;yhat - 위와 같은 다이어그램을 적용하면 예제1은 아래와 같이 표현가능 . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;1&quot; &quot;x&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;1&quot; -&gt; &quot;node1=yhat&quot; &quot;x&quot; -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Identity&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Identity 1 1 node1=yhat node1=yhat 1&#45;&gt;node1=yhat x x x&#45;&gt;node1=yhat gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;x&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x&quot; -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Identity&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Identity x x node1=yhat node1=yhat x&#45;&gt;node1=yhat - 예제2의 아키텍처 . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;x&quot; &quot;x**2&quot; &quot;x**3&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x&quot; -&gt; &quot;node1=yhat&quot; &quot;x**2&quot; -&gt; &quot;node1=yhat&quot; &quot;x**3&quot; -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: Identity&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: Identity x x node1=yhat node1=yhat x&#45;&gt;node1=yhat x**2 x**2 x**2&#45;&gt;node1=yhat x**3 x**3 x**3&#45;&gt;node1=yhat &#54400;&#51060;3&#51060; &#49892;&#54056;&#54624; &#49688;&#46020; &#51080;&#51020; . - 아키텍처 + 옵티마이저 . torch.manual_seed(40352) ## 초기가중치를 동일하게 layer1 = torch.nn.Linear(in_features=1,out_features=2,bias=False) activation1 = torch.nn.ReLU() layer2 = torch.nn.Linear(in_features=2,out_features=1,bias=False) net3 = torch.nn.Sequential(layer1,activation1,layer2) optimizer3= torch.optim.SGD(net3.parameters(),lr=0.1) . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net3(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3ded5be0&gt;] . - Step 1~4 . for epoc in range(10000): ## 1 yhat=net3(X) ## 2 loss=loss_fn(y,yhat) ## 3 loss.backward() ## 4 optimizer3.step() net3.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3dec55b0&gt;] . - 왜 가중치가 변하지 않는가? (이것보다 더 좋은 fitting이 있음을 우리는 이미 알고있는데..) . W1=(layer1.weight.data).T W2=(layer2.weight.data).T W1,W2 . (tensor([[2.8065e-04, 1.8409e+00]]), tensor([[0.0719], [1.9181]])) . u1=X@W1 plt.plot(u1) #plt.plot(X@W1) . [&lt;matplotlib.lines.Line2D at 0x7fac3de21c10&gt;, &lt;matplotlib.lines.Line2D at 0x7fac3de21c40&gt;] . v1=activation1(u1) plt.plot(v1) #plt.plot(activation1(X@W1)) . [&lt;matplotlib.lines.Line2D at 0x7fac3dd958b0&gt;, &lt;matplotlib.lines.Line2D at 0x7fac3dd958e0&gt;] . _yhat=v1@W2 plt.plot(X,y,&#39;.&#39;) plt.plot(X,_yhat,&#39;--&#39;) #plt.plot(X,activation1(X@W1)@W2,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3dd0f5e0&gt;] . - 고약한 상황에 빠졌음. . &#54400;&#51060;4: &#45331;&#51008; &#49888;&#44221;&#47581; . - Custom Activation Function . def mooyaho(input): return torch.sigmoid(200*input) class MOOYAHO(torch.nn.Module): def __init__(self): super().__init__() # init the base class def forward(self, input): return mooyaho(input) # simply apply already implemented SiLU . _x=torch.linspace(-10,10,100) plt.plot(_x,mooyaho(_x)) . [&lt;matplotlib.lines.Line2D at 0x7fac3dcf7550&gt;] . - 아키텍처 . torch.manual_seed(1) # 초기가중치를 똑같이 하기 위해서.. layer1=torch.nn.Linear(in_features=1,out_features=500,bias=True) activation1=MOOYAHO() layer2=torch.nn.Linear(in_features=500,out_features=1,bias=True) net4=torch.nn.Sequential(layer1,activation1,layer2) optimizer4=torch.optim.SGD(net4.parameters(),lr=0.001) . plt.plot(X,y,&#39;.&#39;) plt.plot(X,net4(X).data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3dc63fd0&gt;] . - step1~4 . for epoc in range(5000): # 1 yhat=net4(X) # 2 loss=loss_fn(yhat,y) # 3 loss.backward() # 4 optimizer4.step() net4.zero_grad() . - result . plt.plot(X,y,&#39;.&#39;) plt.plot(X,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3c3d2bb0&gt;] . - 넓은 신경망은 과적합을 하는 경우가 종종있다. . - 무엇이든 맞출 수 있음 . torch.manual_seed(43052) __X = torch.linspace(-1,1,100).reshape(100,1) __y = torch.randn(100,1) . plt.plot(__X,__y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3c34c1c0&gt;] . torch.manual_seed(1) # 초기가중치를 똑같이 하기 위해서.. layer1=torch.nn.Linear(in_features=1,out_features=500,bias=True) activation1=MOOYAHO() layer2=torch.nn.Linear(in_features=500,out_features=1,bias=True) net4=torch.nn.Sequential(layer1,activation1,layer2) optimizer4=torch.optim.SGD(net4.parameters(),lr=0.001) . - step1~4 . for epoc in range(5000): # 1 __yhat=net4(__X) # 2 loss=loss_fn(__yhat,__y) # 3 loss.backward() # 4 optimizer4.step() net4.zero_grad() . - result . plt.plot(__X,__y,) plt.plot(__X,__yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fac3c333fd0&gt;] . loss_fn(__y,__y*0), loss_fn(__y,__yhat.data) . (tensor(1.1437), tensor(0.7460)) . &#49689;&#51228; . - 예제2: polynomial regression 에서 . optimizer= torch.optim.SGD(net.parameters(),lr=0.01) . 대신에 . optimizer= torch.optim.SGD(net.parameters(),lr=0.1) . 로 변경하여 학습하고 결과를 관찰할것. . 설명: 위와 같은 결과가 나온 이유는... .",
            "url": "https://guebin.github.io/2021BDA/2021/10/05/(4%EC%A3%BC%EC%B0%A8)-10%EC%9B%945%EC%9D%BC.html",
            "relUrl": "/2021/10/05/(4%EC%A3%BC%EC%B0%A8)-10%EC%9B%945%EC%9D%BC.html",
            "date": " • Oct 5, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "(4주차) 9월30일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/4) Step 1~2 요약 (1) . - (2/4) Step 1~2 요약 (2), Step 3: derivation . - (3/4) Step 4: update (1) . - (4/4) Step 4: update (2), Step 1~4 를 for 문으로 처리 . import torch import numpy as np . Data . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(43052) n=100 ones= torch.ones(n) x,_ = torch.randn(n).sort() X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ ytrue = X@W . step1~2 &#50836;&#50557; . &#48169;&#48277;1: &#47784;&#45944;&#51012; &#51649;&#51217;&#49440;&#50616; + loss&#54632;&#49688;&#46020; &#51649;&#51217;&#49440;&#50616; . What1=torch.tensor([-5.0,10.0],requires_grad=True) yhat1=X@What1 loss1=torch.mean((y-yhat1)**2) loss1 . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;2: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=False) + loss &#51649;&#51217;&#49440;&#50616; . net2=torch.nn.Linear(in_features=2,out_features=1,bias=False) net2.weight.data= torch.tensor([[-5.0,10.0]]) yhat2=net2(X) loss2=torch.mean((y.reshape(100,1)-yhat2)**2) loss2 . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;3: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=True) + loss &#51649;&#51217;&#49440;&#50616; . net3=torch.nn.Linear(in_features=1,out_features=1,bias=True) net3.weight.data= torch.tensor([[10.0]]) net3.bias.data= torch.tensor([[-5.0]]) yhat3=net3(x.reshape(100,1)) loss3=torch.mean((y.reshape(100,1)-yhat3)**2) loss3 . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;4: &#47784;&#45944;&#49885;&#51012; &#51649;&#51217;&#49440;&#50616; + loss&#54632;&#49688;&#45716; torch.nn.MSELoss() . What4=torch.tensor([-5.0,10.0],requires_grad=True) yhat4=X@What4 lossfn=torch.nn.MSELoss() loss4=lossfn(y,yhat4) loss4 . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . &#48169;&#48277;5: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=False) + loss&#54632;&#49688;&#45716; torch.nn.MSELoss() . net5=torch.nn.Linear(in_features=2,out_features=1,bias=False) net5.weight.data= torch.tensor([[-5.0,10.0]]) yhat5=net5(X) #lossfn=torch.nn.MSELoss() loss5=lossfn(y.reshape(100,1),yhat5) loss5 . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . &#48169;&#48277;6: &#47784;&#45944;&#49885;&#51012; torch.nn&#51004;&#47196; &#49440;&#50616; (bias=True) + loss&#54632;&#49688;&#45716; torch.nn.MSELoss() . net6=torch.nn.Linear(in_features=1,out_features=1,bias=True) net6.weight.data= torch.tensor([[10.0]]) net6.bias.data= torch.tensor([[-5.0]]) yhat6=net6(x.reshape(100,1)) loss6=lossfn(y.reshape(100,1),yhat6) loss6 . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . step3: derivation . loss1 . loss1.backward() . What1.grad.data . tensor([-13.4225, 11.8893]) . 이것이 손계산을 통한 이론적인 미분값과 일치함은 이전시간에 확인하였음. | . loss2 . loss2.backward() . net2.weight.grad . tensor([[-13.4225, 11.8893]]) . loss3 . loss3.backward() . net3.bias.grad,net3.weight.grad . (tensor([[-13.4225]]), tensor([[11.8893]])) . loss4 . loss4.backward() . What4.grad.data . tensor([-13.4225, 11.8893]) . loss5 . loss5.backward() . net5.weight.grad . tensor([[-13.4225, 11.8893]]) . loss6 . loss6.backward() . net6.bias.grad,net6.weight.grad . (tensor([[-13.4225]]), tensor([[11.8893]])) . step4: update . loss1 . What1.data ## update 전 . tensor([-5., 10.]) . lr=0.1 What1.data = What1.data - lr*What1.grad.data ## update 후 What1 . tensor([-3.6577, 8.8111], requires_grad=True) . loss2 . net2.weight.data . tensor([[-5., 10.]]) . optmz2 = torch.optim.SGD(net2.parameters(),lr=0.1) . optmz2.step() ## update . net2.weight.data ## update 후 . tensor([[-3.6577, 8.8111]]) . loss3 . net3.bias.data,net3.weight.data . (tensor([[-5.]]), tensor([[10.]])) . optmz3 = torch.optim.SGD(net3.parameters(),lr=0.1) . optmz3.step() . net3.bias.data,net3.weight.data . (tensor([[-3.6577]]), tensor([[8.8111]])) . list(net3.parameters()) . [Parameter containing: tensor([[8.8111]], requires_grad=True), Parameter containing: tensor([[-3.6577]], requires_grad=True)] . loss4 . What4.data ## update 전 . tensor([-5., 10.]) . lr=0.1 What4.data = What4.data - lr*What4.grad.data ## update 후 What4 . tensor([-3.6577, 8.8111], requires_grad=True) . loss5 . net5.weight.data . tensor([[-5., 10.]]) . optmz5 = torch.optim.SGD(net5.parameters(),lr=0.1) . optmz5.step() ## update . net5.weight.data ## update 후 . tensor([[-3.6577, 8.8111]]) . loss6 . net6.bias.data,net6.weight.data . (tensor([[-5.]]), tensor([[10.]])) . optmz6 = torch.optim.SGD(net6.parameters(),lr=0.1) . optmz6.step() . net6.bias.data,net6.weight.data . (tensor([[-3.6577]]), tensor([[8.8111]])) . step1~4&#47484; &#48152;&#48373;&#54616;&#47732;&#46108;&#45796;. . net=torch.nn.Linear(in_features=2,out_features=1,bias=False) ## 모형정의 optmz=torch.optim.SGD(net.parameters(),lr=0.1) mseloss=torch.nn.MSELoss() for epoc in range(100): # step1: yhat yhat=net(X) ## yhat 계산 # step2: loss loss=mseloss(y.reshape(100,1),yhat) # step3: derivation loss.backward() # step4: update optmz.step() optmz.zero_grad() ## 외우세요.. . list(net.parameters()) . [Parameter containing: tensor([[2.4459, 4.0043]], requires_grad=True)] . &#49689;&#51228; . 아래를 실행해보고 결과를 관찰하라. . net=torch.nn.Linear(in_features=2,out_features=1,bias=False) ## 모형정의 optmz=torch.optim.SGD(net.parameters(),lr=0.1) mseloss=torch.nn.MSELoss() for epoc in range(100): # step1: yhat yhat=net(X) ## yhat 계산 # step2: loss loss=mseloss(y.reshape(100,1),yhat) # step3: derivation loss.backward() # step4: update optmz.step() .",
            "url": "https://guebin.github.io/2021BDA/2021/09/30/(4%EC%A3%BC%EC%B0%A8)-9%EC%9B%9430%EC%9D%BC.html",
            "relUrl": "/2021/09/30/(4%EC%A3%BC%EC%B0%A8)-9%EC%9B%9430%EC%9D%BC.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "(3주차) 9월28일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/3): 9월14-16일 강의노트의 일부내용 추가설명 . - (2/3): torch.nn.Linear()를 사용하여 yhat을 계산하기, torch.nn.MSELoss()를 이용하여 loss를 계산하기 . - (3/3): 과제설명 . Import . import torch import numpy as np . Data . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(43052) n=100 ones= torch.ones(n) x,_ = torch.randn(n).sort() X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ ytrue = X@W . &#51060;&#51204;&#48169;&#48277;&#50836;&#50557; . - step1: yhat . - step2: loss . - step3: derivation . - step4: update . step1: yhat . - feedforward 신경망을 설계하는 과정 . - 이 단계가 잘 완료되었다면, 임의의 ${ bf hat{W}}$을 넣었을 때 $ bf hat{y}$를 계산할 수 있어야 함 . &#48169;&#48277;1: &#51649;&#51217;&#49440;&#50616; (&#45236;&#44032; &#44277;&#49885;&#51012; &#50508;&#44256; &#51080;&#50612;&#50556; &#54620;&#45796;) . What=torch.tensor([-5.0,10.0],requires_grad=True) . yhat1=X@What . yhat1 . tensor([-29.8211, -28.6215, -24.9730, -21.2394, -19.7919, -19.6354, -19.5093, -19.4352, -18.7223, -18.0793, -16.9040, -16.0918, -16.0536, -15.8746, -14.4690, -14.3193, -13.6426, -12.8578, -12.5486, -12.4213, -11.9484, -11.1034, -10.8296, -10.6210, -10.5064, -10.0578, -9.8063, -9.7380, -9.7097, -9.6756, -8.8736, -8.7195, -8.6880, -8.1592, -7.7752, -7.7716, -7.7339, -7.7208, -7.6677, -7.1551, -7.0004, -6.8163, -6.7081, -6.5655, -6.4480, -6.3612, -6.0566, -5.6031, -5.5589, -5.2137, -4.3446, -4.3165, -3.8047, -3.5801, -3.4793, -3.4325, -2.3545, -2.3440, -1.8434, -1.7799, -1.5386, -1.0161, -0.8103, 0.4426, 0.5794, 0.9125, 1.1483, 1.4687, 1.4690, 1.5234, 1.6738, 2.0592, 2.1414, 2.8221, 3.1536, 3.6682, 4.2907, 4.8037, 4.8531, 4.9414, 5.3757, 5.3926, 5.6973, 6.0239, 6.1261, 6.5317, 7.2891, 8.4032, 8.4936, 9.2794, 9.9943, 10.0310, 10.4369, 11.7886, 15.8323, 17.4440, 18.9350, 21.0560, 21.0566, 21.6324], grad_fn=&lt;MvBackward&gt;) . &#48169;&#48277;2: torch.nn.Linear() &#49324;&#50857; . net = torch.nn.Linear(in_features=2 ,out_features=1, bias=False) . net.weight.data . tensor([[0.3320, 0.1982]]) . net.weight.data=torch.tensor([[-5.0,10.0]]) . net.weight.data . tensor([[-5., 10.]]) . net(X) . tensor([[-29.8211], [-28.6215], [-24.9730], [-21.2394], [-19.7919], [-19.6354], [-19.5093], [-19.4352], [-18.7223], [-18.0793], [-16.9040], [-16.0918], [-16.0536], [-15.8746], [-14.4690], [-14.3193], [-13.6426], [-12.8578], [-12.5486], [-12.4213], [-11.9484], [-11.1034], [-10.8296], [-10.6210], [-10.5064], [-10.0578], [ -9.8063], [ -9.7380], [ -9.7097], [ -9.6756], [ -8.8736], [ -8.7195], [ -8.6880], [ -8.1592], [ -7.7752], [ -7.7716], [ -7.7339], [ -7.7208], [ -7.6677], [ -7.1551], [ -7.0004], [ -6.8163], [ -6.7081], [ -6.5655], [ -6.4480], [ -6.3612], [ -6.0566], [ -5.6031], [ -5.5589], [ -5.2137], [ -4.3446], [ -4.3165], [ -3.8047], [ -3.5801], [ -3.4793], [ -3.4325], [ -2.3545], [ -2.3440], [ -1.8434], [ -1.7799], [ -1.5386], [ -1.0161], [ -0.8103], [ 0.4426], [ 0.5794], [ 0.9125], [ 1.1483], [ 1.4687], [ 1.4690], [ 1.5234], [ 1.6738], [ 2.0592], [ 2.1414], [ 2.8221], [ 3.1536], [ 3.6682], [ 4.2907], [ 4.8037], [ 4.8531], [ 4.9414], [ 5.3757], [ 5.3926], [ 5.6973], [ 6.0239], [ 6.1261], [ 6.5317], [ 7.2891], [ 8.4032], [ 8.4936], [ 9.2794], [ 9.9943], [ 10.0310], [ 10.4369], [ 11.7886], [ 15.8323], [ 17.4440], [ 18.9350], [ 21.0560], [ 21.0566], [ 21.6324]], grad_fn=&lt;MmBackward&gt;) . yhat2=net(X) . &#48169;&#48277;3: torch.nn.Linear()&#49324;&#50857;, bias=True . net = torch.nn.Linear(in_features=1 ,out_features=1, bias=True) . net.weight.data . tensor([[0.3480]]) . net.weight.data=torch.tensor([[10.0]]) . net.bias.data=torch.tensor([-5.0]) . net.weight,net.bias . (Parameter containing: tensor([[10.]], requires_grad=True), Parameter containing: tensor([-5.], requires_grad=True)) . net(x.reshape(100,1)) . tensor([[-29.8211], [-28.6215], [-24.9730], [-21.2394], [-19.7919], [-19.6354], [-19.5093], [-19.4352], [-18.7223], [-18.0793], [-16.9040], [-16.0918], [-16.0536], [-15.8746], [-14.4690], [-14.3193], [-13.6426], [-12.8578], [-12.5486], [-12.4213], [-11.9484], [-11.1034], [-10.8296], [-10.6210], [-10.5064], [-10.0578], [ -9.8063], [ -9.7380], [ -9.7097], [ -9.6756], [ -8.8736], [ -8.7195], [ -8.6880], [ -8.1592], [ -7.7752], [ -7.7716], [ -7.7339], [ -7.7208], [ -7.6677], [ -7.1551], [ -7.0004], [ -6.8163], [ -6.7081], [ -6.5655], [ -6.4480], [ -6.3612], [ -6.0566], [ -5.6031], [ -5.5589], [ -5.2137], [ -4.3446], [ -4.3165], [ -3.8047], [ -3.5801], [ -3.4793], [ -3.4325], [ -2.3545], [ -2.3440], [ -1.8434], [ -1.7799], [ -1.5386], [ -1.0161], [ -0.8103], [ 0.4426], [ 0.5794], [ 0.9125], [ 1.1483], [ 1.4687], [ 1.4690], [ 1.5234], [ 1.6738], [ 2.0592], [ 2.1414], [ 2.8221], [ 3.1536], [ 3.6682], [ 4.2907], [ 4.8037], [ 4.8531], [ 4.9414], [ 5.3757], [ 5.3926], [ 5.6973], [ 6.0239], [ 6.1261], [ 6.5317], [ 7.2891], [ 8.4032], [ 8.4936], [ 9.2794], [ 9.9943], [ 10.0310], [ 10.4369], [ 11.7886], [ 15.8323], [ 17.4440], [ 18.9350], [ 21.0560], [ 21.0566], [ 21.6324]], grad_fn=&lt;AddmmBackward&gt;) . . step2: loss . &#48169;&#48277;1: &#49552;&#49892;&#54632;&#49688;&#47484; &#51649;&#51217;&#51221;&#51032;&#54616;&#45716; &#48169;&#48277; . loss=torch.mean((y-yhat1)**2) loss . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . loss=torch.mean((y-yhat2)**2) loss . tensor(176.2661, grad_fn=&lt;MeanBackward0&gt;) . 176.2661? 이건 잘못된 결과임 | . loss=torch.mean((y.reshape(100,1)-yhat2)**2) loss . tensor(85.8769, grad_fn=&lt;MeanBackward0&gt;) . &#48169;&#48277;2: torch.nn.MSELoss()&#47484; &#49324;&#50857;&#54616;&#50668; &#49552;&#49892;&#54632;&#49688;&#47484; &#51221;&#51032;&#54616;&#45716; &#48169;&#48277; . lossfn=torch.nn.MSELoss() . loss=lossfn(y,yhat1) loss . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . loss=lossfn(y.reshape(100,1),yhat2) loss . tensor(85.8769, grad_fn=&lt;MseLossBackward&gt;) . &#49689;&#51228; . - model: $y_i= w_0+w_1 x_{i1}+w_2 x_{i2} + epsilon_i = 2.5 + 4x_{1i} + -2x_{2i}+ epsilon_i, quad i=1,2, dots,n$ . torch.manual_seed(43052) n=100 ones= torch.ones(n) x1,_ = torch.randn(n).sort() x2,_ = torch.randn(n).sort() X = torch.vstack([ones,x1,x2]).T W = torch.tensor([2.5,4,-2]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ ytrue = X@W . X . tensor([[ 1.0000, -2.4821, -2.3721], [ 1.0000, -2.3621, -2.3032], [ 1.0000, -1.9973, -2.2271], [ 1.0000, -1.6239, -2.0301], [ 1.0000, -1.4792, -1.9157], [ 1.0000, -1.4635, -1.8241], [ 1.0000, -1.4509, -1.6696], [ 1.0000, -1.4435, -1.6675], [ 1.0000, -1.3722, -1.4723], [ 1.0000, -1.3079, -1.4405], [ 1.0000, -1.1904, -1.4111], [ 1.0000, -1.1092, -1.3820], [ 1.0000, -1.1054, -1.3803], [ 1.0000, -1.0875, -1.3456], [ 1.0000, -0.9469, -1.3255], [ 1.0000, -0.9319, -1.2860], [ 1.0000, -0.8643, -1.2504], [ 1.0000, -0.7858, -1.2095], [ 1.0000, -0.7549, -1.1498], [ 1.0000, -0.7421, -1.1151], [ 1.0000, -0.6948, -1.0980], [ 1.0000, -0.6103, -1.0609], [ 1.0000, -0.5830, -0.9825], [ 1.0000, -0.5621, -0.9672], [ 1.0000, -0.5506, -0.9396], [ 1.0000, -0.5058, -0.9208], [ 1.0000, -0.4806, -0.8768], [ 1.0000, -0.4738, -0.7517], [ 1.0000, -0.4710, -0.7091], [ 1.0000, -0.4676, -0.7027], [ 1.0000, -0.3874, -0.6918], [ 1.0000, -0.3719, -0.6561], [ 1.0000, -0.3688, -0.6153], [ 1.0000, -0.3159, -0.5360], [ 1.0000, -0.2775, -0.4784], [ 1.0000, -0.2772, -0.3936], [ 1.0000, -0.2734, -0.3763], [ 1.0000, -0.2721, -0.3283], [ 1.0000, -0.2668, -0.3227], [ 1.0000, -0.2155, -0.2860], [ 1.0000, -0.2000, -0.2842], [ 1.0000, -0.1816, -0.2790], [ 1.0000, -0.1708, -0.2472], [ 1.0000, -0.1565, -0.2199], [ 1.0000, -0.1448, -0.2170], [ 1.0000, -0.1361, -0.1952], [ 1.0000, -0.1057, -0.1886], [ 1.0000, -0.0603, -0.1829], [ 1.0000, -0.0559, -0.1447], [ 1.0000, -0.0214, -0.0723], [ 1.0000, 0.0655, -0.0667], [ 1.0000, 0.0684, -0.0625], [ 1.0000, 0.1195, -0.0539], [ 1.0000, 0.1420, -0.0356], [ 1.0000, 0.1521, 0.0306], [ 1.0000, 0.1568, 0.0783], [ 1.0000, 0.2646, 0.1328], [ 1.0000, 0.2656, 0.1925], [ 1.0000, 0.3157, 0.2454], [ 1.0000, 0.3220, 0.2519], [ 1.0000, 0.3461, 0.3517], [ 1.0000, 0.3984, 0.3816], [ 1.0000, 0.4190, 0.3831], [ 1.0000, 0.5443, 0.3850], [ 1.0000, 0.5579, 0.4247], [ 1.0000, 0.5913, 0.4431], [ 1.0000, 0.6148, 0.4589], [ 1.0000, 0.6469, 0.4709], [ 1.0000, 0.6469, 0.4711], [ 1.0000, 0.6523, 0.4944], [ 1.0000, 0.6674, 0.4969], [ 1.0000, 0.7059, 0.5234], [ 1.0000, 0.7141, 0.5614], [ 1.0000, 0.7822, 0.5874], [ 1.0000, 0.8154, 0.5899], [ 1.0000, 0.8668, 0.6259], [ 1.0000, 0.9291, 0.6296], [ 1.0000, 0.9804, 0.7098], [ 1.0000, 0.9853, 0.7154], [ 1.0000, 0.9941, 0.7437], [ 1.0000, 1.0376, 0.7786], [ 1.0000, 1.0393, 0.8346], [ 1.0000, 1.0697, 0.8432], [ 1.0000, 1.1024, 0.8558], [ 1.0000, 1.1126, 0.8803], [ 1.0000, 1.1532, 0.9951], [ 1.0000, 1.2289, 1.0430], [ 1.0000, 1.3403, 1.0580], [ 1.0000, 1.3494, 1.0685], [ 1.0000, 1.4279, 1.1723], [ 1.0000, 1.4994, 1.2669], [ 1.0000, 1.5031, 1.3621], [ 1.0000, 1.5437, 1.3738], [ 1.0000, 1.6789, 1.4183], [ 1.0000, 2.0832, 1.4193], [ 1.0000, 2.2444, 1.5095], [ 1.0000, 2.3935, 1.6424], [ 1.0000, 2.6056, 1.8131], [ 1.0000, 2.6057, 2.0058], [ 1.0000, 2.6632, 2.2810]]) . - torch.nn.Linear() 를 이용하여 $ bf{ hat{W}}= begin{bmatrix}1 1 1 end{bmatrix}$ 에 대한 $ hat{y}$를 구하라. .",
            "url": "https://guebin.github.io/2021BDA/2021/09/28/(3%EC%A3%BC%EC%B0%A8)-9%EC%9B%9428%EC%9D%BC.html",
            "relUrl": "/2021/09/28/(3%EC%A3%BC%EC%B0%A8)-9%EC%9B%9428%EC%9D%BC.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "(2주차) 9월14일, 9월16일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/5) 회귀모형 소개, 손실 함수 . - (2/5) 경사하강법, 경사하강법을 이용하여 회귀계수 1회 업데이트 . - (3/5) 회귀계수 반복 업데이트 . - (4/5) 학습률 . - (5/5) 사과영상 . import . import torch import numpy as np import matplotlib.pyplot as plt . &#47196;&#46300;&#47605; . - 회귀분석 $ to$ 로지스틱 $ to$ 심층신경망(DNN) $ to$ 합성곱신경망(CNN) . Data . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(43052) n=100 ones= torch.ones(n) x,_ = torch.randn(n).sort() X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ ytrue = X@W . plt.plot(x,y,&#39;o&#39;) plt.plot(x,ytrue,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f259112c0d0&gt;] . &#54617;&#49845;&#51060;&#46976;? . - 파란점만 주어졌을때, 주황색 점선을 추론하는것. 좀 더 정확하게 말하면 given data로 $ begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$를 최대한 $ begin{bmatrix} 2.5 4 end{bmatrix}$와 비슷하게 찾는것. . given data : $ big {(x_i,y_i) big }_{i=1}^{n}$ . | parameter: ${ bf W}= begin{bmatrix} w_0 w_1 end{bmatrix}$ . | estimated parameter: ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$ . | . - 더 쉽게 말하면 아래의 그림을 보고 적당한 추세선을 찾는것이다. . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f2591013f40&gt;] . - 시도: $( hat{w}_0, hat{w}_1)=(-5,10)$을 선택하여 선을 그려보고 적당한지 판단. . $ hat{y}_i=-5 +10 x_i$ 와 같이 $y_i$의 값을 적합시키겠다는 의미 | . plt.plot(x,y,&#39;o&#39;) plt.plot(x,-5+10*x,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f2590ff7cd0&gt;] . - 벡터표현으로 주황색점선을 계산 . What=torch.tensor([-5.0,10.0]) plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@What,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f2590f69730&gt;] . &#54028;&#46972;&#47700;&#53552;&#47484; &#54617;&#49845;&#54616;&#45716; &#48169;&#48277; (&#51201;&#45817;&#54620; &#49440;&#51004;&#47196; &#50629;&#45936;&#51060;&#53944; &#54616;&#45716; &#48169;&#48277;) . - 이론적으로 추론 &lt;- 회귀분석시간에 배운것 . - 컴퓨터의 반복계산을 이용하여 추론 (경사하강법) &lt;- 우리가 오늘 파이토치로 실습해볼 내용. . (1) initial value: 임의의 선을 일단 그어본다. . What= torch.tensor([-5.0,10.0],requires_grad=True) What . tensor([-5., 10.], requires_grad=True) . 처음에는 ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}= begin{bmatrix} -5 10 end{bmatrix} $ 를 대입해서 주황색 점선을 적당히 그려보자는 의미 . | 끝에 requires_grad=True는 나중에 미분을 위한 것 . | . yhat=X@What yhat . tensor([-29.8211, -28.6215, -24.9730, -21.2394, -19.7919, -19.6354, -19.5093, -19.4352, -18.7223, -18.0793, -16.9040, -16.0918, -16.0536, -15.8746, -14.4690, -14.3193, -13.6426, -12.8578, -12.5486, -12.4213, -11.9484, -11.1034, -10.8296, -10.6210, -10.5064, -10.0578, -9.8063, -9.7380, -9.7097, -9.6756, -8.8736, -8.7195, -8.6880, -8.1592, -7.7752, -7.7716, -7.7339, -7.7208, -7.6677, -7.1551, -7.0004, -6.8163, -6.7081, -6.5655, -6.4480, -6.3612, -6.0566, -5.6031, -5.5589, -5.2137, -4.3446, -4.3165, -3.8047, -3.5801, -3.4793, -3.4325, -2.3545, -2.3440, -1.8434, -1.7799, -1.5386, -1.0161, -0.8103, 0.4426, 0.5794, 0.9125, 1.1483, 1.4687, 1.4690, 1.5234, 1.6738, 2.0592, 2.1414, 2.8221, 3.1536, 3.6682, 4.2907, 4.8037, 4.8531, 4.9414, 5.3757, 5.3926, 5.6973, 6.0239, 6.1261, 6.5317, 7.2891, 8.4032, 8.4936, 9.2794, 9.9943, 10.0310, 10.4369, 11.7886, 15.8323, 17.4440, 18.9350, 21.0560, 21.0566, 21.6324], grad_fn=&lt;MvBackward&gt;) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f2590ecd910&gt;] . (2) 첫번째 수정: 적당한 선의 &#39;적당한 정도&#39;를 판단하고 더 적당한 선으로 업데이트 한다. . - &#39;적당한 정도&#39;를 판단하기 위한 장치: loss function 도입! . $loss= sum_{i=1}^{n}(y_i- hat{y}_i)^2= sum_{i=1}^{n}(y_i-( hat{w}_0+ hat{w}_1x_i))^2$ . $=({ bf y}-{ bf hat{y}})^ top({ bf y}-{ bf hat{y}})=({ bf y}-{ bf X}{ bf hat{W}})^ top({ bf y}-{ bf X}{ bf hat{W}})$ . - loss 함수의 특징 . $y_i approx hat{y}_i$ 일수록 loss값이 작다. | $y_i approx hat{y}_i$ 이 되도록 $( hat{w}_0, hat{w}_1)$을 잘 찍으면 loss값이 작다. | (중요) 주황색 점선이 &#39;적당할 수록&#39; loss값이 작다. | . loss=torch.sum((y-yhat)**2) loss . tensor(8587.6875, grad_fn=&lt;SumBackward0&gt;) . - 우리의 목표: 이 loss(=8587.6875)을 더 줄이자. $ to$ 아예 모든 조합 $( hat{w}_0, hat{w}_1)$에 대하여 가장 작은 loss를 찾으면 좋겠다. . - 문제의 치환: 생각해보니까 우리의 문제는 아래와 같이 수학적으로 단순화 되었다. . 적당해보이는 주황색 선을 찾자 $ to$ $loss(w_0,w_1)$를 최소로하는 $(w_0,w_1)$의 값을 찾자. | . - 수정된 목표: $loss(w_0,w_1)$를 최소로 하는 $(w_0,w_1)$을 구하라. . 단순한 수학문제가 되었다. 마치 $loss(w)=w^2-2w+3$ 을 최소화하는 $w$를 찾으라는 것과 같음. | . - 우리의 무기: 경사하강법, 벡터미분 . . ($ ast$) &#51104;&#49884; &#44221;&#49324;&#54616;&#44053;&#48277;&#51012; &#47532;&#48624;&#54616;&#51088;. . 경사하강법 아이디어 (1차원) . (step 1) 임의의 점을 찍는다. . (step 2) 그 점에서 순간기울기를 구한다. (접선) &lt;-- 미분 . (step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 움직인다. (순간기울기와 같은 방향으로 움직이면 점점 커질테니까) . (팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 조절한다. . 경사하강법 아이디어 (2차원) . - 경사하강법 아이디어 (1차원) . (step 1) 임의의 점을 찍는다. . (step 2) 그 점에서 순간기울기를 구한다. (접평면) &lt;-- 편미분 . (step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 각각 움직인다. (순간기울기와 같은 방향으로 움직이면 점점 커질테니까) . (팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 각각 조절한다. . loss를 줄이도록 ${ bf W}$를 개선하는 방법 . - $수정값 leftarrow 원래값 - 기울어진크기(=미분계수) times alpha $ . 여기에서 $ alpha$는 전체적인 보폭의 크기를 결정한다. 즉 $ alpha$값이 클수록 한번의 update에 움직이는 양이 크다. | . - ${ bf W} leftarrow { bf W} - alpha times frac{ partial}{ partial { bf W}}loss(w_0,w_1)$ . 마이너스의 의미: 기울기의 부호를 보고 반대방향으로 움직여라. . | $ frac{ partial}{ partial { bf W}}loss(w_0,w_1):$ 기울기의 절대값 크기와 비례하여 움직이는 정도를 조정하라. . | $ alpha$의 의미: 전체적인 보폭의 속도를 조절, $ alpha$가 크면 전체적으로 빠르게 움직인다. 다리의 길이로 비유할 수 있다. . | . . - 우리의 목표: loss=8587.6875 인데, 이걸 줄이는 것이 목표라고 했었음. 이것을 줄이는 방법이 경사하강법이다. . - 경사하강법으로 loss를 줄이기 위해서는 $ frac{ partial}{ partial { bf W}}loss(w_0,w_1)$의 계산이 필요한데, 이를 위해서 벡터미분이 필요하다. . loss.backward() . 미분해라! 뭘로? requires_grad=True를 가진 텐서로!!loss=torch.sum((y-yhat)**2)= torch.sum((y-X@What)**2) # 이었고 What=torch.tensor([-5.0,10.0],requires_grad=True) # 이므로 결국 What으로 미분하라는 의미. # 미분한 식이 나오는 것이 아니고, # 그 식에 (-5.0, 10.0)을 대입한 계수값이 계산됨. . | . 정확하게 말하면 미분을 활용하여 $(-5,10)$에서의 순간기울기를 구했다는 의미임. | . What.grad.data . tensor([-1342.2523, 1188.9307]) . 이것이 의미하는건 $(-5,10)$에서의 순간기울기가 $(-1342.2523, 1188.9307)$ 이라는 의미 | . - 잘계산한것이 맞는가? 손계산으로 검증하여 보자. . $loss(w_0,w_1)=(y- hat{y})^ top (y- hat{y})=(y-XW)^ top (y-XW)$ . | $ frac{ partial}{ partial W}loss(w_0,w_1)=-2X^ top y+2X^ top X W$ . | . - 2 * X.T @ y + 2 * X.T @ X @ What . tensor([-1342.2522, 1188.9305], grad_fn=&lt;AddBackward0&gt;) . alpha=0.001 print(&#39;수정전: &#39; + str(What.data)) print(&#39;수정하는폭: &#39; +str(-alpha * What.grad.data)) print(&#39;수정후: &#39; +str(What.data-alpha * What.grad.data)) print(&#39;*참값: (2.5,4)&#39; ) . 수정전: tensor([-5., 10.]) 수정하는폭: tensor([ 1.3423, -1.1889]) 수정후: tensor([-3.6577, 8.8111]) *참값: (2.5,4) . Wbefore = What.data Wafter = What.data-alpha * What.grad.data Wbefore, Wafter . (tensor([-5., 10.]), tensor([-3.6577, 8.8111])) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@Wbefore,&#39;--&#39;,color=&#39;b&#39;) #수정전: 파란점선 plt.plot(x,X@Wafter,&#39;--&#39;,color=&#39;r&#39;) #수정후: 빨간점선 plt.title(&quot;before: blue // after: red&quot;) . Text(0.5, 1.0, &#39;before: blue // after: red&#39;) . (3) Learn (=estimate $ bf hat{W})$: . What= torch.tensor([-5.0,10.0],requires_grad=True) . alpha=0.001 for epoc in range(30): What.grad=None yhat=X@What loss=torch.sum((y-yhat)**2) loss.backward() What.data = What.data-alpha * What.grad.data . What.data ## true: (2.5,4) . tensor([2.4290, 4.0144]) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@What.data),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f25887040a0&gt;] . &#54028;&#46972;&#47700;&#53552;&#51032; &#49688;&#51221;&#44284;&#51221;&#51012; &#44288;&#52272;&#54624; &#49688; &#50630;&#51012;&#44620;? (&#54617;&#49845;&#44284;&#51221; &#47784;&#45768;&#53552;&#47553;) . - 기록을 해보자. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . What= torch.tensor([-5.0,10.0],requires_grad=True) alpha=0.001 for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . - $ hat{y}$ 관찰 . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[3],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f258867d6d0&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[10],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f25885eb490&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[15],&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f25885ce730&gt;] . - $ hat{ bf W}$ . Whats . [[-5.0, 10.0], [-3.657747745513916, 8.81106948852539], [-2.554811716079712, 7.861191749572754], [-1.649186372756958, 7.101552963256836], [-0.9060714244842529, 6.49347448348999], [-0.29667866230010986, 6.006272315979004], [0.2027742564678192, 5.615575313568115], [0.6119104623794556, 5.302003383636475], [0.9469034671783447, 5.050129413604736], [1.2210699319839478, 4.847657680511475], [1.4453645944595337, 4.684779167175293], [1.6287915706634521, 4.553659439086914], [1.778746247291565, 4.448036193847656], [1.90129816532135, 4.3628973960876465], [2.0014259815216064, 4.294229507446289], [2.0832109451293945, 4.238814353942871], [2.149996757507324, 4.194070339202881], [2.204521894454956, 4.157923698425293], [2.249027729034424, 4.128708839416504], [2.285348415374756, 4.105085849761963], [2.31498384475708, 4.0859761238098145], [2.339160442352295, 4.070511341094971], [2.3588807582855225, 4.057991027832031], [2.3749637603759766, 4.0478515625], [2.3880786895751953, 4.039637088775635], [2.3987717628479004, 4.032979965209961], [2.40748929977417, 4.027583599090576], [2.414595603942871, 4.023208141326904], [2.4203879833221436, 4.019659042358398], [2.4251089096069336, 4.016779899597168]] . plt.plot(losses) . [&lt;matplotlib.lines.Line2D at 0x7f25885296d0&gt;] . Animation . plt.rcParams[&#39;figure.figsize&#39;] = (10,4) plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . from matplotlib import animation fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect $ alpha$&#50640; &#45824;&#54616;&#50668; ($ alpha$&#45716; &#54617;&#49845;&#47456;) . (1) $ alpha$가 너무 작다면? $ to$ 비효율적이다. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.0001 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (2) $ alpha$가 크다면? $ to$ 다른의미에서 비효율적이다 + 위험하다.. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.0083 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (3) $ alpha=0.0085$ . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.0085 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (4) $ alpha=0.01$ . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.01 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#49689;&#51228; . - 학습률($ alpha$)를 조정하며 실습해보고 스크린샷 제출 . &#45796;&#47336;&#44592; &#49899;&#51648;&#47564; &#54644;&#50556;&#54616;&#45716; &#49324;&#49548;&#54620; &#47928;&#51228;&#46308; . (A1) &#49552;&#49892;&#54632;&#49688; . - $ sum_{i=1}^{n}(y_i- hat{y}_i)^2$ 대신에 . $ frac{1}{n} sum_{i=1}^{n}(y_i- hat{y}_i)^2$ | $ frac{1}{2n} sum_{i=1}^{n}(y_i- hat{y}_i)^2$ | . 중 하나를 사용하여도 상관없다. . (A2) &#48324;&#54364;&#47196; &#54364;&#49884;&#46108; &#51216;&#51060; &#51221;&#47568; $(2.5,4.0)$&#51068;&#44620;? $ Longleftrightarrow$ l&#51060; &#51221;&#47568; $w_0=2.5$, $w_1=4.0$&#50640;&#49436; &#52572;&#49548;&#54868; &#46104;&#45716;&#44032;? . - np.argmin 소개 . _a=np.array([0,2,5,2,3,4]) np.argmin(_a) . 0 . np.argmin(l) . 598 . 이건 무슨 값이지?? . - 왜 이런일이 생기는가? . _X=np.array([[1,6,3],[1,-5,5]]) . _X . array([[ 1, 6, 3], [ 1, -5, 5]]) . np.argmin(_X) . 4 . - array의 구조가 너무 컴퓨터 위주의 숫자임.. $ to$ np.unravel_index() 함수사용 . np.unravel_index(4,_X.shape) . (1, 1) . - 이것을 응용하면 . np.unravel_index(np.argmin(l),l.shape) . (17, 20) . _w0[17],_w1[20] . (2.5, 4.0) . - (2.5,4.0)에서 l이 최소값을 가지는 것이 맞긴함 . - 그런데 이론적으로 그래야 하는 것은 아님. . torch.sum((y-2.5-4.0*x)**2) . tensor(26.6494) . XX=np.matrix(X) yy=np.matrix(y).T . (XX.T*XX).I * XX.T * yy . matrix([[2.4458692], [4.004343 ]], dtype=float32) . torch.sum((y-2.4458692-4.004343*x)**2) . tensor(26.3600) . 진짜로 (2.4458692,4.004343) 에서의 로스가 더 작음 | . - $n$이 커질수록 (2.4458692, 4.004343) 의 값은 점점 (2.5,4.0)의 값에 가까워 진다. . (A3) &#54665;&#48289;&#53552;&#50752; &#50676;&#48289;&#53552; . - 아래의 매트릭스를 관찰하자. . XX . matrix([[ 1. , -2.482113 ], [ 1. , -2.3621461 ], [ 1. , -1.9972954 ], [ 1. , -1.6239362 ], [ 1. , -1.4791915 ], [ 1. , -1.4635365 ], [ 1. , -1.450925 ], [ 1. , -1.4435216 ], [ 1. , -1.3722302 ], [ 1. , -1.3079282 ], [ 1. , -1.1903973 ], [ 1. , -1.109179 ], [ 1. , -1.1053556 ], [ 1. , -1.0874591 ], [ 1. , -0.94689655], [ 1. , -0.9319339 ], [ 1. , -0.8642649 ], [ 1. , -0.78577816], [ 1. , -0.7548619 ], [ 1. , -0.74213064], [ 1. , -0.6948388 ], [ 1. , -0.610345 ], [ 1. , -0.5829591 ], [ 1. , -0.56210476], [ 1. , -0.55064297], [ 1. , -0.50577736], [ 1. , -0.48062643], [ 1. , -0.4737953 ], [ 1. , -0.47096547], [ 1. , -0.46755713], [ 1. , -0.3873588 ], [ 1. , -0.37194738], [ 1. , -0.3687963 ], [ 1. , -0.31592152], [ 1. , -0.27751535], [ 1. , -0.27715707], [ 1. , -0.27338728], [ 1. , -0.27207515], [ 1. , -0.2667671 ], [ 1. , -0.21550845], [ 1. , -0.20004053], [ 1. , -0.18163072], [ 1. , -0.17081414], [ 1. , -0.1565458 ], [ 1. , -0.14479806], [ 1. , -0.13611706], [ 1. , -0.10566129], [ 1. , -0.06031348], [ 1. , -0.05588722], [ 1. , -0.02136729], [ 1. , 0.06554431], [ 1. , 0.06835173], [ 1. , 0.11953046], [ 1. , 0.14198998], [ 1. , 0.15207446], [ 1. , 0.15675156], [ 1. , 0.26455274], [ 1. , 0.26559785], [ 1. , 0.3156574 ], [ 1. , 0.32201108], [ 1. , 0.346143 ], [ 1. , 0.39839193], [ 1. , 0.4189721 ], [ 1. , 0.5442578 ], [ 1. , 0.557936 ], [ 1. , 0.591254 ], [ 1. , 0.61482644], [ 1. , 0.64686656], [ 1. , 0.64689904], [ 1. , 0.6523392 ], [ 1. , 0.6673753 ], [ 1. , 0.7059195 ], [ 1. , 0.7141374 ], [ 1. , 0.78221494], [ 1. , 0.8153611 ], [ 1. , 0.8668233 ], [ 1. , 0.9290748 ], [ 1. , 0.98036987], [ 1. , 0.9853081 ], [ 1. , 0.99413556], [ 1. , 1.0375688 ], [ 1. , 1.039256 ], [ 1. , 1.0697267 ], [ 1. , 1.1023871 ], [ 1. , 1.112612 ], [ 1. , 1.1531745 ], [ 1. , 1.2289088 ], [ 1. , 1.3403202 ], [ 1. , 1.3493598 ], [ 1. , 1.4279404 ], [ 1. , 1.4994265 ], [ 1. , 1.503098 ], [ 1. , 1.5436871 ], [ 1. , 1.6788615 ], [ 1. , 2.083233 ], [ 1. , 2.2444 ], [ 1. , 2.393501 ], [ 1. , 2.6056044 ], [ 1. , 2.605658 ], [ 1. , 2.66324 ]], dtype=float32) . - 두번째 col을 선택하고 싶다. . XX[:,1] . matrix([[-2.482113 ], [-2.3621461 ], [-1.9972954 ], [-1.6239362 ], [-1.4791915 ], [-1.4635365 ], [-1.450925 ], [-1.4435216 ], [-1.3722302 ], [-1.3079282 ], [-1.1903973 ], [-1.109179 ], [-1.1053556 ], [-1.0874591 ], [-0.94689655], [-0.9319339 ], [-0.8642649 ], [-0.78577816], [-0.7548619 ], [-0.74213064], [-0.6948388 ], [-0.610345 ], [-0.5829591 ], [-0.56210476], [-0.55064297], [-0.50577736], [-0.48062643], [-0.4737953 ], [-0.47096547], [-0.46755713], [-0.3873588 ], [-0.37194738], [-0.3687963 ], [-0.31592152], [-0.27751535], [-0.27715707], [-0.27338728], [-0.27207515], [-0.2667671 ], [-0.21550845], [-0.20004053], [-0.18163072], [-0.17081414], [-0.1565458 ], [-0.14479806], [-0.13611706], [-0.10566129], [-0.06031348], [-0.05588722], [-0.02136729], [ 0.06554431], [ 0.06835173], [ 0.11953046], [ 0.14198998], [ 0.15207446], [ 0.15675156], [ 0.26455274], [ 0.26559785], [ 0.3156574 ], [ 0.32201108], [ 0.346143 ], [ 0.39839193], [ 0.4189721 ], [ 0.5442578 ], [ 0.557936 ], [ 0.591254 ], [ 0.61482644], [ 0.64686656], [ 0.64689904], [ 0.6523392 ], [ 0.6673753 ], [ 0.7059195 ], [ 0.7141374 ], [ 0.78221494], [ 0.8153611 ], [ 0.8668233 ], [ 0.9290748 ], [ 0.98036987], [ 0.9853081 ], [ 0.99413556], [ 1.0375688 ], [ 1.039256 ], [ 1.0697267 ], [ 1.1023871 ], [ 1.112612 ], [ 1.1531745 ], [ 1.2289088 ], [ 1.3403202 ], [ 1.3493598 ], [ 1.4279404 ], [ 1.4994265 ], [ 1.503098 ], [ 1.5436871 ], [ 1.6788615 ], [ 2.083233 ], [ 2.2444 ], [ 2.393501 ], [ 2.6056044 ], [ 2.605658 ], [ 2.66324 ]], dtype=float32) . 정상적을 잘 선택되었다. | . - 이제 XX에서 첫번째 row를 선택하고 싶다면? . XX[0,:] . matrix([[ 1. , -2.482113]], dtype=float32) . - X에 관심을 가져보자. . - 첫번째 row를 뽑고싶다면? . X[0,:] . tensor([ 1.0000, -2.4821]) . - 두번째 col을 뽑고 싶다면? . X[:,1] . tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435, -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319, -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621, -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719, -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155, -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603, -0.0559, -0.0214, 0.0655, 0.0684, 0.1195, 0.1420, 0.1521, 0.1568, 0.2646, 0.2656, 0.3157, 0.3220, 0.3461, 0.3984, 0.4190, 0.5443, 0.5579, 0.5913, 0.6148, 0.6469, 0.6469, 0.6523, 0.6674, 0.7059, 0.7141, 0.7822, 0.8154, 0.8668, 0.9291, 0.9804, 0.9853, 0.9941, 1.0376, 1.0393, 1.0697, 1.1024, 1.1126, 1.1532, 1.2289, 1.3403, 1.3494, 1.4279, 1.4994, 1.5031, 1.5437, 1.6789, 2.0832, 2.2444, 2.3935, 2.6056, 2.6057, 2.6632]) . - shape을 비교하여 보자. . XX.shape, (XX[0,:]).shape, (XX[:,1]).shape . ((100, 2), (1, 2), (100, 1)) . 이게 상식적임 | . X.shape, (X[0,:]).shape, (X[:,1]).shape . (torch.Size([100, 2]), torch.Size([2]), torch.Size([100])) . row-vec, col-vec의 구분없이 그냥 길이2인 벡터, 길이가 100인 벡터로 고려됨 | row-vec, col-vec의 구분을 하려면 2차원이 필요한데 1차원으로 축소가 되면서 생기는 현상 | 대부분의 경우 별로 문제가 되지 않음. | 수학적으로는 col-vec, row-vec를 엄밀하게 구분하는 것이 좋지만, 프로그래밍 효율을 생각하면 떄로는 구분이 모호한게 유리할 수도 있다. | .",
            "url": "https://guebin.github.io/2021BDA/2021/09/16/(2-3%EC%A3%BC%EC%B0%A8)-9%EC%9B%9414,-9%EC%9B%9416%EC%9D%BC.html",
            "relUrl": "/2021/09/16/(2-3%EC%A3%BC%EC%B0%A8)-9%EC%9B%9414,-9%EC%9B%9416%EC%9D%BC.html",
            "date": " • Sep 16, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "(2주차) 9월9일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/4) Path 설명 . - (2/4) 이미지 크롤링 . - (3/4) 모형학습 및 결과분석 . - (4/4) 테스트 . import . from fastai.data.all import * from fastai.vision.all import * . Path . - 기능: 현재폴더, 혹은 그 하위폴더들에 속한 파일의 목록을 볼 수 있다. . path=Path() # Path클래스에서 인스턴스생성 . (path/&#39;ghtop_images&#39;).ls() . (#2) [Path(&#39;ghtop_images/token.png&#39;),Path(&#39;ghtop_images/sparknb.gif&#39;)] . - Path(...)에서 ...에 무엇을 넣느냐에 따라 원하는 경로를 설정할 수 있다. . path=Path(&#39;/home&#39;) . path.ls() . (#1) [Path(&#39;/home/cgb4&#39;)] . - 폴더를 만들수 있다. . path=Path() . (path/&#39;asdf&#39;).mkdir() . (path/&#39;asdf&#39;).ls() . (#0) [] . - 이미 폴더가 존재할 때는 아래와 같이 에러가 발생 . (path/&#39;asdf&#39;).mkdir() . FileExistsError Traceback (most recent call last) /tmp/ipykernel_258436/283275367.py in &lt;module&gt; -&gt; 1 (path/&#39;asdf&#39;).mkdir() ~/anaconda3/envs/bda2021/lib/python3.8/pathlib.py in mkdir(self, mode, parents, exist_ok) 1286 self._raise_closed() 1287 try: -&gt; 1288 self._accessor.mkdir(self, mode) 1289 except FileNotFoundError: 1290 if not parents or self.parent == self: FileExistsError: [Errno 17] File exists: &#39;asdf&#39; . (path/&#39;asdf&#39;).mkdir(exist_ok=True) . - 생성한 폴더를 지우는 방법 . (path/&#39;asdf&#39;).rmdir() . &#51060;&#48120;&#51648; &#53356;&#47204;&#47553; . - 이미지 크롤링은 (1) 검색 (2) 이미지 주소를 찾음 (3) 해당주소로 이동하여 저장하는 과정을 반복하면 된다. . - 교재: 빙을 이용하여 이미지 크롤링 . 단점: 애져에 가입, 완전무료가 아님 (학생에게 1년간 무료) | . - 다른방법: 덕덕고를 이용한 이미지 크롤링 . ref: https://github.com/fastai/fastbook/blob/master/utils.py | . def search_images_ddg(key,max_n=200): &quot;&quot;&quot;Search for &#39;key&#39; with DuckDuckGo and return a unique urls of &#39;max_n&#39; images (Adopted from https://github.com/deepanprabhu/duckduckgo-images-api) &quot;&quot;&quot; url = &#39;https://duckduckgo.com/&#39; params = {&#39;q&#39;:key} res = requests.post(url,data=params) searchObj = re.search(r&#39;vqd=([ d-]+) &amp;&#39;,res.text) if not searchObj: print(&#39;Token Parsing Failed !&#39;); return requestUrl = url + &#39;i.js&#39; headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0&#39;} params = ((&#39;l&#39;,&#39;us-en&#39;),(&#39;o&#39;,&#39;json&#39;),(&#39;q&#39;,key),(&#39;vqd&#39;,searchObj.group(1)),(&#39;f&#39;,&#39;,,,&#39;),(&#39;p&#39;,&#39;1&#39;),(&#39;v7exp&#39;,&#39;a&#39;)) urls = [] while True: try: res = requests.get(requestUrl,headers=headers,params=params) data = json.loads(res.text) for obj in data[&#39;results&#39;]: urls.append(obj[&#39;image&#39;]) max_n = max_n - 1 if max_n &lt; 1: return L(set(urls)) # dedupe if &#39;next&#39; not in data: return L(set(urls)) requestUrl = url + data[&#39;next&#39;] except: pass . - search_images_ddg(검색어)를 이용하여 검색어에 해당하는 url을 얻는다. . search_images_ddg(&#39;hynn&#39;,max_n=5) . (#5) [&#39;https://yt3.ggpht.com/a/AGF-l7_1jF579BUaWHBEpY95iZAb0WI2SC4vykeo3A=s900-c-k-c0xffffffff-no-rj-mo&#39;,&#39;http://talkimg.imbc.com/TVianUpload/tvian/TViews/image/2020/03/21/GRMTjLNM9a88637203974033409433.jpg&#39;,&#39;https://images.genius.com/a37e8f087886e8a9f1f1d4d4d02aba44.960x960x1.jpg&#39;,&#39;https://www.nautiljon.com/images/people/01/59/hynn_99095.jpg?0&#39;,&#39;https://lastfm.freetls.fastly.net/i/u/770x0/f6744fc617da497938bf0560c82fe0d2.jpg#f6744fc617da497938bf0560c82fe0d2&#39;] . - download_images(저장하고싶은폴더위치, url의리스트)를 이용하여 url에 해당하는 이미지를 저장하고 싶은 폴더에 저장. . path=Path() . path.ls() . (#14) [Path(&#39;2021-09-06-cat2.jpeg&#39;),Path(&#39;2021-09-06-hani03.jpg&#39;),Path(&#39;2021-09-06-hani01.jpeg&#39;),Path(&#39;ghtop_images&#39;),Path(&#39;2021-09-07-(1주차) 9월7일.ipynb&#39;),Path(&#39;Untitled.ipynb&#39;),Path(&#39;2021-08-17-(A1) 깃허브와 fastpages를 이용하여 블로그 개설하기.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2021-09-02-(1주차) 9월2일.ipynb&#39;),Path(&#39;2021-09-06-cat1.png&#39;)...] . download_images(path,urls=search_images_ddg(&#39;hynn&#39;,max_n=5)) . 현재 working dir에 5개의 이미지가 저장된다. | . keywords = &#39;hynn&#39;, &#39;iu&#39; path=Path(&#39;singer&#39;) . if not path.exists(): # 현재폴더에 singer라는 폴더가 있는지 체크 path.mkdir() # 현재폴더에 singer라는 폴더가 만들어짐 for keyword in keywords: # keyword=&#39;hynn&#39;, keyword=&#39;iu&#39; 일때 아래내용을 반복 lastpath=path/keyword # ./singer/hynn or ./singer/iu lastpath.mkdir(exist_ok=True) # make ./singer/hynn or ./singer/iu urls=search_images_ddg(keyword) # &#39;hynn&#39; 검색어로 url들의 리스트를 얻음 download_images(lastpath,urls=urls) # 그 url에 해당하는 이미지들을 ./singer/hynn or ./singer/iu 에 저장 . Cleaning Data . - 탐색기로 파일들을 살펴보니 조금 이상한 확장자도 있음. . - 조금 이상해보이는 확장자도 열리기는 함. . PILImage.create(&#39;./singer/iu/00000006.jpg:large&#39;) . verify_images(get_image_files(path)) . (#4) [Path(&#39;singer/iu/00000041.jpg&#39;),Path(&#39;singer/iu/00000029.jpg&#39;),Path(&#39;singer/iu/00000125.jpg&#39;),Path(&#39;singer/hynn/00000077.png&#39;)] . - 위에 해당하는 이미지를 수동으로 지워줌. . - csv을 받았으면 df를 만들어야 하듯이, 이미지 파일들을 받았으면 dls를 만들어야 fastai가 지원하는 함수로 분석하기 좋다. . dls = ImageDataLoaders.from_folder( path, train=&#39;singer&#39;, valid_pct=0.2, item_tfms=Resize(224)) . dls.show_batch(max_n=16) . - 모형을 만들고 학습을 시키자. . learn=cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(7) . epoch train_loss valid_loss error_rate time . 0 | 1.069038 | 0.753938 | 0.264706 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.638990 | 0.531955 | 0.220588 | 00:04 | . 1 | 0.498534 | 0.338006 | 0.147059 | 00:04 | . 2 | 0.392531 | 0.268666 | 0.132353 | 00:04 | . 3 | 0.313377 | 0.214198 | 0.102941 | 00:04 | . 4 | 0.262075 | 0.227022 | 0.088235 | 00:04 | . 5 | 0.216234 | 0.228273 | 0.088235 | 00:04 | . 6 | 0.192656 | 0.218852 | 0.088235 | 00:04 | . learn.show_results(max_n=16) . &#50724;&#45813;&#48516;&#49437; . interp = Interpretation.from_learner(learn) interp.plot_top_losses(16) . - 수동으로 특정 observation에 대한 예측결과를 확인하여 보자. . dls.train_ds . (#272) [(PILImage mode=RGB size=960x960, TensorCategory(1)),(PILImage mode=RGB size=540x793, TensorCategory(1)),(PILImage mode=RGB size=800x1200, TensorCategory(1)),(PILImage mode=RGB size=720x960, TensorCategory(1)),(PILImage mode=RGB size=500x500, TensorCategory(0)),(PILImage mode=RGB size=1418x2000, TensorCategory(1)),(PILImage mode=RGB size=1920x1280, TensorCategory(1)),(PILImage mode=RGB size=480x360, TensorCategory(0)),(PILImage mode=RGB size=630x1045, TensorCategory(0)),(PILImage mode=RGB size=799x1200, TensorCategory(1))...] . training set | . dls.train_ds[0] . (PILImage mode=RGB size=960x960, TensorCategory(1)) . dls.train_ds[0] 가 의미하는 것은 첫번쨰 observation을 의미함. 즉 $(x_1,y_1)$ | $x_1=$PILImage mode=RGB size=960x960 | $y_1=$TensorCategory(1) | . dls.train_ds[210][0] . $x_{211}$=위의 이미지 | . dls.train_ds[210][1] . TensorCategory(0) . $y_{211}=$TensorCategory(0) | . x210=dls.train_ds[210][0] . learn.predict(x210) . (&#39;hynn&#39;, tensor(0), tensor([0.8893, 0.1107])) . Test . path = Path() . if not (path/&#39;test&#39;).exists(): (path/&#39;test&#39;).mkdir() . urls=search_images_ddg(&#39;hynn 박혜원&#39;,max_n=20) download_images(path/&#39;test&#39;,urls=urls) testset=get_image_files(path/&#39;test&#39;) testset . (#20) [Path(&#39;test/00000010.jpg&#39;),Path(&#39;test/00000005.jpg&#39;),Path(&#39;test/00000013.jpg&#39;),Path(&#39;test/00000011.jpg&#39;),Path(&#39;test/00000003.jpg&#39;),Path(&#39;test/00000000.jpg&#39;),Path(&#39;test/00000015.png&#39;),Path(&#39;test/00000004.jpg&#39;),Path(&#39;test/00000012.jpg&#39;),Path(&#39;test/00000006.jpg&#39;)...] . for i in range(len(testset)): print(learn.predict(PILImage.create(testset[i]))) . (&#39;hynn&#39;, tensor(0), tensor([1.0000e+00, 1.5190e-06])) . (&#39;hynn&#39;, tensor(0), tensor([0.9516, 0.0484])) . (&#39;hynn&#39;, tensor(0), tensor([0.9904, 0.0096])) . (&#39;hynn&#39;, tensor(0), tensor([9.9952e-01, 4.7845e-04])) . (&#39;hynn&#39;, tensor(0), tensor([0.9990, 0.0010])) . (&#39;hynn&#39;, tensor(0), tensor([0.9983, 0.0017])) . (&#39;hynn&#39;, tensor(0), tensor([0.9923, 0.0077])) . (&#39;iu&#39;, tensor(1), tensor([0.1120, 0.8880])) . (&#39;hynn&#39;, tensor(0), tensor([0.9949, 0.0051])) . (&#39;hynn&#39;, tensor(0), tensor([0.9982, 0.0018])) . (&#39;hynn&#39;, tensor(0), tensor([0.9940, 0.0060])) . (&#39;hynn&#39;, tensor(0), tensor([1.0000e+00, 8.8760e-07])) . (&#39;hynn&#39;, tensor(0), tensor([0.9963, 0.0037])) . (&#39;hynn&#39;, tensor(0), tensor([9.9975e-01, 2.5230e-04])) . (&#39;hynn&#39;, tensor(0), tensor([0.7672, 0.2328])) . (&#39;hynn&#39;, tensor(0), tensor([9.9982e-01, 1.8401e-04])) . (&#39;hynn&#39;, tensor(0), tensor([1.0000e+00, 3.9835e-06])) . (&#39;hynn&#39;, tensor(0), tensor([1.0000e+00, 6.5406e-07])) . (&#39;hynn&#39;, tensor(0), tensor([0.9253, 0.0747])) . (&#39;iu&#39;, tensor(1), tensor([0.1957, 0.8043])) . 결과를 보니까 hynn이 많음 $ to$ 어느정도 맞추는것 같긴하다. | . PILImage.create(testset[7]) . 실제로는 박혜원인데 아이유로 예측한 사진 | . path = Path() . if not (path/&#39;test2&#39;).exists(): (path/&#39;test2&#39;).mkdir() . urls=search_images_ddg(&#39;iu 아이유&#39;,max_n=20) download_images(path/&#39;test2&#39;,urls=urls) testset=get_image_files(path/&#39;test2&#39;) testset . (#20) [Path(&#39;test2/00000010.jpg&#39;),Path(&#39;test2/00000005.jpg&#39;),Path(&#39;test2/00000013.jpg&#39;),Path(&#39;test2/00000011.jpg&#39;),Path(&#39;test2/00000003.jpg&#39;),Path(&#39;test2/00000000.jpg&#39;),Path(&#39;test2/00000004.jpg&#39;),Path(&#39;test2/00000016.jpg&#39;),Path(&#39;test2/00000009.jpeg&#39;),Path(&#39;test2/00000012.jpg&#39;)...] . for i in range(len(testset)): print(learn.predict(PILImage.create(testset[i]))) . (&#39;iu&#39;, tensor(1), tensor([0.0051, 0.9949])) . (&#39;iu&#39;, tensor(1), tensor([8.7392e-06, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0895, 0.9105])) . (&#39;iu&#39;, tensor(1), tensor([0.0011, 0.9989])) . (&#39;iu&#39;, tensor(1), tensor([1.0321e-05, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0211, 0.9789])) . (&#39;iu&#39;, tensor(1), tensor([4.9877e-05, 9.9995e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0031, 0.9969])) . (&#39;iu&#39;, tensor(1), tensor([0.0011, 0.9989])) . (&#39;iu&#39;, tensor(1), tensor([1.5381e-05, 9.9998e-01])) . (&#39;iu&#39;, tensor(1), tensor([7.1447e-05, 9.9993e-01])) . (&#39;iu&#39;, tensor(1), tensor([1.3296e-04, 9.9987e-01])) . (&#39;hynn&#39;, tensor(0), tensor([0.9982, 0.0018])) . (&#39;iu&#39;, tensor(1), tensor([2.5169e-05, 9.9997e-01])) . (&#39;iu&#39;, tensor(1), tensor([1.2726e-05, 9.9999e-01])) . (&#39;iu&#39;, tensor(1), tensor([7.9650e-05, 9.9992e-01])) . (&#39;iu&#39;, tensor(1), tensor([3.0283e-04, 9.9970e-01])) . (&#39;iu&#39;, tensor(1), tensor([6.8668e-05, 9.9993e-01])) . (&#39;iu&#39;, tensor(1), tensor([0.0034, 0.9966])) . (&#39;iu&#39;, tensor(1), tensor([0.0052, 0.9948])) . 결과를 보니 아이유 역시 잘 맞추는 듯 보인다. | . - 정확률이 아쉽긴 하지만 어느정도 유의미한 결과를 얻었다. . &#49689;&#51228; . - 원하는 검색어로 이미지를 모은 뒤 결과를 제출 .",
            "url": "https://guebin.github.io/2021BDA/2021/09/09/(2%EC%A3%BC%EC%B0%A8)-9%EC%9B%949%EC%9D%BC.html",
            "relUrl": "/2021/09/09/(2%EC%A3%BC%EC%B0%A8)-9%EC%9B%949%EC%9D%BC.html",
            "date": " • Sep 9, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "(1주차) 9월7일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . - (1/6): 아나콘다 가상환경 만들기, 파이토치 설치, 주피터랩 설치, conda install 과 pip install 의 차이 . - (2/6): 이미지 분석을 위한 데이터셋 준비 및 정리 . - (3/6): 학습 및 예측 . - (4/6): 코랩설명 + 깃허브/블로그 (뒷부분은 화면전환 오류로 설명이 부실함) . - (5/6): 코랩설명 + 깃허브/블로그 . - (6/6): 우리강아지 이미지를 활용한 예측, 과제설명 . import . from fastai.vision.all import * . &#45936;&#51060;&#53552;&#51200;&#51109;, &#45936;&#51060;&#53552;&#47196;&#45908;&#49828; &#49373;&#49457;&#54980; dls&#47196; &#51200;&#51109; . path=untar_data(URLs.PETS)/&#39;images&#39; . files=get_image_files(path) # 이미지파일들의 이름을 모두 복붙하여 리스트를 만든뒤에 files.txt로 저장하는 과정으로 비유할 수 있음 . files[2] # txt 파일의 3번째 목록 . Path(&#39;/home/cgb4/.fastai/data/oxford-iiit-pet/images/leonberger_5.jpg&#39;) . def label_func(f): if f[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . label_func(&#39;asdf&#39;) . &#39;dog&#39; . dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(224)) . dls.show_batch(max_n=16) . &#54617;&#49845; . learn=cnn_learner(dls,resnet34,metrics=error_rate) . Downloading: &#34;https://download.pytorch.org/models/resnet34-b627a593.pth&#34; to C: Users cgb/.cache torch hub checkpoints resnet34-b627a593.pth . ImportError Traceback (most recent call last) ~ AppData Local Temp/ipykernel_29964/2234347239.py in &lt;module&gt; -&gt; 1 learn=cnn_learner(dls,resnet34,metrics=error_rate) ~ anaconda3 envs bda2021 lib site-packages fastai vision learner.py in cnn_learner(dls, arch, normalize, n_out, pretrained, config, loss_func, opt_func, lr, splitter, cbs, metrics, path, model_dir, wd, wd_bn_bias, train_bn, moms, **kwargs) 177 if n_out is None: n_out = get_c(dls) 178 assert n_out, &#34;`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`&#34; --&gt; 179 model = create_cnn_model(arch, n_out, pretrained=pretrained, **kwargs) 180 181 splitter=ifnone(splitter, meta[&#39;split&#39;]) ~ anaconda3 envs bda2021 lib site-packages fastai vision learner.py in create_cnn_model(arch, n_out, pretrained, cut, n_in, init, custom_head, concat_pool, **kwargs) 141 &#34;Create custom convnet architecture&#34; 142 meta = model_meta.get(arch, _default_meta) --&gt; 143 body = create_body(arch, n_in, pretrained, ifnone(cut, meta[&#39;cut&#39;])) 144 if custom_head is None: 145 nf = num_features_model(nn.Sequential(*body.children())) ~ anaconda3 envs bda2021 lib site-packages fastai vision learner.py in create_body(arch, n_in, pretrained, cut) 63 def create_body(arch, n_in=3, pretrained=True, cut=None): 64 &#34;Cut off the body of a typically pretrained `arch` as determined by `cut`&#34; &gt; 65 model = arch(pretrained=pretrained) 66 _update_first_layer(model, n_in, pretrained) 67 #cut = ifnone(cut, cnn_config(arch)[&#39;cut&#39;]) ~ anaconda3 envs bda2021 lib site-packages torchvision models resnet.py in resnet34(pretrained, progress, **kwargs) 286 progress (bool): If True, displays a progress bar of the download to stderr 287 &#34;&#34;&#34; --&gt; 288 return _resnet(&#39;resnet34&#39;, BasicBlock, [3, 4, 6, 3], pretrained, progress, 289 **kwargs) 290 ~ anaconda3 envs bda2021 lib site-packages torchvision models resnet.py in _resnet(arch, block, layers, pretrained, progress, **kwargs) 260 model = ResNet(block, layers, **kwargs) 261 if pretrained: --&gt; 262 state_dict = load_state_dict_from_url(model_urls[arch], 263 progress=progress) 264 model.load_state_dict(state_dict) ~ anaconda3 envs bda2021 lib site-packages torch hub.py in load_state_dict_from_url(url, model_dir, map_location, progress, check_hash, file_name) 551 r = HASH_REGEX.search(filename) # r is Optional[Match[str]] 552 hash_prefix = r.group(1) if r else None --&gt; 553 download_url_to_file(url, cached_file, hash_prefix, progress=progress) 554 555 if _is_legacy_zip_format(cached_file): ~ anaconda3 envs bda2021 lib site-packages torch hub.py in download_url_to_file(url, dst, hash_prefix, progress) 436 if hash_prefix is not None: 437 sha256 = hashlib.sha256() --&gt; 438 with tqdm(total=file_size, disable=not progress, 439 unit=&#39;B&#39;, unit_scale=True, unit_divisor=1024) as pbar: 440 while True: ~ anaconda3 envs bda2021 lib site-packages tqdm notebook.py in __init__(self, *args, **kwargs) 240 unit_scale = 1 if self.unit_scale is True else self.unit_scale or 1 241 total = self.total * unit_scale if self.total else self.total --&gt; 242 self.container = self.status_printer(self.fp, total, self.desc, self.ncols) 243 self.container.pbar = proxy(self) 244 self.displayed = False ~ anaconda3 envs bda2021 lib site-packages tqdm notebook.py in status_printer(_, total, desc, ncols) 113 # Prepare IPython progress bar 114 if IProgress is None: # #187 #451 #558 #872 --&gt; 115 raise ImportError( 116 &#34;IProgress not found. Please update jupyter and ipywidgets.&#34; 117 &#34; See https://ipywidgets.readthedocs.io/en/stable&#34; ImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html . - 에러를 해결하기 위해서는 아래를 설치하면 된다. . !conda install -c conda-forge jupyterlab_widgets -y !conda install -c conda-forge ipywidgets -y !conda install -c conda-forge nodejs -y . - 위를 설치하고 커널을 재시작하면 정상적으로 모형이 만들어진다. . learn=cnn_learner(dls,resnet34,metrics=error_rate) . /home/cgb4/anaconda3/envs/bda2021/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /tmp/pip-req-build-pma2oi4d/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.148441 | 0.018828 | 0.006766 | 00:12 | . epoch train_loss valid_loss error_rate time . 0 | 0.040593 | 0.014769 | 0.002706 | 00:11 | . &#50696;&#52769; . learn.predict(files[0]) . (&#39;dog&#39;, tensor(1), tensor([6.1421e-07, 1.0000e+00])) . learn.show_results() . &#50724;&#45813;&#48516;&#49437; . interp = Interpretation.from_learner(learn) . interp.plot_top_losses(16) . &#51652;&#51676; &#51096;&#46104;&#45716;&#44172; &#47582;&#45716;&#44148;&#44032;? . PILImage.create(&#39;2021-09-06-cat1.png&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-cat1.png&#39;)) . (&#39;cat&#39;, tensor(0), tensor([1.0000e+00, 1.7844e-07])) . - 헷갈리는 고양이 사진인데 잘 구분한다. . PILImage.create(&#39;2021-09-06-cat2.jpeg&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-cat2.jpeg&#39;)) . (&#39;cat&#39;, tensor(0), tensor([9.9984e-01, 1.6283e-04])) . PILImage.create(&#39;2021-09-06-hani01.jpeg&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-hani01.jpeg&#39;)) . (&#39;dog&#39;, tensor(1), tensor([5.0984e-04, 9.9949e-01])) . PILImage.create(&#39;2021-09-06-hani02.jpeg&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-hani02.jpeg&#39;)) . (&#39;dog&#39;, tensor(1), tensor([7.1694e-06, 9.9999e-01])) . PILImage.create(&#39;2021-09-06-hani03.jpg&#39;) . learn.predict(PILImage.create(&#39;2021-09-06-hani03.jpg&#39;)) . (&#39;dog&#39;, tensor(1), tensor([4.5399e-04, 9.9955e-01])) . &#45796;&#51020;&#49884;&#44036; . 이미지 크롤링 --&gt; 데이터셋트 --&gt; A,B 구분하는 모델 | . &#49689;&#51228; . 위의 사진들 이외에 사진들을 바탕으로 예측을 하는 모형구축. | 예측결과를 스샷으로 저장하여 제출 (이미지도 함께 스샷할것) | . &#49689;&#51228;&#52280;&#44256;&#51088;&#47308; . import PIL urls=&#39;https://t1.daumcdn.net/cfile/tistory/9925F03C5AD486B033&#39; urllib.request.urlretrieve(urls,&#39;temp.png&#39;) learn.predict(PILImage.create(&#39;temp.png&#39;)) . (&#39;dog&#39;, tensor(1), tensor([6.6117e-04, 9.9934e-01])) .",
            "url": "https://guebin.github.io/2021BDA/2021/09/07/(1%EC%A3%BC%EC%B0%A8)-9%EC%9B%947%EC%9D%BC.html",
            "relUrl": "/2021/09/07/(1%EC%A3%BC%EC%B0%A8)-9%EC%9B%947%EC%9D%BC.html",
            "date": " • Sep 7, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "(1주차) 9월2일",
            "content": "&#44053;&#51032;&#50689;&#49345; . - (1/3): 과목소개 . - (2/3): 카카오톡 채널 소개 . - (3/3): 텐서플로우-케라스 vs 파이토치-fastai, 과제안내 . . &#47112;&#54252;&#53944; . - 카카오톡 스샷제출 .",
            "url": "https://guebin.github.io/2021BDA/2021/09/02/(1%EC%A3%BC%EC%B0%A8)-9%EC%9B%942%EC%9D%BC.html",
            "relUrl": "/2021/09/02/(1%EC%A3%BC%EC%B0%A8)-9%EC%9B%942%EC%9D%BC.html",
            "date": " • Sep 2, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "(A1) 깃허브와 fastpages를 이용하여 블로그 개설하기",
            "content": "About this doc . - 본 포스트는 2021년 1학기 Python 입문 강의내용중 일부를 업로드 하였음. . - Github, fastpages를 사용하여 블로그를 개설하고 관리하는 방법에 대한 설명임. . .",
            "url": "https://guebin.github.io/2021BDA/2021/08/17/(A1)-%EA%B9%83%ED%97%88%EB%B8%8C%EC%99%80-fastpages%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EA%B0%9C%EC%84%A4%ED%95%98%EA%B8%B0.html",
            "relUrl": "/2021/08/17/(A1)-%EA%B9%83%ED%97%88%EB%B8%8C%EC%99%80-fastpages%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EA%B0%9C%EC%84%A4%ED%95%98%EA%B8%B0.html",
            "date": " • Aug 17, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "최규빈 . guebin@jbnu.ac.kr | 자연과학대학교 본관 205호 | 카카오톡 오픈채널 1 | . 2021년 2학기 종료후 폐쇄예정 &#8617; . |",
          "url": "https://guebin.github.io/2021BDA/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://guebin.github.io/2021BDA/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}